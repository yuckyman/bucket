{
  "last_updated": "2025-10-22T22:11:59.217535+00:00",
  "pending_count": 985,
  "processed_count": 15,
  "pending_articles": [
    {
      "id": "73024b23c72ccbdfbadfcf8d3a3d3545",
      "url": "https://arxiv.org/abs/2510.18311",
      "title": "Reimagining Disassembly Interfaces with Visualization: Combining Instruction Tracing and Control Flow with DisViz",
      "content": "arXiv:2510.18311v1 Announce Type: new \nAbstract: In applications where efficiency is critical, developers may examine their compiled binaries, seeking to understand how the compiler transformed their source code and what performance implications that transformation may have. This analysis is challenging due to the vast number of disassembled binary instructions and the many-to-many mappings between them and the source code. These problems are exacerbated as source code size increases, giving the compiler more freedom to map and disperse binary instructions across the disassembly space. Interfaces for disassembly typically display instructions as an unstructured listing or sacrifice the order of execution. We design a new visual interface for disassembly code that combines execution order with control flow structure, enabling analysts to both trace through code and identify familiar aspects of the computation. Central to our approach is a novel layout of instructions grouped into basic blocks that displays a looping structure in an intuitive way. We add to this disassembly representation a unique block-based mini-map that leverages our layout and shows context across thousands of disassembly instructions. Finally, we embed our disassembly visualization in a web-based tool, DisViz, which adds dynamic linking with source code across the entire application. DizViz was developed in collaboration with program analysis experts following design study methodology and was validated through evaluation sessions with ten participants from four institutions. Participants successfully completed the evaluation tasks, hypothesized about compiler optimizations, and noted the utility of our new disassembly view. Our evaluation suggests that our new integrated view helps application developers in understanding and navigating disassembly code.",
      "author": "Shadmaan Hye, Matthew P. LeGendre, Katherine E. Isaacs",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 257,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013389+00:00",
      "updated_at": "2025-10-22T21:39:15.013390+00:00"
    },
    {
      "id": "3ada2516406f65c525cacbcfb32481d0",
      "url": "https://arxiv.org/abs/2510.18296",
      "title": "Relief or displacement? How teachers are negotiating generative AI's role in their professional practice",
      "content": "arXiv:2510.18296v1 Announce Type: new \nAbstract: As generative AI (genAI) rapidly enters classrooms, accompanied by district-level policy rollouts and industry-led teacher trainings, it is important to rethink the canonical ``adopt and train'' playbook. Decades of educational technology research show that tools promising personalization and access often deepen inequities due to uneven resources, training, and institutional support. Against this backdrop, we conducted semi-structured interviews with 22 teachers from a large U.S. school district that was an early adopter of genAI. Our findings reveal the motivations driving adoption, the factors underlying resistance, and the boundaries teachers negotiate to align genAI use with their values. We further contribute by unpacking the sociotechnical dynamics -- including district policies, professional norms, and relational commitments -- that shape how teachers navigate the promises and risks of these tools.",
      "author": "Aayushi Dangol, Smriti Kotiyal, Robert Wolfe, Alex J. Bowers, Antonio Vigil, Jason Yip, Julie A. Kientz, Suleman Shahid, Tom Yeh, Vincent Cho, Katie Davis",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 131,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013351+00:00",
      "updated_at": "2025-10-22T21:39:15.013352+00:00"
    },
    {
      "id": "53c61575385a88158bfe5b00667132c5",
      "url": "https://arxiv.org/abs/2510.18185",
      "title": "Enhancing Urban Data Exploration: Layer Toggling and Visibility-Preserving Lenses for Multi-Attribute Spatial Analysis",
      "content": "arXiv:2510.18185v1 Announce Type: new \nAbstract: We propose two novel interaction techniques for visualization-assisted exploration of urban data: Layer Toggling and Visibility-Preserving Lenses. Layer Toggling mitigates visual overload by organizing information into separate layers while enabling comparisons through controlled overlays. This technique supports focused analysis without losing spatial context and allows users to switch layers using a dedicated button. Visibility-Preserving Lenses adapt their size and transparency dynamically, enabling detailed inspection of dense spatial regions and temporal attributes. These techniques facilitate urban data exploration and improve prediction. Understanding complex phenomena related to crime, mobility, and residents' behavior is crucial for informed urban planning. Yet navigating such data often causes cognitive overload and visual clutter due to overlapping layers. We validate our visualization tool through a user study measuring performance, cognitive load, and interaction efficiency. Using real-world data from Sao Paulo, we demonstrate how our approach enhances exploratory and analytical tasks and provides guidelines for future interactive systems.",
      "author": "Karelia Salinas, Luis Gustavo Nonato, Jean-Daniel Fekete, Fernanda Bartolo dos Santos Saran",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013323+00:00",
      "updated_at": "2025-10-22T21:39:15.013325+00:00"
    },
    {
      "id": "3e0a0586ae28cc5bb15745b915ba40d9",
      "url": "https://arxiv.org/abs/2510.18158",
      "title": "Design and Challenges of Mental Health Assessment Tools Based on Natural Language Interaction",
      "content": "arXiv:2510.18158v1 Announce Type: new \nAbstract: Mental health assessments are of central importance to individuals' well-being. Conventional assessment methodologies predominantly depend on clinical interviews and standardised self-report questionnaires. Nevertheless, the efficacy of these methodologies is frequently impeded by factors such as subjectivity, recall bias, and accessibility issues. Furthermore, concerns regarding bias and privacy may result in misreporting in data collected through self-reporting in mental health research. The present study examined the design opportunities and challenges inherent in the development of a mental health assessment tool based on natural language interaction with large language models (LLMs). An interactive prototype system was developed using conversational AI for non-invasive mental health assessment, and was evaluated through semi-structured interviews with 11 mental health professionals (six counsellors and five psychiatrists). The analysis identified key design considerations for future development, highlighting how AI-driven adaptive questioning could potentially enhance the reliability of self-reported data while identifying critical challenges, including privacy protection, algorithmic bias, and cross-cultural applicability. This study provides an empirical foundation for mental health technology innovation by demonstrating the potential and limitations of natural language interaction in mental health assessment.",
      "author": "Yixue Cai, Xiyan Su, Dongpeng Yao, Rongduo Han, Nan Gao, Haining Zhang",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 183,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013293+00:00",
      "updated_at": "2025-10-22T21:39:15.013294+00:00"
    },
    {
      "id": "590f6d0c28eadd22249b530c2d8816f1",
      "url": "https://arxiv.org/abs/2510.18039",
      "title": "Presenting Large Language Models as Companions Affects What Mental Capacities People Attribute to Them",
      "content": "arXiv:2510.18039v1 Announce Type: new \nAbstract: How does messaging about about large language models (LLMs) in public discourse influence the way people think about and interact with these models? To answer this question, we randomly assigned participants (N = 470) to watch a short informational video presenting LLMs as either machines, tools, or companions -- or to watch no video. We then assessed how strongly they believed LLMs to possess various mental capacities, such as the ability have intentions or remember things. We found that participants who watched the companion video reported believing that LLMs more fully possessed these capacities than did participants in other groups. In a follow-up study (N = 604), we replicated these findings and found nuanced effects on how these videos impact people's reliance on LLM-generated responses when seeking out factual information. Together, these studies highlight the impact of messaging about AI -- beyond technical advances in AI -- to generate broad societal impact.",
      "author": "Allison Chen, Sunnie S. Y. Kim, Angel Franyutti, Amaya Dharmasiri, Kushin Mukherjee, Olga Russakovsky, Judith E. Fan",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 157,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013254+00:00",
      "updated_at": "2025-10-22T21:39:15.013257+00:00"
    },
    {
      "id": "a4cd06d7c1c456cc380acd2e0a4bad21",
      "url": "https://www.sciencedirect.com/science/article/pii/S000689932500561X?dgcid=rss_sd_all",
      "title": "Association of long-term blood pressure exposure with DTI-ALPS: A population-based cohort study",
      "content": "<p>Publication date: 1 December 2025</p><p><b>Source:</b> Brain Research, Volume 1868</p><p>Author(s): Sihui Guo, Jing Sun, Yufan Zhang, Xiaoshuai Li, Jing Li, Xinyu Zhao, Pengfei Zhao, Shuohua Chen, Shouling Wu, Ying Hui, Zhenchang Wang, Han Lv</p>",
      "author": "",
      "published_date": null,
      "source": "Brain Research",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 33,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:09.319123+00:00",
      "updated_at": "2025-10-22T21:39:09.319124+00:00"
    },
    {
      "id": "3b7a3c4694790d658db9ad1f2eb5187d",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005452?dgcid=rss_sd_all",
      "title": "Mitigating choice overload: The interactive effects of set size and overall preference revealed by hierarchical drift diffusion modeling and electroencephalography",
      "content": "<p>Publication date: 1 November 2025</p><p><b>Source:</b> NeuroImage, Volume 321</p><p>Author(s): Xiaoyang Huang, Sihua Xu</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 12,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:05.649892+00:00",
      "updated_at": "2025-10-22T21:39:05.649893+00:00"
    },
    {
      "id": "6810e3a35ace1e93a6fd599ec22268cf",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005336?dgcid=rss_sd_all",
      "title": "The effectiveness of auditory stimulation in sleep varies with thalamocortical spindle phase",
      "content": "<p>Publication date: 1 November 2025</p><p><b>Source:</b> NeuroImage, Volume 321</p><p>Author(s): Hugo R. Jourde, Arina Ujevco, Emily B.J. Coffey</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 16,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:05.649874+00:00",
      "updated_at": "2025-10-22T21:39:05.649875+00:00"
    },
    {
      "id": "e15211b43f08536edd9ca55793b9e9e3",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005373?dgcid=rss_sd_all",
      "title": "Beyond the sound: Uncovering the distinct neural mechanisms of ERAN and MMN in music perception",
      "content": "<p>Publication date: 1 November 2025</p><p><b>Source:</b> NeuroImage, Volume 321</p><p>Author(s): Panke Gao, Hantong Wang, Yingzi Qiu, Jingjing Zhang</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 16,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:05.649855+00:00",
      "updated_at": "2025-10-22T21:39:05.649856+00:00"
    },
    {
      "id": "1a1f1f24e873b1d0ced21a177ef98fc1",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005282?dgcid=rss_sd_all",
      "title": "Motor imagery and self-recognition from actions",
      "content": "<p>Publication date: 1 November 2025</p><p><b>Source:</b> NeuroImage, Volume 321</p><p>Author(s): A. Kadambi, H. Lu, M. Monti, S. Narayanan, M. Iacoboni</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 18,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:05.649835+00:00",
      "updated_at": "2025-10-22T21:39:05.649837+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "7ac9402a5fcc31c13bd5fecc147f8803",
      "url": "https://arxiv.org/abs/2510.17948",
      "title": "Studying the Effects of Robot Intervention on School Shooters in Virtual Reality",
      "content": "arXiv:2510.17948v1 Announce Type: cross \nAbstract: We advance the understanding of robotic intervention in high-risk scenarios by examining their potential to distract and impede a school shooter. To evaluate this concept, we conducted a virtual reality study with 150 university participants role-playing as a school shooter. Within the simulation, an autonomous robot predicted the shooter's movements and positioned itself strategically to interfere and distract. The strategy the robot used to approach the shooter was manipulated -- either moving directly in front of the shooter (aggressive) or maintaining distance (passive) -- and the distraction method, ranging from no additional cues (low), to siren and lights (medium), to siren, lights, and smoke to impair visibility (high). An aggressive, high-distraction robot reduced the number of victims by 46.6% relative to a no-robot control. This outcome underscores both the potential of robotic intervention to enhance safety and the pressing ethical questions surrounding their use in school environments.",
      "author": "Christopher A McClurg, Alan R Wagner",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 152,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013538+00:00",
      "updated_at": "2025-10-22T22:11:59.120499+00:00",
      "metadata": {
        "processed_at": "2025-10-22T22:11:59.120511+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "c9ba746dd81f139c79990866061294b1",
      "url": "https://arxiv.org/abs/2510.17938",
      "title": "The Integration of Artificial Intelligence in Undergraduate Medical Education in Spain: Descriptive Analysis and International Perspectives",
      "content": "arXiv:2510.17938v1 Announce Type: cross \nAbstract: AI is transforming medical practice and redefining the competencies that future healthcare professionals need to master. Despite international recommendations, the integration of AI into Medicine curricula in Spain had not been systematically evaluated until now. A cross-sectional study (July-September 2025) including Spanish universities offering the official degree in Medicine, according to the 'Register of Universities, Centers and Degrees (Registro de Universidades, Centros y T\\'itulos RUCT)'. Curricula and publicly available institutional documentation were reviewed to identify courses and competencies related to AI in the 2025-2026 academic year. The analysis was performed using descriptive statistics. Of the 52 universities analyzed, ten (19.2%) offer specific AI courses, whereas 36 (69.2%) include no related content. Most of the identified courses are elective, with a credit load ranging from three to six ECTS, representing on average 1.17% of the total 360 credits of the degree. The University of Ja\\'en is the only institution offering a compulsory course with AI content. The territorial analysis reveals marked disparities: Andalusia leads with 55.5% of its universities incorporating AI training, while several communities lack any initiative in this area. The integration of AI into the medical degree in Spain is incipient, fragmented, and uneven, with a low weight in ECTS. The limited training load and predominance of elective courses restrict the preparation of future physicians to practice in a healthcare environment increasingly mediated by AI. The findings support the establishment of minimum standards and national monitoring of indicators.",
      "author": "Ana En\\'eriz Janeiro, Karina Pitombeira Pereira, Julio Mayol, Javier Crespo, Fernando Carballo, Juan B. Cabello, Manel Ramos-Casals, Bibiana P\\'erez Corbacho, Juan Turnes",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 244,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013510+00:00",
      "updated_at": "2025-10-22T22:11:59.120516+00:00",
      "metadata": {
        "processed_at": "2025-10-22T22:11:59.120518+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "9e22ea847d50204f75bd3c3c1bc7fac0",
      "url": "https://arxiv.org/abs/2510.17842",
      "title": "Vibe Coding: Toward an AI-Native Paradigm for Semantic and Intent-Driven Programming",
      "content": "arXiv:2510.17842v1 Announce Type: cross \nAbstract: Recent advances in large language models have enabled developers to generate software by conversing with artificial intelligence systems rather than writing code directly. This paper introduces vibe coding, an emerging AI-native programming paradigm in which a developer specifies high-level functional intent along with qualitative descriptors of the desired \"vibe\" (tone, style, or emotional resonance). An intelligent agent then transforms those specifications into executable software. We formalize the definition of vibe coding and propose a reference architecture that includes an intent parser, a semantic embedding engine, an agentic code generator, and an interactive feedback loop. A hypothetical implementation is described. We compare vibe coding with declarative, functional, and prompt-based programming, and we discuss its implications for software engineering, human-AI collaboration, and responsible AI practice. Finally, we examine reported productivity gains and democratizing effects, review recent studies that highlight vulnerabilities and potential slowdowns, identify key challenges such as alignment, reproducibility, bias, explainability, maintainability, and security, and outline future directions and open research questions.",
      "author": "Vinay Bamil",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 166,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013476+00:00",
      "updated_at": "2025-10-22T22:11:59.120520+00:00",
      "metadata": {
        "processed_at": "2025-10-22T22:11:59.120522+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "570e7717f47415628b301b2bd685cde6",
      "url": "https://arxiv.org/abs/2510.18625",
      "title": "Effects of Virtual Controller Representation and Virtuality on Selection Performance in Extended Reality",
      "content": "arXiv:2510.18625v1 Announce Type: new \nAbstract: We present an experiment exploring how the controller's virtual representation impacts target acquisition performance across MR and VR contexts. Participants performed selection tasks comparing four visual configurations: a virtual controller, a virtual hand, both the controller and the hand, and neither representation. We found performance comparable between VR and MR, and switching between them did not impact the user's ability to perform basic tasks. Controller representations mimicking reality enhanced performance across both modes. However, users perceived performance differently in MR, indicating the need for unique MR design considerations, particularly regarding spatial awareness.",
      "author": "Eric DeDeMarbre, Jay Henderson, J. Felipe Gonzalez, Rob Teather",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 97,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013447+00:00",
      "updated_at": "2025-10-22T22:11:59.120524+00:00",
      "metadata": {
        "processed_at": "2025-10-22T22:11:59.120525+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "32c5dd99b98b913f61ed3529393b1a6e",
      "url": "https://arxiv.org/abs/2510.18385",
      "title": "Khelte Khelte Shikhi: A Proposed HCI Framework for Gamified Interactive Learning with Minecraft in Bangladeshi Education Systems",
      "content": "arXiv:2510.18385v1 Announce Type: new \nAbstract: Game-based learning shows real promise for engaging students in well-funded schools, but what about everyone else? We propose a practical framework for implementing Minecraft Education Edition in Bangladesh's 130,000 schools where 55 percent lack reliable internet, rural areas experience 12-16 hour daily power availability, only 8 percent of rural schools have computer access, and student-teacher ratios reach 52:1. Our approach tackles these constraints head-on with three deployment tiers: cloud-based multiplayer for urban schools with stable infrastructure (15 percent), local area network solutions with solar power for semi-urban contexts (30 percent), and offline turn-based modes using refurbished hardware for rural settings (55 percent). We provide eight pre-built curriculum-aligned worlds with complete Bangla localization covering topics from Lalbagh Fort reconstruction to monsoon flood simulation. The interface accommodates first-time users through progressive complexity, culturally familiar metaphors using local farming and architecture, and accessibility features including keyboard-only controls and 200 percent text scaling. Teacher training spans 48 hours across digital literacy, pedagogical integration, and content creation. We detail evaluation protocols with specific benchmarks: 15 percent learning gains, 70 percent transfer task mastery, System Usability Scale scores above 70, and sub-two-dollar cost per student-hour. This framework has not been empirically validated; it synthesizes game-based learning theory, HCI principles, and contextual analysis to provide implementable specifications for pilot testing in resource-constrained settings.",
      "author": "Mohd Ruhul Ameen, Akif Islam, Momen Khandokar Ope",
      "published_date": "2025-10-22T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 221,
      "reading_time": 1,
      "created_at": "2025-10-22T21:39:15.013423+00:00",
      "updated_at": "2025-10-22T22:11:59.120527+00:00",
      "metadata": {
        "processed_at": "2025-10-22T22:11:59.120534+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7cd5915b404adde9692f79fbf455044d",
      "url": "http://ieeexplore.ieee.org/document/11037651",
      "title": "A Force/Torque Taxonomy for Classifying States During Physical Co-Manipulation",
      "content": "Achieving seamless human-robot collaboration requires a deeper understanding of how agents manage and communicate forces during shared tasks. Force interactions during collaborative manipulation are inherently complex, especially when considering how they evolve over time. To address this complexity, we propose a taxonomy of decomposed force and torque components, providing a structured framework for examining haptic communication and informing the development of robots capable of performing meaningful collaborative manipulation tasks with human partners. We propose a standardized terminology for force decomposition and classification, bridging the varied language in previous literature in the field, and conduct a review of physical human-human interaction and haptic communication. The proposed taxonomy allows for a more effective and nuanced discussion of important force combinations that we expect to occur during collaborative manipulation (between human-human or human-robot teams). We also include example scenarios to illustrate the value of the proposed taxonomy in describing interactions between agents.",
      "author": "",
      "published_date": "2025-06-17T13:16:38+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 149,
      "reading_time": 1,
      "created_at": "2025-10-22T19:39:35.894565+00:00",
      "updated_at": "2025-10-22T20:18:02.696610+00:00",
      "metadata": {
        "processed_at": "2025-10-22T20:18:02.696620+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "bffad7ff78c038ce61497f8206575f26",
      "url": "http://ieeexplore.ieee.org/document/11045422",
      "title": "Haptic Relocation Away From the Fingertip: Where, Why, and How",
      "content": "Tactile haptic devices are often designed to render meaningful, complex, and realistic touch-based information on users\u2019 skin. While fingertips and hands are the most preferred body locations to render haptic feedback, recent trends allow such feedback to be extended to alternative body locations (e.g., wrist, arm, torso, foot) for various scenarios due to reasons such as wearability and needs of the application. In this paper, I address the new concept of haptic relocation. It refers to scenarios in which the expected feedback is related to the fingertips but rendered on a different body location instead \u2013 e.g., contact forces registered by two robotic fingers during teleoperation rendered to the users\u2019 wrist instead of the fingers. I investigated the design choices of wearable haptic devices for haptic relocation concerning different body locations, targeted applications, and actuator selection. I discuss approaches and design choices from the literature by speculating on the possible reasons, and conclude the paper by highlighting some challenges and issues to be mindful of in the future. This paper will guide engineers and researchers in searching for alternative haptic rendering solutions \u2013 especially when fingers and hands are not available for haptic interaction.",
      "author": "",
      "published_date": "2025-06-20T13:16:43+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 194,
      "reading_time": 1,
      "created_at": "2025-10-22T19:39:35.894534+00:00",
      "updated_at": "2025-10-22T20:18:02.696624+00:00",
      "metadata": {
        "processed_at": "2025-10-22T20:18:02.696626+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "654c67a50800e0d8df1eb841fa063ae1",
      "url": "http://ieeexplore.ieee.org/document/10918829",
      "title": "Tactile\u2013Thermal Interactions: Cooperation and Competition",
      "content": "This review focuses on the interactions between the cutaneous senses, and in particular touch and temperature, as these are the most relevant for developing skin-based display technologies for use in virtual reality (VR) and for designing multimodal haptic devices. A broad spectrum of research is reviewed ranging from studies that have examined the mechanisms involved in thermal intensification and tactile masking, to more applied work that has focused on implementing thermal-tactile illusions such as thermal referral and illusory wetness in VR environments. Research on these tactile-thermal illusions has identified the differences between the senses of cold and warmth in terms of their effects on the perception of object properties and the prevalence of the perceptual experiences elicited. They have also underscored the fundamental spatial and temporal differences between the tactile and thermal senses. The wide-ranging body of research on compound sensations such as wetness and stickiness has highlighted the mechanisms involved in sensing moisture and provided a framework for measuring these sensations in a variety of contexts. Although the interactions between the two senses are complex, it is clear that the addition of thermal inputs to a tactile display enhances both user experience and enables novel sensory experiences.",
      "author": "",
      "published_date": "2025-03-10T13:16:41+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 198,
      "reading_time": 1,
      "created_at": "2025-10-22T19:39:35.894493+00:00",
      "updated_at": "2025-10-22T20:18:02.696630+00:00",
      "metadata": {
        "processed_at": "2025-10-22T20:18:02.696632+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "604cffaea5736d8c2935253598862e29",
      "url": "http://ieeexplore.ieee.org/document/11174044",
      "title": "Twenty Years of World Haptics: Retrospective and Future Directions",
      "content": "null",
      "author": "",
      "published_date": "2025-09-19T13:16:57+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-22T19:39:35.894455+00:00",
      "updated_at": "2025-10-22T20:18:02.696634+00:00",
      "metadata": {
        "processed_at": "2025-10-22T20:18:02.696636+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "65fcee7a3858adcf1bad71db41168384",
      "url": "http://ieeexplore.ieee.org/document/11174043",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-09-19T13:16:57+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-22T19:39:35.894435+00:00",
      "updated_at": "2025-10-22T20:18:02.696638+00:00",
      "metadata": {
        "processed_at": "2025-10-22T20:18:02.696640+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}