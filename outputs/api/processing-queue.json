{
  "last_updated": "2026-01-05T04:53:34.291108+00:00",
  "pending_count": 615,
  "processed_count": 385,
  "pending_articles": [
    {
      "id": "8bec92d361ee3c15eab323f1288a063d",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1699179",
      "title": "State-dependent filtering as a mechanism toward visual robustness",
      "content": "Robustness, defined as a system's ability to maintain functional reliability in the face of perturbations, is achieved through its capacity to filter external disturbances using internal priors encoded in its structure and states. While biophysical neural networks are widely recognized for their robustness, the precise mechanisms underlying this resilience remain poorly understood. In this study, we explore how orientation-selective neurons arranged in a one-dimensional ring network respond to perturbations, with the aim of uncovering insights into the robustness of visual subsystems in the brain. By analyzing the steady-state dynamics of a rate-based network, we characterize how the activation state of neurons influences the network's response to disturbances. Our results demonstrate that the activation state of neurons, rather than their firing rates alone, governs the network's sensitivity to perturbations. We further show that lateral connectivity modulates this effect by shaping the response profile across spatial frequency components. These findings suggest a state-dependent filtering mechanism that contributes to the robustness of visual circuits, offering theoretical insight into how different components of perturbations are selectively modulated within the network.",
      "author": "Yaoyu Zhang",
      "published_date": "2025-12-10T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 176,
      "reading_time": 1,
      "created_at": "2026-01-04T23:42:12.809892+00:00",
      "updated_at": "2026-01-04T23:42:12.809893+00:00"
    },
    {
      "id": "63022d1bdc6d9b1938f28ac79ff063e0",
      "url": "https://www.frontiersin.org/articles/10.3389/fnhum.2025.1594106",
      "title": "Leveraging transcranial ultrasound stimulation to enhance self-regulation in emotion and sleep",
      "content": "This Perspective article discusses the emerging potential of transcranial ultrasound stimulation (TUS) as a non-invasive neuromodulatory technique for enhancing self-regulatory processes, particularly emotion and sleep regulation, in healthy individuals. Offering high spatial precision and the ability to target both cortical and deep brain regions, TUS uses focused ultrasound waves to induce acute and delayed effects on brain activity. We propose that combining TUS with neurofeedback methods and/or specific cognitive training exercises may capitalise on these neuroplastic effects, thereby augmenting and prolonging their impact to support lasting improvements in self-regulation. We focus on the domains of sleep and emotion regulation, where such an integrated approach may strengthen resilience and promote healthier functioning in the general population. Our aim is to highlight the potential of TUS-based integrated interventions for supporting mental health and well-being in non-clinical populations and to outline key directions for future research.",
      "author": "Elsa Fouragnan",
      "published_date": "2025-12-19T00:00:00+00:00",
      "source": "Frontiers Human Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 143,
      "reading_time": 1,
      "created_at": "2026-01-04T23:42:11.338306+00:00",
      "updated_at": "2026-01-04T23:42:11.338307+00:00"
    },
    {
      "id": "6309fe51366e561677734ca5a0b65f2e",
      "url": "https://www.frontiersin.org/articles/10.3389/fnbot.2025.1706626",
      "title": "IAP-TransUNet: integration of the attention mechanism and pyramid pooling for medical image segmentation",
      "content": "IntroductionThe combination of CNN and Transformer has attracted much attention for medical image segmentation due to its superior performance at present. However, the segmentation performance is affected by limitations such as the local receptive field and static weights of CNN convolution operations, as well as insufficient information exchange between Transformer local regions.MethodsTo address these issues, an integrated attention mechanism and pyramid pooling network is proposed in this paper. Firstly, an efficient channel attention mechanism is embedded into CNN to extract more comprehensive image features. Then, CBAM_ASPP module is introduced into the bottleneck layer to obtain multi-scale context information. Finally, in order to address the limitations of traditional convolution, depthwise separable convolution is used to achieve a lightweight network.ResultsThe experiments based on the Synapse multi organ segmentation dataset and ACDC dataset showed that the proposed IAP-TransUNet achieved Dice similarity coefficients (DSCs) of 78.85% and 90.46%, respectively. Compared with the state-of-the-art method, for the Synapse multi organ segmentation dataset, the Hausdorff distance was reduced by 2.92%. For the ACDC dataset, the segmentation accuracy of the left ventricle, myocardium, and right ventricle was improved by 0.14%, 1.89%, and 0.23%, respectively.DiscussionThe experimental results demonstrate that the proposed network has improved the effectiveness and shows strong performance on both CT and MRI data, which suggests its potential for generalization across different medical imaging modalities.",
      "author": "Quan Liu",
      "published_date": "2025-12-01T00:00:00+00:00",
      "source": "Frontiers Neurorobotics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 219,
      "reading_time": 1,
      "created_at": "2026-01-04T23:42:09.907838+00:00",
      "updated_at": "2026-01-04T23:42:09.907840+00:00"
    },
    {
      "id": "93d4f700e0aae9a6263822cab2b038cf",
      "url": "http://doi.org/10.1037/pmu0000299",
      "title": "Capturing coordination and intentionality in joint musical improvisation.",
      "content": "Humans collaborate with each other on a wide variety of tasks that are often largely improvised and unscripted. In this study, we investigated the dynamics of coordination in a joint musical improvisation task, what the effect of intentions is on coordination, and how musicians propagate these intentions. To quantify coordination within musical trios, we derived per-musician time series of acoustic features to which we applied effective transfer entropy (ETE) and empirical dynamic modeling (EDM), two methods derived from complex systems science. Using ETE allowed us to investigate coordination as directional information flow between musicians, whereas through EDM we conceptualized coordination as the predictability of a complex system. We found that both techniques, when applied to root-mean-square (RMS) amplitude time series, could be used to distinguish coordinating from noncoordinating musicians. Various other feature\u2013technique combinations, such as fractal dimension\u2013ETE and Tonnetz distance\u2013EDM, were also viable. Our results further suggest that coordination improves as an intention gets more shared, that is, as more musicians in the joint improvisation have the same intention. Lastly, we found evidence suggesting that musicians increase the predictability of their playing when seeking to end a performance, though our results did not provide an indication that this was done with the intention of improving coordination with partners. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2023-08-03T00:00:00+00:00",
      "source": "Psychomusicology",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2026-01-04T23:41:54.348912+00:00",
      "updated_at": "2026-01-04T23:41:54.348914+00:00"
    },
    {
      "id": "bbe0f805e4fabfb0ac2476417484aef4",
      "url": "http://ieeexplore.ieee.org/document/10946856",
      "title": "Enhancing Video Experiences for DHH Individuals Through Sound-Inspired Motion Caption-Based Spatiotemporal Tacton",
      "content": "When deaf and hard of hearing (DHH) individuals watch videos, captions are essential for them to understand the linguistic content. Current captions, however, are not suitable for conveying non-verbal sound information, such as background music, sound effects, or speech nuances. In this paper, we designed a multimodal system, Motion Caption Haptic System (MCHS), that enables DHH individuals to encounter sounds in videos through animated caption and spatiotemporal vibration patterns, supporting a more vivid and immersive experience. We elaborately designed motion captions and spatiotemporal haptic patterns for representative sound effects and spoken emotions to work well together through surveys from 27 DHH and 64 hearing participants. An evaluation with 19 DHH individuals demonstrated the capabilities and potential of the MCHS to improve their video viewing experience, along with a discussion of important issues that need to be addressed when designing multimodal captioning systems for the DHH viewers.",
      "author": "",
      "published_date": "2025-04-01T13:17:18+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2026-01-04T23:21:35.801030+00:00",
      "updated_at": "2026-01-04T23:21:35.801031+00:00"
    },
    {
      "id": "609521b013e1243f77b1eaa5e01eab3e",
      "url": "http://ieeexplore.ieee.org/document/10965524",
      "title": "VibTac: A High-Resolution High-Bandwidth Tactile Sensing Finger for Multi-Modal Perception in Robotic Manipulation",
      "content": "Tactile sensing is pivotal for enhancing robot manipulation abilities by providing crucial feedback for localized information. However, existing sensors often lack the necessary resolution and bandwidth required for intricate tasks. To address this gap, we introduce VibTac, a novel multi-modal tactile sensing finger designed to offer high-resolution and high-bandwidth tactile sensing simultaneously. VibTac seamlessly integrates vision-based and vibration-based tactile sensing modes to achieve high-resolution and high-bandwidth tactile sensing respectively, leveraging a streamlined human-inspired design for versatility in tasks. This paper outlines the key design elements of VibTac and its fabrication methods, highlighting the significance of the Elastomer Gel Pad (EGP) in its sensing mechanism. The sensor's multi-modal performance is validated through 3D reconstruction and spectral analysis to discern tactile stimuli effectively. In experimental trials, VibTac demonstrates its efficacy by achieving over 90% accuracy in insertion tasks involving objects emitting distinct sounds, such as ethernet connectors. Leveraging vision-based tactile sensing for object localization and employing a deep learning model for \u201cclick\u201d sound classification, VibTac showcases its robustness in real-world scenarios.",
      "author": "",
      "published_date": "2025-04-15T13:16:45+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 169,
      "reading_time": 1,
      "created_at": "2026-01-04T23:21:35.801001+00:00",
      "updated_at": "2026-01-04T23:21:35.801002+00:00"
    },
    {
      "id": "42247a9e793d9dae5103ccfc54fc3b5e",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.02.697327v1?rss=1",
      "title": "Evaluating and Classifying Gentleness in VR-Based Surgical Simulation: A VR+fNIRS Study",
      "content": "Gentleness, defined as the ability to handle tissues delicately and minimize unnecessary force, is a key indicator of surgical proficiency. Objective and real-time assessment of gentleness in virtual reality (VR)-based training can enhance the understanding of both psychomotor and cognitive aspects of surgical skill. This study evaluates and classifies participants gentleness during VR-based surgical simulations using fNIRS-derived hemodynamic features. We trained and compared several machine learning models to assess performance. Twenty-three volunteers with no prior laparoscopic experience performed a virtual reality-based laparoscopic double-grasper task while hemodynamic activity over frontal and motor cortical areas was recorded using eighteen fNIRS channels. Alongside fNIRS, we collected subjective workload (NASA-TLX), error numbers, and a VR gentleness score. This task involves using two grasper tools simultaneously to perform the tissue like balloon manipulation in a VR environment. We extracted temporal features (slope, RMS, standard deviation) and trained machine learning models to classify performance levels based on cortical activation. Labels were binarized as low vs. high using median splits for the gentleness score. Models were evaluated with stratified 5-fold cross-validation and summarized by accuracy. Results showed stronger right-frontal HbO activity and increased left-motor HbR responses in the low-performance group, suggesting greater cognitive effort and less efficient motor strategies during VR-based laparoscopic manipulation. Across classifiers and feature sets, slope-based features consistently outperformed variability- and amplitude-based metrics. Among the tested models, HbR slope features achieved the best overall classification performance, with the highest accuracy obtained using K-Nearest Neighbor and Random Forest classifiers (accuracy {approx} 0.89, AUC up to 0.97). These findings demonstrate that fNIRS-derived hemodynamic dynamics can reliably discriminate between high and low VR performance levels, supporting their potential use in automated performance assessment and neuroadaptive feedback frameworks for VR-based surgical training.",
      "author": "Sanli, S., Keles, H. O.",
      "published_date": "2026-01-04T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 284,
      "reading_time": 1,
      "created_at": "2026-01-04T23:21:16.331660+00:00",
      "updated_at": "2026-01-04T23:21:16.331662+00:00"
    },
    {
      "id": "3a9a78f8a19dd3283fbdf2a3d79bb820",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.04.697560v1?rss=1",
      "title": "When exploration replaces storage: how eye movements shape visual working memory",
      "content": "Visual working memory (VWM) is traditionally studied while constraining eye movements and limiting access to visual input, yet in natural vision humans constantly explore and resample their environment. Only a few studies have examined VWM utilization when participants were allowed to interact with the environment and found that participants often preferred to resample their environment rather than rely on VWM storage. However, since eye movements were not controlled in these studies, the link between VWM utilization and free visual exploration remained unknown. In two experiments (N = 40), we investigated how visual exploration shapes reliance on VWM versus perceptual input. Participants searched for a common target across two item sets and could either store multiple items for comparison or repeatedly resample the sets by switching between them. Results revealed that when switching was achieved through eye movements, participants consistently relied more on visual resampling and less on VWM; in contrast, when switching required a manual response, they shifted toward greater VWM use. This pattern persisted even when peripheral input was equated, suggesting that natural exploration through eye movements reduces the cognitive cost of acquiring visual information, leading to a strategic reduction in VWM use. Our findings challenge fixation-based approaches to VWM research and highlight the importance of studying cognition under ecological viewing conditions.",
      "author": "Qais, R., Knight, R., Yuval-Greenberg, S.",
      "published_date": "2026-01-04T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 213,
      "reading_time": 1,
      "created_at": "2026-01-04T23:21:16.331608+00:00",
      "updated_at": "2026-01-04T23:21:16.331613+00:00"
    },
    {
      "id": "fca62ea89e4642259308a4cccd8727ef",
      "url": "https://www.frontiersin.org/articles/10.3389/fnhum.2025.1628840",
      "title": "Gender perception of pareidolia faces in emergency department patients: the influence of physician gender",
      "content": "BackgroundThe assessment of gender perception influenced by ambiguous facial cues in patients requiring emergency medical attention remains ambiguous. Pareidolia faces represent unconscious errors in facial recognition, wherein a wide array of visual attributes contribute to the interpretation of facial features. This study aims to explore the mechanisms underlying gender perception in individuals undergoing emergency medical treatment, employing an innovative digital pareidolia assessment to evaluate gender perception within the context of face pareidolia.MethodsFifty adult patients treated by a female physician in the green triage zone participated in the study. Target images consisted of face pareidolia images, while non-target images were scrambled. All images were standardized for size, tone, and light intensity. Patients instructed the pareidolia images and were asked if they discerned a face; if they answered \u2018No,\u2019 the next image was shown. If they saw a face, they identified the associated gender. Their responses and reaction times were systematically recorded digitally.ResultsOur findings revealed that, regardless of wait times, patients were significantly more likely to identify pareidolia faces as male rather than female, especially after being examined by a female physician. Additionally, male patients exhibited slightly longer reaction times than females when responding to pareidolia images.ConclusionThe outcomes of this investigation provide critical insights into the influence of pareidolia on gender perception of faces in the emergency department setting. It underscores the notion that gender biases, which arise from both biological and sociocultural factors, can affect the dynamics of patient-physician interactions.",
      "author": "Nilg\u00fcn Altunta\u015f",
      "published_date": "2025-12-19T00:00:00+00:00",
      "source": "Frontiers Human Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 239,
      "reading_time": 1,
      "created_at": "2026-01-04T23:21:05.747020+00:00",
      "updated_at": "2026-01-04T23:21:05.747022+00:00"
    },
    {
      "id": "f3f8b276d80884f468b2b1a1efe62eca",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1703748",
      "title": "A method for measuring first glymphatic influx of a cerebrospinal fluid tracer in the human brain",
      "content": "IntroductionThe glymphatic system is a brain-wide perivascular transport route for fluids and solutes in which cerebrospinal fluid (CSF) serves as a conduit for solute transport and clearance of brain waste. Intrathecal contrast-enhanced magnetic resonance imaging (MRI), where the intrathecal contrast agent serves as a CSF tracer, has been developed to measure glymphatic function in humans. The normalized MRI T1 signal is a semiquantitative measure of CSF flow and exchange with the brain. Objective: To estimate first-time tracer appearance within brain tissue after intrathecal tracer injection.MethodsThis study implemented segmented regression analysis to estimate the first-time tracer appearance of an intrathecal tracer within brain tissues. An increase (breakpoint) in the normalized MRI T1 signal was defined to represent first glymphatic influx of the tracer. The study included 30 reference (REF) subjects with no identified CSF disturbance and 15 patients with a diagnosis of idiopathic intracranial hypertension (IIH). We developed and evaluated the method in REF subjects and further compared it between the two study groups.ResultsThe time to initial glymphatic tracer enrichment in the REF cohort was approximately 1\u202fh in the frontal, temporal, parietal, and occipital cerebral cortex and ranged from two to 4\u202fh in the corresponding white matter regions. In subcortical limbic structures and basal ganglia structures, it was 0.6 and 2.2\u202fh, respectively. Compared with REF subjects, IIH patients presented a non-significant mean difference in the first appearance of \u00b10.5\u202fh in the cerebral cortex and white matter regions, with somewhat longer estimated delays in the parietal and insular white matter regions. The results are presented as time series plots and estimates with 95% confidence intervals. Additionally, we provide supplementary R code, which can be adapted for use in future studies, and outline a basic assessment of true versus estimated breakpoints using simulated data.ConclusionSegmented regression was found feasible to quantify the time to first glymphatic enrichment, i.e., increase in the normalized MRI T1 signal. Moreover, the method seems reasonable to differentiate first glymphatic influx between the cohorts.",
      "author": "Per Kristian Eide",
      "published_date": "2025-12-19T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 328,
      "reading_time": 1,
      "created_at": "2026-01-04T23:21:03.357827+00:00",
      "updated_at": "2026-01-04T23:21:03.357828+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "449f636bf19a9861fc4e17d727b8b060",
      "url": "https://www.nature.com/articles/s44159-025-00516-z",
      "title": "The development of spatial perception with and without visual experience",
      "content": "",
      "author": "",
      "published_date": "2026-01-05T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-05T04:01:08.772008+00:00",
      "updated_at": "2026-01-05T04:53:34.181756+00:00",
      "metadata": {
        "processed_at": "2026-01-05T04:53:34.181766+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "268dc7d9e199fe37a63e69771335c93e",
      "url": "https://www.nature.com/articles/s41467-025-68218-x",
      "title": "Neuronal feedback loop of the suprachiasmatic nucleus generates robust circadian rhythms",
      "content": "",
      "author": "",
      "published_date": "2026-01-05T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-05T04:01:08.771984+00:00",
      "updated_at": "2026-01-05T04:53:34.181770+00:00",
      "metadata": {
        "processed_at": "2026-01-05T04:53:34.181772+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "13475a4a2dc3268439d30940634f498d",
      "url": "https://www.reddit.com/r/Python/comments/1q3oxk3/i_built_a_tui_process_manager_that_uses_a_local/",
      "title": "I built a TUI Process Manager that uses a Local LLM to classify and \"roast\" background processes",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>**What My Project Does** </p> <p>A terminal\u2011based TUI that monitors the process tree (parent, CPU, memory, I/O) and feeds this context to a local LLM (Llama 3 via Ollama or Groq). The model classifies each process as \u201cCritical\u201d or \u201cBloatware\u201d. For bloatware it prints a short roast and offers to kill it. BrainKernel (<a href=\"https://github.com/mprajyothreddy/brainkernel\">https://github.com/mprajyothreddy/brainkernel</a>) replaces the usual CPU % sorting with \u201cvibe\u2011based\u201d sorting, shows a live\u2011updating table while staying under 1 % CPU, and can auto\u2011suspend distraction apps (e.g., Steam, games) when they exceed a user\u2011defined threshold. </p> <p>**Target Audience** </p> <p>- Developers who are tired of manually hunting down unknown processes. </p> <p>- Linux and Windows users who want a task manager that explains what a process is doing. </p> <p>- Anyone experimenting with local edge AI integrated into the OS loop. </p> <p>**Comparison** </p> <p>- **vs htop/top:** htop is faster and lighter but assumes you know every process name. BrainKernel adds semantic classification, giving meaning to each PID. </p> <p>- **vs Windows Task Manager:** Windows\u2019 manager is GUI\u2011centric and lacks process explanations. BrainKernel is keyboard\u2011centric, can \u201croast\u201d bloatware for entertainment, and includes a focus/governor mode that can automatically suspend or kill distracting applications.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Turbulent-Spark6633\"> /u/Turbulent-Spark6633 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1q3oxk3/i_built_a_tui_process_manager_that_uses_a_local/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1q3oxk3/i_built_a_tui_process_manager_that_uses_a_local/\">[comments]</a></span>",
      "author": "/u/Turbulent-Spark6633",
      "published_date": "2026-01-04T12:37:12+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2026-01-05T04:00:32.753453+00:00",
      "updated_at": "2026-01-05T04:53:34.181775+00:00",
      "metadata": {
        "processed_at": "2026-01-05T04:53:34.181777+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a5170a3e4e73f927bab87d58eee2211a",
      "url": "https://www.robinsloan.com/winter-garden/agi-is-here/",
      "title": "AGI Is Here",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46494830\">Comments</a>",
      "author": "",
      "published_date": "2026-01-05T02:55:34+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-05T04:00:31.614830+00:00",
      "updated_at": "2026-01-05T04:53:34.181779+00:00",
      "metadata": {
        "processed_at": "2026-01-05T04:53:34.181781+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "742c7afff5640d695b966fc3d40ebeee",
      "url": "https://sparkbox.com/foundry/helene_and_mobile_web_performance",
      "title": "During Helene, I just wanted a plain text website",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46494734\">Comments</a>",
      "author": "",
      "published_date": "2026-01-05T02:36:20+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-05T04:00:31.614751+00:00",
      "updated_at": "2026-01-05T04:53:34.181783+00:00",
      "metadata": {
        "processed_at": "2026-01-05T04:53:34.181785+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "742c7afff5640d695b966fc3d40ebeee",
      "url": "https://sparkbox.com/foundry/helene_and_mobile_web_performance",
      "title": "During Helene, I just wanted a plain text website",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46494734\">Comments</a>",
      "author": "",
      "published_date": "2026-01-05T02:36:20+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-05T04:00:31.614751+00:00",
      "updated_at": "2026-01-05T04:53:34.181783+00:00",
      "metadata": {
        "processed_at": "2026-01-05T04:53:34.181785+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "eb99afbf44b79cf266312c85904181fc",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.04.697049v1?rss=1",
      "title": "Snipers under stress: mentally simulated motor actions are resistant to acute stress in police officers",
      "content": "Motor imagery (MI) is a sensorimotor process allowing to mentally simulate a motor action and is widely used to enhance performance in domains such as rehabilitation, sport, and professional training. Although MI is increasingly incorporated into stress-management interventions, the reciprocal relationship, that is, the effect of acute stress on MI remain paradoxically poorly understood, particularly in ecologically valid settings. The present study investigated this issue in professional police officers performing physically and mentally a precise handgun manipulation task under graded stress conditions (psychological stress). Fourteen participants was exposed to controlled, work-related stress scenarios designed to closely reflect real operational demands, while stress markers and temporal features of executed and imagined movements were recorded. Our results clearly indicate that motor imagery was preserved despite significant increases in stress levels, as the temporal characteristics of both executed and imagined movements remained stable across experimental conditions. These findings indicate that MI is resilient to acute psychosocial stress and support its relevance as a reliable training and performance-optimization tool in high-demand occupational settings.",
      "author": "Monier, H., Grospretre, S., gueugneau, n.",
      "published_date": "2026-01-04T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 169,
      "reading_time": 1,
      "created_at": "2026-01-05T01:58:40.646395+00:00",
      "updated_at": "2026-01-05T03:51:03.521025+00:00",
      "metadata": {
        "processed_at": "2026-01-05T03:51:03.521034+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "26ba4f933eb13bc59665501411f5051d",
      "url": "https://www.reddit.com/r/Python/comments/1q3ecap/generating_graphs_and_jsons_with_vlrgg_tournaments/",
      "title": "Generating graphs and jsons with vlrgg tournaments",
      "content": "<!-- SC_OFF --><div class=\"md\"><h1>What My Project Does</h1> <p>A Python tool that scrapes <strong>Valorant player stats from</strong> <a href=\"http://VLR.gg\"><strong>VLR.gg</strong></a> and exports clean <strong>JSON files</strong> with <strong>KDA and player images</strong>.<br /> It also includes a <strong>bar graph generator</strong> to visualize and compare players across <strong>career-wide stats or specific tournaments</strong> (single or multiple events).</p> <h1>Target Audience</h1> <p>Primarily for <strong>developers, analysts, and Valorant fans</strong> who want to analyze <a href=\"http://VLR.gg\">VLR.gg</a> data locally.<br /> It\u2019s a <strong>personal / educational project</strong>, not meant for production-scale scraping.</p> <h1>Comparison</h1> <p>Unlike most <a href=\"http://VLR.gg\">VLR.gg</a> scrapers, this project:</p> <ul> <li>Supports <strong>career-based and tournament-based</strong> stats</li> <li>Can scrape <strong>multiple tournaments at once</strong></li> <li>Extracts <strong>player profile images</strong></li> <li>Includes a built-in <strong>visual graph generator</strong> instead of only raw data</li> </ul> <p><a href=\"https://github.com/MateusVega/vlrgg-stats-scraper\">https://github.com/MateusVega/vlrgg-stats-scraper</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Stock-Loquat111\"> /u/Stock-Loquat111 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1q3ecap/generating_graphs_and_jsons_with_vlrgg_tournaments/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1q3ecap/generating_graphs_and_jsons_with_vlrgg_tournaments/\">[comments]</a></span>",
      "author": "/u/Stock-Loquat111",
      "published_date": "2026-01-04T02:53:52+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 136,
      "reading_time": 1,
      "created_at": "2026-01-05T01:57:59.966480+00:00",
      "updated_at": "2026-01-05T03:51:03.521038+00:00",
      "metadata": {
        "processed_at": "2026-01-05T03:51:03.521040+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "26f15f2dc00801d44e361a5bc13a4f96",
      "url": "https://www.dampfkraft.com/showa-100.html",
      "title": "The Showa Hundred Year Problem",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46440997\">Comments</a>",
      "author": "",
      "published_date": "2025-12-31T03:35:58+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-05T01:57:58.641175+00:00",
      "updated_at": "2026-01-05T03:51:03.521043+00:00",
      "metadata": {
        "processed_at": "2026-01-05T03:51:03.521044+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7170c824cb5edc82590e7ad1302327c4",
      "url": "https://warontherocks.com/2015/11/millennium-challenge-the-real-story-of-a-corrupted-military-exercise-and-its-legacy/",
      "title": "Millennium Challenge: A corrupted military exercise and its legacy (2015)",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46493623\">Comments</a>",
      "author": "",
      "published_date": "2026-01-04T23:43:48+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-05T01:57:58.641138+00:00",
      "updated_at": "2026-01-05T03:51:03.521047+00:00",
      "metadata": {
        "processed_at": "2026-01-05T03:51:03.521048+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}