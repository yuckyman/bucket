{
  "last_updated": "2025-10-15T01:07:46.445085+00:00",
  "pending_count": 963,
  "processed_count": 37,
  "pending_articles": [
    {
      "id": "24d3b7b86a24e9927fb04405c2c5149f",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1714129",
      "title": "Correction: Pathological respiratory chemoreflex activation predicts improvement of neurocognitive function in response to continuous positive airway pressure therapy",
      "content": "",
      "author": "Robert Joseph Thomas",
      "published_date": "2025-10-13T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-10-14T23:37:40.200978+00:00",
      "updated_at": "2025-10-14T23:37:40.200979+00:00"
    },
    {
      "id": "1d8d5e8cf0c2514bbeb45a8e0b9c28f5",
      "url": "http://ieeexplore.ieee.org/document/10856220",
      "title": "Editorial: Harnessing Reviews to Advance Biomedical Engineering's New Horizons",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:20.519171+00:00",
      "updated_at": "2025-10-14T23:19:20.519173+00:00"
    },
    {
      "id": "6071ce99ab68021ed48d4600bdeec843",
      "url": "http://ieeexplore.ieee.org/document/10856214",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:20.519153+00:00",
      "updated_at": "2025-10-14T23:19:20.519155+00:00"
    },
    {
      "id": "f18dbf7099a24df1b7e9875d0258e8eb",
      "url": "http://ieeexplore.ieee.org/document/10856213",
      "title": "IEEE Engineering in Medicine and Biology Society",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:20+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:20.519134+00:00",
      "updated_at": "2025-10-14T23:19:20.519136+00:00"
    },
    {
      "id": "a26bf16b66c97163157b9e77eece47f8",
      "url": "https://arxiv.org/abs/2510.10173",
      "title": "Chord Colourizer: A Near Real-Time System for Visualizing Musical Key",
      "content": "arXiv:2510.10173v1 Announce Type: new \nAbstract: This paper introduces Chord Colourizer, a near real-time system that detects the musical key of an audio signal and visually represents it through a novel graphical user interface (GUI). The system assigns colours to musical notes based on Isaac Newton's original colour wheel, preserving historical links between pitch and hue, and also integrates an Arduino-controlled LED display using 3D-printed star-shaped diffusers to offer a physical ambient media representation. The method employs Constant-Q Transform (CQT) chroma features for chord estimation and visualization, followed by threshold-based filtering and tonal enhancement to isolate the root, third, and fifth. A confidence score is computed for each detection to ensure reliability, and only chords with moderate to very strong certainty are visualized. The graphical interface dynamically updates a colour-coded keyboard layout, while the LED display provides the same colour information via spatial feedback. This multi-modal system enhances user interaction with harmonic content, offering innovative possibilities for education and artistic performance. Limitations include slight latency and the inability to detect extended chords, which future development will aim to address through refined filtering, adaptive thresholds, and support for more complex harmonies such as sevenths and augmented chords. Future work will also explore integration with alternative visualization styles, and the comparison of audio analysis libraries to improve detection speed and precision. Plans also include formal user testing to evaluate perception, usability, and cross-cultural interpretations of colour-pitch mappings.",
      "author": "Paul Haimes",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:15.826397+00:00",
      "updated_at": "2025-10-14T23:19:15.826399+00:00"
    },
    {
      "id": "a90200a0740732a66657cc732eea0d85",
      "url": "https://arxiv.org/abs/2510.10169",
      "title": "BrainForm: a Serious Game for BCI Training and Data Collection",
      "content": "arXiv:2510.10169v1 Announce Type: new \nAbstract: $\\textit{BrainForm}$ is a gamified Brain-Computer Interface (BCI) training system designed for scalable data collection using consumer hardware and a minimal setup. We investigated (1) how users develop BCI control skills across repeated sessions and (2) perceptual and performance effects of two visual stimulation textures. Game Experience Questionnaire (GEQ) scores for Flow}, Positive Affect, Competence and Challenge were strongly positive, indicating sustained engagement. A within-subject study with multiple runs, two task complexities, and post-session questionnaires revealed no significant performance differences between textures but increased ocular irritation over time. Online metrics$\\unicode{x2013}$Task Accuracy, Task Time, and Information Transfer Rate$\\unicode{x2013}$improved across sessions, confirming learning effects for symbol spelling, even under pressure conditions. Our results highlight the potential of $\\textit{BrainForm}$ as a scalable, user-friendly BCI research tool and offer guidance for sustained engagement and reduced training fatigue.",
      "author": "Michele Romani, Devis Zanoni, Elisabetta Farella, Luca Turchet",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 137,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:15.826362+00:00",
      "updated_at": "2025-10-14T23:19:15.826364+00:00"
    },
    {
      "id": "02d894c10dea13361198625d9ad89008",
      "url": "https://arxiv.org/abs/2510.10079",
      "title": "How AI Companionship Develops: Evidence from a Longitudinal Study",
      "content": "arXiv:2510.10079v1 Announce Type: new \nAbstract: The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.",
      "author": "Angel Hsing-Chi Hwang, Fiona Li, Jacy Reese Anthis, Hayoun Noh",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:15.826334+00:00",
      "updated_at": "2025-10-14T23:19:15.826336+00:00"
    },
    {
      "id": "8b50c49af24c77fed69cf1aab8ddd70f",
      "url": "https://arxiv.org/abs/2510.10049",
      "title": "ALLOY: Generating Reusable Agent Workflows from User Demonstration",
      "content": "arXiv:2510.10049v1 Announce Type: new \nAbstract: Large language models (LLMs) enable end-users to delegate complex tasks to autonomous agents through natural language. However, prompt-based interaction faces critical limitations: Users often struggle to specify procedural requirements for tasks, especially those that don't have a factually correct solution but instead rely on personal preferences, such as posting social media content or planning a trip. Additionally, a ''successful'' prompt for one task may not be reusable or generalizable across similar tasks. We present ALLOY, a system inspired by classical HCI theories on Programming by Demonstration (PBD), but extended to enhance adaptability in creating LLM-based web agents. ALLOY enables users to express procedural preferences through natural demonstrations rather than prompts, while making these procedures transparent and editable through visualized workflows that can be generalized across task variations. In a study with 12 participants, ALLOY's demonstration--based approach outperformed prompt-based agents and manual workflows in capturing user intent and procedural preferences in complex web tasks. Insights from the study also show how demonstration--based interaction complements the traditional prompt-based approach.",
      "author": "Jiawen Li, Zheng Ning, Yuan Tian, Toby Jia-jun Li",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 172,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:15.826302+00:00",
      "updated_at": "2025-10-14T23:19:15.826303+00:00"
    },
    {
      "id": "f15b692c58c4f6e66d11738e7c1f70c0",
      "url": "https://arxiv.org/abs/2510.10048",
      "title": "Between Knowledge and Care: Evaluating Generative AI-Based IUI in Type 2 Diabetes Management Through Patient and Physician Perspectives",
      "content": "arXiv:2510.10048v1 Announce Type: new \nAbstract: Generative AI systems are increasingly adopted by patients seeking everyday health guidance, yet their reliability and clinical appropriateness remain uncertain. Taking Type 2 Diabetes Mellitus (T2DM) as a representative chronic condition, this paper presents a two-part mixed-methods study that examines how patients and physicians in China evaluate the quality and usability of AI-generated health information. Study~1 analyzes 784 authentic patient questions to identify seven core categories of informational needs and five evaluation dimensions -- \\textit{Accuracy, Safety, Clarity, Integrity}, and \\textit{Action Orientation}. Study~2 involves seven endocrinologists who assess responses from four mainstream AI models across these dimensions. Quantitative and qualitative findings reveal consistent strengths in factual and lifestyle guidance but significant weaknesses in medication interpretation, contextual reasoning, and empathy. Patients view AI as an accessible ``pre-visit educator,'' whereas clinicians highlight its lack of clinical safety and personalization. Together, the findings inform design implications for interactive health systems, advocating for multi-model orchestration, risk-aware fallback mechanisms, and emotionally attuned communication to ensure trustworthy AI assistance in chronic disease care.",
      "author": "Yibo Meng, Ruiqi Chen, Zhiming Liu, Xiaolan Ding, Yan Guan",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:15.826272+00:00",
      "updated_at": "2025-10-14T23:19:15.826274+00:00"
    },
    {
      "id": "3653041d719657ec99cf54c85afa8073",
      "url": "https://arxiv.org/abs/2510.10019",
      "title": "\"Can I Decorate My Teeth With Diamonds?\": Exploring Multi-Stakeholder Perspectives on Using VR to Reduce Children's Dental Anxiety",
      "content": "arXiv:2510.10019v1 Announce Type: new \nAbstract: Dental anxiety is prevalent among children, often leading to missed treatment and potential negative effects on their mental well-being. While several interventions (e.g., pharmacological and psychotherapeutic techniques) have been introduced for anxiety alleviation, the recently emerged virtual reality (VR) technology, with its immersive and playful nature, opened new opportunities for complementing and enhancing the therapeutic effects of existing interventions. In this light, we conducted a series of co-design workshops with 13 children aged 10-12 to explore how they envisioned using VR to address their fear and stress associated with dental visits, followed by interviews with parents (n = 13) and two dentists. Our findings revealed that children expected VR to provide immediate relief, social support, and a sense of control during dental treatment, parents sought educational opportunities for their children to learn about oral health, and dentists prioritized treatment efficiency and safety issues. Drawing from the findings, we discuss the considerations of multi-stakeholders for developing VR-assisted anxiety management applications for children within and beyond dental settings.",
      "author": "Yaxuan Mao, Yanheng Li, Duo Gong, Pengcheng An, Yuhan Luo",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2025-10-14T23:19:15.826242+00:00",
      "updated_at": "2025-10-14T23:19:15.826243+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "78a543035cc17e2ac6f73ecd95cce1ae",
      "url": "http://ieeexplore.ieee.org/document/10750441",
      "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities and Future Directions",
      "content": "Foundation model, trained on a diverse range of data and adaptable to a myriad of tasks, is advancing healthcare. It fosters the development of healthcare artificial intelligence (AI) models tailored to the intricacies of the medical field, bridging the gap between limited AI models and the varied nature of healthcare practices. The advancement of a healthcare foundation model (HFM) brings forth tremendous potential to augment intelligent healthcare services across a broad spectrum of scenarios. However, despite the imminent widespread deployment of HFMs, there is currently a lack of clear understanding regarding their operation in the healthcare field, their existing challenges, and their future trajectory. To answer these critical inquiries, we present a comprehensive and in-depth examination that delves into the landscape of HFMs. It begins with a comprehensive overview of HFMs, encompassing their methods, data, and applications, to provide a quick understanding of the current progress. Subsequently, it delves into a thorough exploration of the challenges associated with data, algorithms, and computing infrastructures in constructing and widely applying foundation models in healthcare. Furthermore, this survey identifies promising directions for future development in this field. We believe that this survey will enhance the community's understanding of the current progress of HFMs and serve as a valuable source of guidance for future advancements in this domain.",
      "author": "",
      "published_date": "2024-11-12T13:16:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-10-14T23:38:08.210014+00:00",
      "updated_at": "2025-10-15T01:07:46.344852+00:00",
      "metadata": {
        "processed_at": "2025-10-15T01:07:46.344861+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f3ebee0a159c29e785b8640ab568613e",
      "url": "http://ieeexplore.ieee.org/document/10729663",
      "title": "Data- and Physics-Driven Deep Learning Based Reconstruction for Fast MRI: Fundamentals and Methodologies",
      "content": "Magnetic Resonance Imaging (MRI) is a pivotal clinical diagnostic tool, yet its extended scanning times often compromise patient comfort and image quality, especially in volumetric, temporal and quantitative scans. This review elucidates recent advances in MRI acceleration via data and physics-driven models, leveraging techniques from algorithm unrolling models, enhancement-based methods, and plug-and-play models to the emerging full spectrum of generative model-based methods. We also explore the synergistic integration of data models with physics-based insights, encompassing the advancements in multi-coil hardware accelerations like parallel imaging and simultaneous multi-slice imaging, and the optimization of sampling patterns. We then focus on domain-specific challenges and opportunities, including image redundancy exploitation, image integrity, evaluation metrics, data heterogeneity, and model generalization. This work also discusses potential solutions and future research directions, with an emphasis on the role of data harmonization and federated learning for further improving the general applicability and performance of these methods in MRI reconstruction.",
      "author": "",
      "published_date": "2024-10-22T13:18:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-14T23:38:08.209979+00:00",
      "updated_at": "2025-10-15T01:07:46.344865+00:00",
      "metadata": {
        "processed_at": "2025-10-15T01:07:46.344867+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b71ad97ebddb2087936b4010c1aaf456",
      "url": "http://ieeexplore.ieee.org/document/10746601",
      "title": "Artificial General Intelligence for Medical Imaging Analysis",
      "content": "Large-scale Artificial General Intelligence (AGI) models, including Large Language Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented success in a variety of general domain tasks. Yet, when applied directly to specialized domains like medical imaging, which require in-depth expertise, these models face notable challenges arising from the medical field's inherent complexities and unique characteristics. In this review, we delve into the potential applications of AGI models in medical imaging and healthcare, with a primary focus on LLMs, Large Vision Models, and Large Multimodal Models. We provide a thorough overview of the key features and enabling techniques of LLMs and AGI, and further examine the roadmaps guiding the evolution and implementation of AGI models in the medical sector, summarizing their present applications, potentialities, and associated challenges. In addition, we highlight potential future research directions, offering a holistic view on upcoming ventures. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare, and beyond.",
      "author": "",
      "published_date": "2024-11-07T13:17:37+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-14T23:38:08.209949+00:00",
      "updated_at": "2025-10-15T01:07:46.344869+00:00",
      "metadata": {
        "processed_at": "2025-10-15T01:07:46.344871+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3970a7e47edc49703b34feadbd5d1dab",
      "url": "http://ieeexplore.ieee.org/document/10720187",
      "title": "Exhaled Breath Analysis: From Laboratory Test to Wearable Sensing",
      "content": "Breath analysis and monitoring have emerged as pivotal components in both clinical research and daily health management, particularly in addressing the global health challenges posed by respiratory and metabolic disorders. The advancement of breath analysis strategies necessitates a multidisciplinary approach, seamlessly integrating expertise from medicine, biology, engineering, and materials science. Recent innovations in laboratory methodologies and wearable sensing technologies have ushered in an era of precise, real-time, and in situ breath analysis and monitoring. This comprehensive review elucidates the physical and chemical aspects of breath analysis, encompassing respiratory parameters and both volatile and non-volatile constituents. It emphasizes their physiological and clinical significance, while also exploring cutting-edge laboratory testing techniques and state-of-the-art wearable devices. Furthermore, the review delves into the application of sophisticated data processing technologies in the burgeoning field of breathomics and examines the potential of breath control in human-machine interaction paradigms. Additionally, it provides insights into the challenges of translating innovative laboratory and wearable concepts into mainstream clinical and daily practice. Continued innovation and interdisciplinary collaboration will drive progress in breath analysis, potentially revolutionizing personalized medicine through entirely non-invasive breath methodology.",
      "author": "",
      "published_date": "2024-10-16T13:15:55+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 182,
      "reading_time": 1,
      "created_at": "2025-10-14T23:38:08.209919+00:00",
      "updated_at": "2025-10-15T01:07:46.344874+00:00",
      "metadata": {
        "processed_at": "2025-10-15T01:07:46.344875+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "483769689d304d6940ab358e0b085a8c",
      "url": "http://ieeexplore.ieee.org/document/10771694",
      "title": "Earable Multimodal Sensing and Stimulation: A Prospective Toward Unobtrusive Closed-Loop Biofeedback",
      "content": "The human ear has emerged as a bidirectional gateway to the brain's and body's signals. Recent advances in around-the-ear and in-ear sensors have enabled the assessment of biomarkers and physiomarkers derived from brain and cardiac activity using ear-electroencephalography (ear-EEG), photoplethysmography (ear-PPG), and chemical sensing of analytes from the ear, with ear-EEG having been taken beyond-the-lab to outer space. Parallel advances in non-invasive and minimally invasive brain stimulation techniques have leveraged the ear's access to two cranial nerves to modulate brain and body activity. The vestibulocochlear nerve stimulates the auditory cortex and limbic system with sound, while the auricular branch of the vagus nerve indirectly but significantly couples to the autonomic nervous system and cardiac output. Acoustic and current mode stimuli delivered using discreet and unobtrusive earables are an active area of research, aiming to make biofeedback and bioelectronic medicine deliverable outside of the clinic, with remote and continuous monitoring of therapeutic responsivity and long-term adaptation. Leveraging recent advances in ear-EEG, transcutaneous auricular vagus nerve stimulation (taVNS), and unobtrusive acoustic stimulation, we review accumulating evidence that combines their potential into an integrated earable platform for closed-loop multimodal sensing and neuromodulation, towards personalized and holistic therapies that are near, in- and around-the-ear.",
      "author": "",
      "published_date": "2024-11-29T13:16:54+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2025-10-14T23:38:08.209886+00:00",
      "updated_at": "2025-10-15T01:07:46.344877+00:00",
      "metadata": {
        "processed_at": "2025-10-15T01:07:46.344879+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "05eb079cb3514dffc1e2eae0e167ba6f",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1557879",
      "title": "Leveraging neuroinformatics to understand cognitive phenotypes in elite athletes through systems neuroscience",
      "content": "IntroductionUnderstanding the cognitive phenotypes of elite athletes offers a unique perspective on the intricate interplay between neurological traits and high-performance behaviors. This study aligns with advancing neuroinformatics by proposing a novel framework designed to capture and analyze the multi-dimensional dependencies of cognitive phenotypes using systems neuroscience methodologies. Traditional approaches often face limitations in disentangling the latent factors influencing cognitive variability or in preserving interpretable data structures.MethodsTo address these challenges, we developed the Latent Cognitive Embedding Network (LCEN), an innovative model that combines biologically inspired constraints with state-of-the-art neural architectures. The model features a specialized embedding mechanism for disentangling latent factors and a tailored optimization strategy incorporating domain-specific priors and regularization techniques.ResultsExperimental evaluations demonstrate LCEN's superiority in predicting and interpreting cognitive phenotypes across diverse datasets, providing deeper insights into the neural underpinnings of elite performance.DiscussionThis work bridges computational modeling, neuroscience, and psychology, contributing to the broader understanding of cognitive variability in specialized populations.",
      "author": "Qi Yu",
      "published_date": "2025-08-19T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 152,
      "reading_time": 1,
      "created_at": "2025-10-14T21:38:43.095983+00:00",
      "updated_at": "2025-10-14T22:14:05.732149+00:00",
      "metadata": {
        "processed_at": "2025-10-14T22:14:05.732194+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3c1afbc01fc8fc7dcf8b6374918d2204",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1618050",
      "title": "Improving EEG classification of alcoholic and control subjects using DWT-CNN-BiGRU with various noise filtering techniques",
      "content": "Electroencephalogram (EEG) signal analysis plays a vital role in diagnosing and monitoring alcoholism, where accurate classification of individuals into alcoholic and control groups is essential. However, the inherent noise and complexity of EEG signals pose significant challenges. This study investigates the impact of three signal denoising techniques' Discrete Wavelet Transform(DWT), Discrete Fourier Transform(DFT), and Discrete Cosine Transform (DCT) Non EEG signal classification performance. The motivation behind this study is to identify the most effective preprocessing method for enhancing deep learning model performance in this domain. A novel DWT-CNN-BiGRU model is proposed, which leverages CNN layers for spatial feature extraction and BiGRU layers for capturing temporal dependencies. Experimental results show that the DWT-based approach, combined with standard scaling, achieves the highest accuracy of 94%, with a precision of 0.94, a recall of 0.95, and an F1-score of 0.94. Compared to the baseline DWT-CNN-BiLSTM model, the proposed method provides a modest yet meaningful improvement of approximately 17% in classification accuracy. These findings highlight the superiority of DWT as a preprocessing method and validate the proposed model's effectiveness for EEG-based classification, contributing to the development of more reliable medical diagnostic tools.",
      "author": "Swati Jain",
      "published_date": "2025-08-19T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 188,
      "reading_time": 1,
      "created_at": "2025-10-14T21:38:43.095951+00:00",
      "updated_at": "2025-10-14T22:14:05.732199+00:00",
      "metadata": {
        "processed_at": "2025-10-14T22:14:05.732201+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "c97b9afbe97173ed5109c3777893fab9",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1609077",
      "title": "Large language models can extract metadata for annotation of human neuroimaging publications",
      "content": "We show that recent (mid-to-late 2024) commercial large language models (LLMs) are capable of good quality metadata extraction and annotation with very little work on the part of investigators for several exemplar real-world annotation tasks in the neuroimaging literature. We investigated the GPT-4o LLM from OpenAI which performed comparably with several groups of specially trained and supervised human annotators. The LLM achieves similar performance to humans, between 0.91 and 0.97 on zero-shot prompts without feedback to the LLM. Reviewing the disagreements between LLM and gold standard human annotations we note that actual LLM errors are comparable to human errors in most cases, and in many cases these disagreements are not errors. Based on the specific types of annotations we tested, with exceptionally reviewed gold-standard correct values, the LLM performance is usable for metadata annotation at scale. We encourage other research groups to develop and make available more specialized \u201cmicro-benchmarks,\u201d like the ones we provide here, for testing both LLMs, and more complex agent systems annotation performance in real-world metadata annotation tasks.",
      "author": "Jessica A. Turner",
      "published_date": "2025-08-20T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2025-10-14T21:38:43.095920+00:00",
      "updated_at": "2025-10-14T22:14:05.732203+00:00",
      "metadata": {
        "processed_at": "2025-10-14T22:14:05.732204+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f5e32b36502a2bfe6ee22f7d18f1ecdb",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1628538",
      "title": "A correlation-based tool for quantifying membrane periodic skeleton associated periodicity",
      "content": "IntroductionThe advent of super-resolution microscopy revealed the membrane-associated periodic skeleton (MPS), a specialized neuronal cytoskeletal structure composed of actin rings spaced 190 nm apart by two spectrin dimers. While numerous ion channels, cell adhesion molecules, and signaling proteins have been shown to associate with the MPS, tools for accurate and unbiased quantification of their periodic localization remain scarce.MethodsWe developed Napari-WaveBreaker (https://github.com/SamKVs/napari-k2-WaveBreaker), an open-source plugin for the Napari image viewer. The tool quantifies MPS periodicity using autocorrelation and assesses periodic co-distribution between targets using cross-correlation. Performance was evaluated using both simulated datasets and STED microscopy images of periodic and non-periodic axonal proteins.ResultsNapari-WaveBreaker output parameters accurately reflected the visually observed periodicity and detected spatial shifts between two periodic targets. The approach was robust across varying image qualities and reliably distinguished periodic from non-periodic protein distributions.DiscussionNapari-WaveBreaker provides an unbiased, quantitative framework for analyzing MPS-associated periodicity and co-distribution enabling new insights into the molecular organization and modulation of the MPS.",
      "author": "Hanne B. Rasmussen",
      "published_date": "2025-08-22T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 156,
      "reading_time": 1,
      "created_at": "2025-10-14T21:38:43.095887+00:00",
      "updated_at": "2025-10-14T22:14:05.732207+00:00",
      "metadata": {
        "processed_at": "2025-10-14T22:14:05.732208+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "6be42688c92db7d5ef87c0d90ba2f2bb",
      "url": "https://www.benedelman.org/applovin-nonconsensual-installs/",
      "title": "AppLovin Nonconsensual Installs",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45584226\">Comments</a>",
      "author": "",
      "published_date": "2025-10-14T20:13:53+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-10-14T21:37:44.677758+00:00",
      "updated_at": "2025-10-14T22:14:05.732211+00:00",
      "metadata": {
        "processed_at": "2025-10-14T22:14:05.732213+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}