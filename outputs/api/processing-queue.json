{
  "last_updated": "2025-11-27T06:24:41.821958+00:00",
  "pending_count": 705,
  "processed_count": 295,
  "pending_articles": [
    {
      "id": "fa250840ddf6808c93613cad856c7c25",
      "url": "http://doi.org/10.1037/cns0000353",
      "title": "Unmuting lucid dreams: Speech decoding and vocalization in real time.",
      "content": "Since the 1970s, scientists have been searching for ways to communicate with people in lucid dreams (LDs), during which it is possible to maintain consciousness. Previously, dreamers could hear sounds from reality and respond with some simple signals, but they could not speak back. In this study, facial surface electromyography (EMG) was tested as a proof of concept for unmuting people in LDs. Remmyo, an EMG distinctive constructed language, was used. The software was developed to translate facial EMG impulses into Remmyo sounds and letters, translate words into English, and digitally vocalize the final text in English. Four LD practitioners were trained to pronounce a short phrase or a word in Remmyo and were then asked to achieve the same task in LDs under polysomnographic observation. LDs were verified by preagreed eye movements in rapid eye movement (REM) sleep. Four volunteers tried to speak in Remmyo in 15 LDs. Due to software failures, mispronunciations, and missing sounds, the decoding efficiency in real time or in recordings ranged from 13% to 81%. The first phrase and word heard from sleeping people were \u201cno war\u201d and \u201cfreedom.\u201d The later was automatically translated and vocalized in English in real time for 11 times. Despite controversial results, the study shows that, with further development, people could possibly talk in LDs and could be heard in reality with the help of EMG sensors. To achieve this goal, a range of possible obstacles is discussed. This technology could provide opportunities for LD studies and their practical applications. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2023-03-13T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 260,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241402+00:00",
      "updated_at": "2025-11-27T05:44:11.241405+00:00"
    },
    {
      "id": "3cace5c5d9bdc4eeebb05365c3e99538",
      "url": "http://doi.org/10.1037/cns0000402",
      "title": "Creating a world in the head: The conscious apprehension of neural content originating from internal sources.",
      "content": "Klein et al. (2023) argued that the evolutionary transition from respondent to agent during the Cambrian explosion would be a promising vantage point from which to gain insight into the evolution of organic sentience. They focused on how increased competition for resources\u2014in consequence of the proliferation of new, neurally sophisticated life-forms\u2014made awareness of the external world (in the service of agentic acts) an adaptive priority. The explanatory scope of Klein et al. (2023) was limited to consideration of the conscious apprehension of externally sourced content\u2014that is, content delivered from the sensory registration of objects occupying phenomenal space. But consciousness\u2014at least for humans\u2014takes its objects from internal as well as external sources. In the present article, we extend their analysis to the question of how internally sourced content (i.e., mental states) became the object of conscious apprehension. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2024-09-09T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 145,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241357+00:00",
      "updated_at": "2025-11-27T05:44:11.241359+00:00"
    },
    {
      "id": "4dc92cbf63e410e4d59f6ffd2f7dec90",
      "url": "http://doi.org/10.1037/cns0000406",
      "title": "Not all minds think alike: Examining the impact of time and task on visual and verbal thought.",
      "content": "Research suggests that individuals have different phenomenological experiences across various tasks. However, little is known about how these experiences vary by task or over time. This study examined participants\u2019 experiences of task-unrelated thoughts (i.e., TUTs), visual, and verbal thoughts across two experimental sessions and two different tasks. In addition, we examined relations between participants\u2019 thoughts and key individual difference factors. In Session 1, participants (<em>n</em> = 85) engaged in a focused-attention meditation and a reading task, then completed a second identical session with a new text. Throughout both tasks, participants were prompted to report on the characteristics of their thoughts. Participants\u2019 ratings of TUT, visual, and verbal thoughts were subject to change over time. Furthermore, on average, participants visualized more and had fewer TUTs while reading compared to meditation; however, no task difference was found for verbal-thinking reports. This suggests that visual imagery is more malleable than verbal-thinking. There was a strong negative correlation between visual and verbal thoughts, suggesting that at any given time, individuals\u2019 thoughts tended to be either predominantly visual or verbal. Finally, individual differences in the tendency to become immersed in narratives and motivation to engage with other people\u2019s perspectives (i.e., mind-reading motivation) were related to higher reports of visual imagery during reading, whereas verbal-thinking was negatively associated with mind-reading motivation and unrelated to TUT. Overall, this study revealed that individuals\u2019 phenomenological experiences vary during tasks and across time, providing a foundation for future work to examine why and how variability in these phenomenological experiences emerge. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2024-10-14T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 259,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241322+00:00",
      "updated_at": "2025-11-27T05:44:11.241324+00:00"
    },
    {
      "id": "c88e3d8b4ab22943bcc6cf02ea5b3b61",
      "url": "https://www.collaboraonline.com/blog/collabora-online-now-available-on-desktop/",
      "title": "Collabora Online Desktop Released with Improved UI from LibreOffice",
      "content": "<p>Article URL: <a href=\"https://www.collaboraonline.com/blog/collabora-online-now-available-on-desktop/\">https://www.collaboraonline.com/blog/collabora-online-now-available-on-desktop/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46064210\">https://news.ycombinator.com/item?id=46064210</a></p>\n<p>Points: 8</p>\n<p># Comments: 0</p>",
      "author": "nogajun",
      "published_date": "2025-11-27T00:59:58+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:01.054840+00:00",
      "updated_at": "2025-11-27T05:44:01.054842+00:00"
    },
    {
      "id": "eaac861dd37ec58db34c6268ac7c5602",
      "url": "https://arxiv.org/abs/2511.21000",
      "title": "PileUp: A Tufting Approach to Soft, Tactile, and Volumetric E-Textile Interfaces",
      "content": "arXiv:2511.21000v1 Announce Type: new \nAbstract: We present PileUp, a tufted pile e-textile sensing approach that offers unique affordances through the tactile expressiveness and richness of its continuous, threaded-volume construction. By integrating conductive yarns in looped or cut pile forms, PileUp transforms soft 3-dimensional textiles into multimodal sensors capable of detecting mechanical deformations such as pressure, bending, and strain, as well as environmental conditions like moisture. We propose a design space that outlines the relationships between texture, form factor, and sensing affordances of tufted textiles. We characterize electrical responses under compression, bending, and strain, reporting sensor behaviors. To demonstrate versatility, we present three application scenarios in which PileUp sensors are seamlessly integrated into soft fabrics: a meditation rug with multi-zone sensing, a fleece sleeve that detects arm motion, and a moisture-sensing wall art. Our results establish tufting as an accessible yet expressive fabrication method for creating integrated sensing textiles, distinguishing our work from traditional flat textile sensors.",
      "author": "Seoyoung Choi, Rashmi Balegar Mohan, Heather Jin Hee Kim, Jisoo Ha, Jeyeon Jo",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 156,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600890+00:00",
      "updated_at": "2025-11-27T05:22:22.600891+00:00"
    },
    {
      "id": "a2add133d5520496c5fa803121537354",
      "url": "https://arxiv.org/abs/2511.20791",
      "title": "Beyond the Legal Lens: A Sociotechnical Taxonomy of Lived Privacy Incidents and Harms",
      "content": "arXiv:2511.20791v1 Announce Type: new \nAbstract: To understand how privacy incidents lead to harms, HCI researchers have historically leveraged legal frameworks. However, these frameworks expect acute, tangible harms and thus may not cover the full range of human experience relevant to modern-day digital privacy. To address this gap, our research builds upon these existing frameworks to develop a more comprehensive representation of people's lived experiences with privacy harms. We analyzed 369 privacy incidents reported by individuals from the general public. We found a broader range of privacy incidents and harms than accounted for in existing legal frameworks. The majority of reported privacy harms were not based on tangible harm, but on fear and loss of psychological safety. We also characterize the actors, motives, and information associated with various incidents. This work contributes a new framework for understanding digital privacy harms that can be utilized both in research and practice.",
      "author": "Kirsten Chapman, Garrett Smith, Kaitlyn Klabacka, Harrison Winslow, Louise Barkhuus, Cori Faklaris, Sauvik Das, Pamela Wisniewski, Bart Piet Knijnenburg, Heather Lipford, Xinru Page",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 148,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600860+00:00",
      "updated_at": "2025-11-27T05:22:22.600862+00:00"
    },
    {
      "id": "292b88c6e85f21229007b0c743726ef5",
      "url": "https://arxiv.org/abs/2511.20660",
      "title": "Transforming Higher Education with AI-Powered Video Lectures",
      "content": "arXiv:2511.20660v1 Announce Type: new \nAbstract: The integration of artificial intelligence (AI) into video lecture production has the potential to transform higher education by streamlining content creation and enhancing accessibility. This paper investigates a semi automated workflow that combines Google Gemini for script generation, Amazon Polly for voice synthesis, and Microsoft PowerPoint for video assembly. Unlike fully automated text to video platforms, this hybrid approach preserves pedagogical intent while ensuring script to slide synchronization, narrative coherence, and customization. Case studies demonstrate the effectiveness of Gemini in generating accurate and context-sensitive scripts for visually rich academic presentations, while Polly provides natural-sounding narration with controllable pacing. A two course pilot study was conducted to evaluate AI generated instructional videos (AIIV) against human instructional videos (HIV). Both qualitative and quantitative results indicate that AIIVs are comparable to HIVs in terms of learning outcomes, with students reporting high levels of clarity, coherence, and usability. However, limitations remain, particularly regarding audio quality and the absence of human-like avatars. The findings suggest that AI assisted video production can reduce instructor workload, improve scalability, and deliver effective learning resources, while future improvements in synthetic voices and avatars may further enhance learner engagement.",
      "author": "Dengsheng Zhang",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 194,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600832+00:00",
      "updated_at": "2025-11-27T05:22:22.600834+00:00"
    },
    {
      "id": "b2b929e629b19ee72ecef79bdd055d3e",
      "url": "https://arxiv.org/abs/2511.20659",
      "title": "Iteration and Co-design of a Physical Web Application for Outdoor Activities with Older Adults",
      "content": "arXiv:2511.20659v1 Announce Type: new \nAbstract: Existing research and physical activity guidelines highlight the benefits of outdoor physical activities for ageing populations. There is potential for technology to facilitate outdoor activity through Physical Web infrastructure. We proposed that embedding Physical Web applications that are engaging and interactive in public open spaces as part of interactive wellness parks can encourage older adults to participate in physical activities outdoors and motivate rehabilitation. We have created an initial design prototype based on design requirements generated from a qualitative field study with 24 older adults to explore their perceptions, experiences, and routines of outdoor physical activities. In this paper, we present an initial prototype and findings from a co-design session with 12 older adults, eliciting their feedback on the design and their ideas for future design iterations.",
      "author": "Fatima Badmos, Emma Murphy, Michael Ward, Damon Berry",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 132,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600800+00:00",
      "updated_at": "2025-11-27T05:22:22.600801+00:00"
    },
    {
      "id": "e0b602457ea2e79ac669aac107f0817e",
      "url": "https://arxiv.org/abs/2511.20657",
      "title": "Intelligent Agents with Emotional Intelligence: Current Trends, Challenges, and Future Prospects",
      "content": "arXiv:2511.20657v1 Announce Type: new \nAbstract: The development of agents with emotional intelligence is becoming increasingly vital due to their significant role in human-computer interaction and the growing integration of computer systems across various sectors of society. Affective computing aims to design intelligent systems that can recognize, evoke, and express human emotions, thereby emulating human emotional intelligence. While previous reviews have focused on specific aspects of this field, there has been limited comprehensive research that encompasses emotion understanding, elicitation, and expression, along with the related challenges. This survey addresses this gap by providing a holistic overview of core components of artificial emotion intelligence. It covers emotion understanding through multimodal data processing, as well as affective cognition, which includes cognitive appraisal, emotion mapping, and adaptive modulation in decision-making, learning, and reasoning. Additionally, it addresses the synthesis of emotional expression across text, speech, and facial modalities to enhance human-agent interaction. This paper identifies and analyzes the key challenges and issues encountered in the development of affective systems, covering state-of-the-art methodologies designed to address them. Finally, we highlight promising future directions, with particular emphasis on the potential of generative technologies to advance affective computing.",
      "author": "Raziyeh Zall, Alireza Kheyrkhah, Erik Cambria, Zahra Naseri, M. Reza Kangavari",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 190,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600764+00:00",
      "updated_at": "2025-11-27T05:22:22.600766+00:00"
    },
    {
      "id": "a72601fde4a9ad0eaa6844b5ec47b8b8",
      "url": "https://arxiv.org/abs/2511.20656",
      "title": "Context-Aware Visual Prompting: Automating Geospatial Web Dashboards with Large Language Models and Agent Self-Validation for Decision Support",
      "content": "arXiv:2511.20656v1 Announce Type: new \nAbstract: The development of web-based geospatial dashboards for risk analysis and decision support is often challenged by the difficulty in visualization of big, multi-dimensional environmental data, implementation complexity, and limited automation. We introduce a generative AI framework that harnesses Large Language Models (LLMs) to automate the creation of interactive geospatial dashboards from user-defined inputs including UI wireframes, requirements, and data sources. By incorporating a structured knowledge graph, the workflow embeds domain knowledge into the generation process and enable accurate and context-aware code completions. A key component of our approach is the Context-Aware Visual Prompting (CAVP) mechanism, which extracts encodes and interface semantics from visual layouts to guide LLM driven generation of codes. The new framework also integrates a self-validation mechanism that uses an agent-based LLM and Pass@k evaluation alongside semantic metrics to assure output reliability. Dashboard snippets are paired with data visualization codebases and ontological representations, enabling a pipeline that produces scalable React-based completions using the MVVM architectural pattern. Our results demonstrate improved performance over baseline approaches and expanded functionality over third party platforms, while incorporating multi-page, fully functional interfaces. We successfully developed a framework to implement LLMs, demonstrated the pipeline for automated code generation, deployment, and performed chain-of-thought AI agents in self-validation. This integrative approach is guided by structured knowledge and visual prompts, providing an innovative geospatial solution in enhancing risk analysis and decision making.",
      "author": "Haowen Xu, Jose Tupayachi, Xiao-Ying Yu",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 230,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600732+00:00",
      "updated_at": "2025-11-27T05:22:22.600733+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "a8adb8e21ba8ade167865b32c18747a2",
      "url": "http://ieeexplore.ieee.org/document/11235877",
      "title": "Separate Timescales for Spatial and Anatomical Information Processing of Body Stimuli",
      "content": "Observing different body stimuli can influence the speed and accuracy of our responses. Prior work indicates this effect is influenced by factors such as spatial congruence and perspective. We hypothesized that the influence of these factors would vary depending on the amount of time that participants had to process visual stimuli. Experiment 1 was a RT task (n = 29) with stimuli varying in spatial congruence (congruent, incongruent, neutral), perspective (first- or third-person), and stimulus type (body or control). Experiment 2 (n = 50) used the same stimuli in a \u201cForced Response\u201d paradigm, which controlled the time participants had to prepare a response. This allowed us to assess responses as a function of preparation time. Experiment 1 showed effects of spatial congruence, with longer RTs and more errors for spatially incongruent stimuli. This effect was greater for body stimuli. Experiment 2 showed that spatial information was processed faster than anatomical information, inducing incorrect responses at short preparation times for spatially incongruent body stimuli. There was little-to-no corresponding effect for control stimuli. Both experiments also showed weak-to-no effects of perspective, which appear to have been driven by spatial congruence. Our results indicate that spatial information is processed faster than anatomical information during observation of body stimuli. These data are consistent with the dual visual streams hypothesis, whereby spatial information would be processed rapidly via the dorsal stream, whereas anatomical processing would occur later via the ventral stream. These data also indicate differences in processing between body and control stimuli.",
      "author": "",
      "published_date": "2025-11-07T13:16:23+00:00",
      "source": "Cognitive Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 248,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:55.672821+00:00",
      "updated_at": "2025-11-27T06:24:41.729563+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729573+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "fb588439a33dbfe3eb0b3edf79a54446",
      "url": "http://ieeexplore.ieee.org/document/11235874",
      "title": "Transient Inhibition of the Posterior Parietal Cortex Affects Action-related But Not Action-unrelated Visual Processing during Path Integration",
      "content": "Path integration refers to the ability to monitor self-motion cues to keep track of changes in position and orientation. This function is often assumed to rely predominantly on medial temporal lobe structures containing grid, place, and head direction cells. Recent evidence, however, suggests that key navigational computations may occur outside this system, for example, in posterior parietal areas. Here, we adopted a novel perspective derived from animal research and examined whether human path integration relies on processing streams in the posterior parietal cortex (PPC), depending on the involvement of actively controlled motion as opposed to passive perception of visual optic flow. We compared the effects of inhibiting the PPC via TMS on two path integration tasks in a virtual reality, only one of which involved active control of a visually simulated forward movement. Behavioral performance showed that distance judgments were selectively affected in the action-related path integration task. This finding shows that the processing of actively controlled motion depends on computations in the PPC, whereas passive processing of optic flow is largely independent of the PPC computations. Our results reinforce the hypothesis that the PPC plays a critical role for the integration of goal locations and self-positional signals within an egocentric frame of reference. In addition to the medial temporal lobe, the posterior parietal system is recruited during tasks involving actively controlled movements, whereas medial temporal computations are sufficient for passive monitoring of positional changes.",
      "author": "",
      "published_date": "2025-11-07T13:16:23+00:00",
      "source": "Cognitive Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 235,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:55.672770+00:00",
      "updated_at": "2025-11-27T06:24:41.729577+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729578+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2b015497e36bc6db03bb62ed1bfea673",
      "url": "https://www.sciencedirect.com/science/article/pii/S0006899325006031?dgcid=rss_sd_all",
      "title": "Mitochondria serve as indispensable components of neuron-glia crosstalk in the trajectory of Alzheimer\u2019s disease",
      "content": "<p>Publication date: 1 January 2026</p><p><b>Source:</b> Brain Research, Volume 1870</p><p>Author(s): Maryam Sardari, Oveis Hosseinzadeh Sahafi, Ameneh Rezayof</p>",
      "author": "",
      "published_date": null,
      "source": "Brain Research",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 16,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:46.479425+00:00",
      "updated_at": "2025-11-27T06:24:41.729583+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729585+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b84f4acfaa385c55c9bcc74850be8c16",
      "url": "http://doi.org/10.1037/cns0000380",
      "title": "Sensory-processing sensitivity as a confounder in the positive relationship between mindful awareness and psychological distress: A theoretical review.",
      "content": "Mindfulness meditation is credited as a positive driver of promoting psychological well-being and reducing stress, anxiety, and depression symptoms. However, dispositional mindfulness has been somewhat correlated with psychological distress, as awareness has been positively correlated with psychological symptoms and negative affective states in many studies. This counterintuitive phenomenon has been tentatively explained in a variety of ways, including a wrong interpretation of the items of the mindfulness assessment scales in nonmeditators. The most credited explanation is that increasing attention to present-moment experiences would boost affective reaction to negative experiences and therefore exacerbate related psychological symptoms. This hypothesis is unsatisfactory, as there is much contrasting evidence in this regard. Therefore, we propose a new hypothesis: in dispositional studies, the assessment of the awareness skill of mindfulness would be affected by sensory-processing sensitivity, which could be a confounder in its relationship with psychological distress. Sensory-processing sensitivity refers to a temperamental trait characterized by both awareness of sensorial stimulation and reactivity to experience. Thus, highly sensitive persons usually report increased awareness of subtleties in the environment, ease of overstimulation, and increased affective reaction to stimulation. In support of our hypothesis, we showed in particular how the most widely used scale for assessing mindful awareness could be paired with and interpreted as a measure of sensory-processing sensitivity. We then propose a set of testable hypotheses to drive future research on this topic. If supported by future experimental results, our hypothesis would shed new light on the overall field of dispositional mindfulness studies. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2023-11-02T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 257,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241481+00:00",
      "updated_at": "2025-11-27T06:24:41.729587+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729588+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a89cc9d9bab0838b2e06072add1ef2ed",
      "url": "http://doi.org/10.1037/cns0000335",
      "title": "A shared perceptual inference for cross-modally induced illusions of self-attribution.",
      "content": "The representation of our own body is malleable. Evidence indicates that multisensory stimulation can trigger an illusory sense of ownership over a fake hand, a partner\u2019s face, or a virtual body. Despite our understanding of the processes supporting the construction of bodily self, we know less about the processes that trigger illusory ownership of nonbody attributes (e.g., voice during articulation) and about whether multisensory stimulation can drive a shared inference across distinct attributes. Here, we compared the classic rubber hand illusion with another multisensory illusion that elicits a sense of ownership over a stranger\u2019s voice during talking. We observed that, given congruent multisensory input, the degree to which one perceived the sense of ownership over the fake hand predicted the degree to which one perceived the sense of ownership over the stranger\u2019s voice, after controlling for task demand and suggestibility. Thus, our results provide evidence for a shared inference supporting subjective sense of self across fundamentally different attributes. We suggest that individual reliance on multisensory signals to drive such an inference can be further explored. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2022-08-25T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 184,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241440+00:00",
      "updated_at": "2025-11-27T06:24:41.729591+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729592+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "880067d85348be61dd8552eba4cb0801",
      "url": "https://www.biorxiv.org/content/10.1101/2025.11.24.690225v1?rss=1",
      "title": "In search for the invisible: motor inhibition in monkey premotor cortex and its RNN replicas",
      "content": "Controlling actions in dynamically changing environments requires flexible and efficient motor control. A fundamental challenge in neuroscience is to uncover how cortical circuits generate, adjust, and sometimes suppress planned movements. To address this, we combined recordings from dorsal premotor cortex (PMd) of macaque monkeys performing a stop-signal task, with a recurrent neural network (RNN) model inferred directly from multi-unit activity. This data-driven \"digital twin\" reproduced the cortical population dynamics underlying motor planning and inhibition, revealing how internal network states shape behavior, and generating synthetic neural trajectories for unseen conditions. RNN internal state explained reaction time fluctuations across trials, reflecting stochastic components of motor readiness and endogenous variability of PMd activity. The same pre-Go latent state also constrained movement inhibition modulating the network's response to Stop signals by reshaping the attractor dynamics of a null-potent subspace. These results establish a mechanistic link between latent cortical dynamics and flexible behavioral control, demonstrating how autonomous RNN inference can uncover circuit-level computations.",
      "author": "Albore, N., Galluzzi, A., Pani, P., Ferraina, S., Mattia, M.",
      "published_date": "2025-11-26T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-11-27T03:56:22.057442+00:00",
      "updated_at": "2025-11-27T04:19:22.977101+00:00",
      "metadata": {
        "processed_at": "2025-11-27T04:19:22.977110+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "5c4d686ee2758200da55417f8978e830",
      "url": "https://www.reddit.com/r/Python/comments/1p7api3/handling_multiple_alembic_migrations_with_a_full/",
      "title": "Handling multiple Alembic migrations with a full team of developers?",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>This has been frustration at its best. We have a team of 10 developers all working on the same codebase. When one person updates or adds a column to their local database we get a revision. However if multiple do so we have multiple revisions so which one is the HEAD? this is costly, time consuming and a bunch of mess. </p> <p>How would you or are you handling this type of use case? I get it Alembic works good if its a sole developer handing it off to another developer and its a one off, but with multiple devs all checking in code this is a headache. </p> <p>Back in the days of SQl we had normal SQL scripts with table updates that would just be appended to. No need for Heads or revisions. It just worked</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Big-Information3242\"> /u/Big-Information3242 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1p7api3/handling_multiple_alembic_migrations_with_a_full/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1p7api3/handling_multiple_alembic_migrations_with_a_full/\">[comments]</a></span>",
      "author": "/u/Big-Information3242",
      "published_date": "2025-11-26T15:32:00+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-11-27T03:55:46.073887+00:00",
      "updated_at": "2025-11-27T04:19:22.977114+00:00",
      "metadata": {
        "processed_at": "2025-11-27T04:19:22.977117+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7dedb2d2d771597940c3732621ba5c45",
      "url": "https://github.com/penpot/penpot",
      "title": "Penpot: The Open-Source Figma",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46064757\">Comments</a>",
      "author": "",
      "published_date": "2025-11-27T02:14:36+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-27T03:55:44.877358+00:00",
      "updated_at": "2025-11-27T04:19:22.977119+00:00",
      "metadata": {
        "processed_at": "2025-11-27T04:19:22.977121+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7dedb2d2d771597940c3732621ba5c45",
      "url": "https://github.com/penpot/penpot",
      "title": "Penpot: The Open-Source Figma",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46064757\">Comments</a>",
      "author": "",
      "published_date": "2025-11-27T02:14:36+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-27T03:55:44.877358+00:00",
      "updated_at": "2025-11-27T04:19:22.977119+00:00",
      "metadata": {
        "processed_at": "2025-11-27T04:19:22.977121+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2a52d1d0b849980dd935afba2dc27c0a",
      "url": "https://blog.briancmoses.com/2025/11/diy-nas-2026-edition.html",
      "title": "DIY NAS: 2026 Edition",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46065034\">Comments</a>",
      "author": "",
      "published_date": "2025-11-27T02:54:23+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-27T03:55:44.877318+00:00",
      "updated_at": "2025-11-27T04:19:22.977123+00:00",
      "metadata": {
        "processed_at": "2025-11-27T04:19:22.977125+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}