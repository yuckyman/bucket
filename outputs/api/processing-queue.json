{
  "last_updated": "2025-09-26T16:18:00.559469+00:00",
  "pending_count": 969,
  "processed_count": 31,
  "pending_articles": [
    {
      "id": "76ccd150fc9727dc042db56a71cf117f",
      "url": "https://arxiv.org/abs/2509.20731",
      "title": "Imagining Design Workflows in Agentic AI Futures",
      "content": "arXiv:2509.20731v1 Announce Type: new \nAbstract: As designers become familiar with Generative AI, a new concept is emerging: Agentic AI. While generative AI produces output in response to prompts, agentic AI systems promise to perform mundane tasks autonomously, potentially freeing designers to focus on what they love: being creative. But how do designers feel about integrating agentic AI systems into their workflows? Through design fiction, we investigated how designers want to interact with a collaborative agentic AI platform. Ten professional designers imagined and discussed collaborating with an AI agent to organise inspiration sources and ideate. Our findings highlight the roles AI agents can play in supporting designers, the division of authority between humans and AI, and how designers' intent can be explained to AI agents beyond prompts. We synthesise our findings into a conceptual framework that identifies authority distribution among humans and AI agents and discuss directions for utilising AI agents in future design workflows.",
      "author": "Samangi Wadinambiarachchi, Jenny Waycott, Yvonne Rogers, Greg Wadley",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570320+00:00",
      "updated_at": "2025-09-26T15:40:32.570322+00:00"
    },
    {
      "id": "3193cc01e650ebe889084a39dc561f20",
      "url": "https://arxiv.org/abs/2509.20666",
      "title": "Understanding Mode Switching in Human-AI Collaboration: Behavioral Insights and Predictive Modeling",
      "content": "arXiv:2509.20666v1 Announce Type: new \nAbstract: Human-AI collaboration is typically offered in one of two of user control levels: guidance, where the AI provides suggestions and the human makes the final decision, and delegation, where the AI acts autonomously within user-defined constraints. Systems that integrate both modes, common in robotic surgery or driving assistance, often overlook shifts in user preferences within a task in response to factors like evolving trust, decision complexity, and perceived control. In this work, we investigate how users dynamically switch between higher and lower levels of control during a sequential decision-making task. Using a hand-and-brain chess setup, participants either selected a piece and the AI decided how it moved (brain mode), or the AI selected a piece and the participant decided how it moved (hand mode). We collected over 400 mode-switching decisions from eight participants, along with gaze, emotional state, and subtask difficulty data. Statistical analysis revealed significant differences in gaze patterns and subtask complexity prior to a switch and in the quality of the subsequent move. Based on these results, we engineered behavioral and task-specific features to train a lightweight model that predicted control level switches ($F1 = 0.65$). The model performance suggests that real-time behavioral signals can serve as a complementary input alongside system-driven mode-switching mechanisms currently used. We complement our quantitative results with qualitative factors that influence switching including perceived AI ability, decision complexity, and level of control, identified from post-game interview analysis. The combined behavioral and modeling insights can help inform the design of shared autonomy systems that need dynamic, subtask-level control switches aligned with user intent and evolving task demands.",
      "author": "Avinash Ajit Nargund, Arthur Caetano, Kevin Yang, Rose Yiwei Liu, Philip Tezaur, Kriteen Shrestha, Qisen Pan, Tobias H\\\"ollerer, Misha Sra",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 268,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570291+00:00",
      "updated_at": "2025-09-26T15:40:32.570293+00:00"
    },
    {
      "id": "1d16aa79e82b67ae2db4ab846d20632b",
      "url": "https://arxiv.org/abs/2509.20571",
      "title": "MechStyle: Augmenting Generative AI with Mechanical Simulation to Create Stylized and Structurally Viable 3D Models",
      "content": "arXiv:2509.20571v1 Announce Type: new \nAbstract: Recent developments in Generative AI enable creators to stylize 3D models based on text prompts. These methods change the 3D model geometry, which can compromise the model's structural integrity once fabricated. We present MechStyle, a system that enables creators to stylize 3D printable models while preserving their structural integrity. MechStyle accomplishes this by augmenting the Generative AI-based stylization process with feedback from a Finite Element Analysis (FEA) simulation. As the stylization process modifies the geometry to approximate the desired style, feedback from the FEA simulation reduces modifications to regions with increased stress. We evaluate the effectiveness of FEA simulation feedback in the augmented stylization process by comparing three stylization control strategies. We also investigate the time efficiency of our approach by comparing three adaptive scheduling strategies. Finally, we demonstrate MechStyle's user interface that allows users to generate stylized and structurally viable 3D models and provide five example applications.",
      "author": "Faraz Faruqi, Amira Abdel-Rahman, Leandra Tejedor, Martin Nisser, Jiaji Li, Vrushank Phadnis, Varun Jampani, Neil Gershenfeld, Megan Hofmann, Stefanie Mueller",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570251+00:00",
      "updated_at": "2025-09-26T15:40:32.570252+00:00"
    },
    {
      "id": "031e1f278d4efe91fb66307ec8481a9e",
      "url": "https://arxiv.org/abs/2509.20553",
      "title": "Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation",
      "content": "arXiv:2509.20553v1 Announce Type: new \nAbstract: Recent advances in multi-agent systems (MAS) enable tools for information search and ideation by assigning personas to agents. However, how users can effectively control, steer, and critically evaluate collaboration among multiple domain-expert agents remains underexplored. We present Perspectra, an interactive MAS that visualizes and structures deliberation among LLM agents via a forum-style interface, supporting @-mention to invite targeted agents, threading for parallel exploration, with a real-time mind map for visualizing arguments and rationales. In a within-subjects study with 18 participants, we compared Perspectra to a group-chat baseline as they developed research proposals. Our findings show that Perspectra significantly increased the frequency and depth of critical-thinking behaviors, elicited more interdisciplinary replies, and led to more frequent proposal revisions than the group chat condition. We discuss implications for designing multi-agent tools that scaffold critical thinking by supporting user control over multi-agent adversarial discourse.",
      "author": "Yiren Liu, Viraj Shah, Sangho Suh, Pao Siangliulue, Tal August, Yun Huang",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570220+00:00",
      "updated_at": "2025-09-26T15:40:32.570222+00:00"
    },
    {
      "id": "392dc948d96793dc59ca50fb7c48e04c",
      "url": "https://arxiv.org/abs/2509.20512",
      "title": "CHOIR: A Chatbot-mediated Organizational Memory Leveraging Communication in University Research Labs",
      "content": "arXiv:2509.20512v1 Announce Type: new \nAbstract: University research labs often rely on chat-based platforms for communication and project management, where valuable knowledge surfaces but is easily lost in message streams. Documentation can preserve knowledge, but it requires ongoing maintenance and is challenging to navigate. Drawing on formative interviews that revealed organizational memory challenges in labs, we designed CHOIR, an LLM-based chatbot that supports organizational memory through four key functions: document-grounded Q&amp;A, Q&amp;A sharing for follow-up discussion, knowledge extraction from conversations, and AI-assisted document updates. We deployed CHOIR in four research labs for one month (n=21), where the lab members asked 107 questions and lab directors updated documents 38 times in the organizational memory. Our findings reveal a privacy-awareness tension: questions were asked privately, limiting directors' visibility into documentation gaps. Students often avoided contribution due to challenges in generalizing personal experiences into universal documentation. We contribute design implications for privacy-preserving awareness and supporting context-specific knowledge documentation.",
      "author": "Sangwook Lee, Adnan Abbas, Yan Chen, Young-Ho Kim, Sang Won Lee",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570186+00:00",
      "updated_at": "2025-09-26T15:40:32.570190+00:00"
    },
    {
      "id": "424fd3a4970e25b0acbcf5e5fe47946d",
      "url": "https://arxiv.org/abs/2508.16509",
      "title": "ML-PWS: Estimating the Mutual Information Between Experimental Time Series Using Neural Networks",
      "content": "arXiv:2508.16509v2 Announce Type: replace-cross \nAbstract: The ability to quantify information transmission is crucial for the analysis and design of natural and engineered systems. The information transmission rate is the fundamental measure for systems with time-varying signals, yet computing it is extremely challenging. In particular, the rate cannot be obtained directly from experimental time-series data without approximations, because of the high dimensionality of the signal trajectory space. Path Weight Sampling (PWS) is a computational technique that makes it possible to obtain the information rate exactly for any stochastic system. However, it requires a mathematical model of the system of interest, be it described by a master equation or a set of differential equations. Here, we present a technique that employs Machine Learning (ML) to develop a generative model from experimental time-series data, which is then combined with PWS to obtain the information rate. We demonstrate the accuracy of this technique, called ML-PWS, by comparing its results on synthetic time-series data generated from a non-linear model against ground-truth results obtained by applying PWS directly to the same model. We illustrate the utility of ML-PWS by applying it to neuronal time-series data.",
      "author": "Manuel Reinhardt, Ga\\v{s}per Tka\\v{c}ik, Pieter Rein ten Wolde",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:31.471108+00:00",
      "updated_at": "2025-09-26T15:40:31.471110+00:00"
    },
    {
      "id": "b35d4e4ccf288a46cb6efcb7cf5f20f8",
      "url": "https://arxiv.org/abs/2502.01360",
      "title": "A Quotient Homology Theory of Representation in Neural Networks",
      "content": "arXiv:2502.01360v3 Announce Type: replace-cross \nAbstract: Previous research has proven that the set of maps implemented by neural networks with a ReLU activation function is identical to the set of piecewise linear continuous maps. Furthermore, such networks induce a hyperplane arrangement splitting the input domain of the network into convex polyhedra $G_J$ over which a network $\\Phi$ operates in an affine manner.\n  In this work, we leverage these properties to define an equivalence class $\\sim_\\Phi$ on top of an input dataset, which can be split into two sets related to the local rank of $\\Phi_J$ and the intersections $\\cap \\text{Im}\\Phi_{J_i}$. We refer to the latter as the \\textit{overlap decomposition} $\\mathcal{O}_\\Phi$ and prove that if the intersections between each polyhedron and an input manifold are convex, the homology groups of neural representations are isomorphic to quotient homology groups $H_k(\\Phi(\\mathcal{M})) \\simeq H_k(\\mathcal{M}/\\mathcal{O}_\\Phi)$. This lets us intrinsically calculate the Betti numbers of neural representations without the choice of an external metric. We develop methods to numerically compute the overlap decomposition through linear programming and a union-find algorithm.\n  Using this framework, we perform several experiments on toy datasets showing that, compared to standard persistent homology, our overlap homology-based computation of Betti numbers tracks purely topological rather than geometric features. Finally, we study the evolution of the overlap decomposition during training on several classification problems while varying network width and depth and discuss some shortcomings of our method.",
      "author": "Kosio Beshkov",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 232,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:31.471076+00:00",
      "updated_at": "2025-09-26T15:40:31.471078+00:00"
    },
    {
      "id": "4acad20b64b64698b31230a66720d623",
      "url": "https://arxiv.org/abs/2507.16080",
      "title": "Interpretable Embeddings of Speech Enhance and Explain Brain Encoding Performance of Audio Models",
      "content": "arXiv:2507.16080v2 Announce Type: replace \nAbstract: Speech foundation models (SFMs) are increasingly hailed as powerful computational models of human speech perception. However, since their representations are inherently black-box, it remains unclear what drives their alignment with brain responses. To remedy this, we built linear encoding models from six interpretable feature families: mel-spectrogram, Gabor filter bank features, speech presence, phonetic, syntactic, and semantic features, and contextualized embeddings from three state-of-the-art SFMs (Whisper, HuBERT, WavLM), quantifying electrocorticography (ECoG) response variance shared between feature classes. Variance-partitioning analyses revealed several key insights: First, the SFMs' alignment with the brain can be mostly explained by their ability to learn and encode simple interpretable speech features. Second, SFMs exhibit a systematic trade-off between encoding of brain-relevant low-level and high-level features across layers. Finally, our results show that SFMs learn brain-relevant semantics which cannot be explained by lower-level speech features, with this capacity increasing with model size and context length. Together, our findings suggest a principled approach to build more interpretable, accurate, and efficient encoding models of the brain by augmenting SFM embeddings with interpretable features.",
      "author": "Riki Shimizu, Richard J. Antonello, Chandan Singh, Nima Mesgarani",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 178,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:31.471041+00:00",
      "updated_at": "2025-09-26T15:40:31.471043+00:00"
    },
    {
      "id": "c20c73f6e85bc427d5f2c976351e99c6",
      "url": "https://arxiv.org/abs/2509.20916",
      "title": "Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density",
      "content": "arXiv:2509.20916v1 Announce Type: cross \nAbstract: This study examines whether sentence-level memory load in comprehension is better explained by linear proximity between syntactically related words or by the structural density of the intervening material. Building on locality-based accounts and cross-linguistic evidence for dependency length minimization, the work advances Intervener Complexity-the number of intervening heads between a head and its dependent-as a structurally grounded lens that refines linear distance measures. Using harmonized dependency treebanks and a mixed-effects framework across multiple languages, the analysis jointly evaluates sentence length, dependency length, and Intervener Complexity as predictors of the Memory-load measure. Studies in Psycholinguistics have reported the contributions of feature interference and misbinding to memory load during processing. For this study, I operationalized sentence-level memory load as the linear sum of feature misbinding and feature interference for tractability; current evidence does not establish that their cognitive contributions combine additively. All three factors are positively associated with memory load, with sentence length exerting the broadest influence and Intervener Complexity offering explanatory power beyond linear distance. Conceptually, the findings reconcile linear and hierarchical perspectives on locality by treating dependency length as an important surface signature while identifying intervening heads as a more proximate indicator of integration and maintenance demands. Methodologically, the study illustrates how UD-based graph measures and cross-linguistic mixed-effects modelling can disentangle linear and structural contributions to processing efficiency, providing a principled path for evaluating competing theories of memory load in sentence comprehension.",
      "author": "Krishna Aggarwal",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 237,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:31.471006+00:00",
      "updated_at": "2025-09-26T15:40:31.471008+00:00"
    },
    {
      "id": "9706f8b61e4824a370d7abb5ff5d9f40",
      "url": "https://arxiv.org/abs/2509.21277",
      "title": "More than a feeling: Expressive style influences cortical speech tracking in subjective cognitive decline",
      "content": "arXiv:2509.21277v1 Announce Type: new \nAbstract: Subjective cognitive decline (SCD) approximately doubles the risk of progressing to MCI and dementia. The present study investigates how one's subjective concerns of his/her own cognition are manifested in the neural dynamics during speech perception. EEG was collected from 56 Cantonese, cognitively normal older adults (aged 60 - 70) while they listened to stimuli of four expressive styles that varied in prosody: scrambled, descriptive, dialogue, and exciting. Using encoding models to predict EEG signals from acoustic, segmentation, and phonotactic features, we found that greater subjective concern was associated with weaker cortical tracking of (1) higher-level linguistic features but not acoustic features and (2) less engaging stimuli (scrambled and descriptive styles) but not prosodically rich stimuli. Overall, our results suggest that early signs of cognitive impairment can be revealed from speech perception via cortical tracking, especially while listening to prosodically flat speech.",
      "author": "Matthew King-Hang Ma, Manson Cheuk-Man Fong, Yun Feng, Cloris Pui-Hang Li, William Shiyuan Wang",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:31.470958+00:00",
      "updated_at": "2025-09-26T15:40:31.470961+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "7c9ccc43b6145740efaef749f5672ac6",
      "url": "https://arxiv.org/abs/2509.20369",
      "title": "AI-driven formative assessment and adaptive learning in data-science education: Evaluating an LLM-powered virtual teaching assistant",
      "content": "arXiv:2509.20369v1 Announce Type: cross \nAbstract: This paper presents VITA (Virtual Teaching Assistants), an adaptive distributed learning (ADL) platform that embeds a large language model (LLM)-powered chatbot (BotCaptain) to provide dialogic support, interoperable analytics, and integrity-aware assessment for workforce preparation in data science. The platform couples context-aware conversational tutoring with formative-assessment patterns designed to promote reflective reasoning. The paper describes an end-to-end data pipeline that transforms chat logs into Experience API (xAPI) statements, instructor dashboards that surface outliers for just-in-time intervention, and an adaptive pathway engine that routes learners among progression, reinforcement, and remediation content. The paper also benchmarks VITA conceptually against emerging tutoring architectures, including retrieval-augmented generation (RAG)--based assistants and Learning Tools Interoperability (LTI)--integrated hubs, highlighting trade-offs among content grounding, interoperability, and deployment complexity. Contributions include a reusable architecture for interoperable conversational analytics, a catalog of patterns for integrity-preserving formative assessment, and a practical blueprint for integrating adaptive pathways into data-science courses. The paper concludes with implementation lessons and a roadmap (RAG integration, hallucination mitigation, and LTI~1.3 / OpenID Connect) to guide multi-course evaluations and broader adoption. In light of growing demand and scalability constraints in traditional instruction, the approach illustrates how conversational AI can support engagement, timely feedback, and personalized learning at scale. Future work will refine the platform's adaptive intelligence and examine applicability across varied educational settings.",
      "author": "Fadjimata I Anaroua, Qing Li, Yan Tang, Hong P. Liu",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 219,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570485+00:00",
      "updated_at": "2025-09-26T16:18:00.450792+00:00",
      "metadata": {
        "processed_at": "2025-09-26T16:18:00.450801+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3efa9f66e0ddd59f944185876e7a51ca",
      "url": "https://arxiv.org/abs/2509.21188",
      "title": "Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey",
      "content": "arXiv:2509.21188v1 Announce Type: new \nAbstract: Clinicians face growing information overload from biomedical literature and guidelines, hindering evidence-based care. Retrieval-augmented generation (RAG) with large language models may provide fast, provenance-linked answers, but requires real-world evaluation. We describe iatroX, a UK-centred RAG-based clinical reference platform, and report early adoption, usability, and perceived clinical value from a formative implementation evaluation. Methods comprised a retrospective analysis of usage across web, iOS, and Android over 16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage metrics were drawn from web and app analytics with bot filtering. A client-side script randomized single-item prompts to approx. 10% of web sessions from a predefined battery assessing usefulness, reliability, and adoption intent. Proportions were summarized with Wilson 95% confidence intervals; free-text comments underwent thematic content analysis. iatroX reached 19,269 unique web users, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile uptake included 1,960 iOS downloads and Android growth (peak >750 daily active users). The survey yielded 1,223 item-level responses: perceived usefulness 86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%; 14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived accuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI 62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK specificity. Early real-world use suggests iatroX can mitigate information overload and support timely answers for UK clinicians. Limitations include small per-item samples and early-adopter bias; future work will include accuracy audits and prospective studies on workflow and care quality.",
      "author": "Kolawole Tytler",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 246,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570450+00:00",
      "updated_at": "2025-09-26T16:18:00.450806+00:00",
      "metadata": {
        "processed_at": "2025-09-26T16:18:00.450808+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2ca1ec84cdfeb409b6478273d82e53c6",
      "url": "https://arxiv.org/abs/2509.20901",
      "title": "CafGa: Customizing Feature Attributions to Explain Language Models",
      "content": "arXiv:2509.20901v1 Announce Type: new \nAbstract: Feature attribution methods, such as SHAP and LIME, explain machine learning model predictions by quantifying the influence of each input component. When applying feature attributions to explain language models, a basic question is defining the interpretable components. Traditional feature attribution methods, commonly treat individual words as atomic units. This is highly computationally inefficient for long-form text and fails to capture semantic information that spans multiple words. To address this, we present CafGa, an interactive tool for generating and evaluating feature attribution explanations at customizable granularities. CafGa supports customized segmentation with user interaction and visualizes the deletion and insertion curves for explanation assessments. Through a user study involving participants of various expertise, we confirm CafGa's usefulness, particularly among LLM practitioners. Explanations created using CafGa were also perceived as more useful compared to those generated by two fully automatic baseline methods: PartitionSHAP and MExGen, suggesting the effectiveness of the system.",
      "author": "Alan Boyle, Furui Cheng, Vil\\'em Zouhar, Mennatallah El-Assady",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570414+00:00",
      "updated_at": "2025-09-26T16:18:00.450810+00:00",
      "metadata": {
        "processed_at": "2025-09-26T16:18:00.450812+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f7f1aa6e4da844f9c6ba6817f13fe942",
      "url": "https://arxiv.org/abs/2509.20817",
      "title": "Even More Kawaii than Real-Person-Driven VTubers? Understanding How Viewers Perceive AI-Driven VTubers",
      "content": "arXiv:2509.20817v1 Announce Type: new \nAbstract: VTubers, digital personas represented by animated avatars, have gained massive popularity. Traditionally, VTubers are operated and voiced by human controllers known as Nakanohito. The reliance on Nakanohito, however, poses risks due to potential personal controversies and operational disruptions. The emergence of AI-driven VTubers offers a new model free from these human constraints. While AI-driven VTubers present benefits such as continuous operation and reduced scandal risk, they also raise questions about authenticity and audience engagement. Therefore, to gain deeper insights, we conduct a case study, investigating viewer perceptions of Neuro-sama, the most popular AI-driven VTuber with 845k followers on Twitch and 753k followers on YouTube. We analyze 108k Reddit posts and 136k YouTube comments, aiming to better understand viewer motivations, how AI constructs the virtual persona, and perceptions of the AI as Nakanohito. Our findings enhance the understanding of AI-driven VTubers and their impact on digital streaming culture.",
      "author": "Yiluo Wei, Yupeng He, Gareth Tyson",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 152,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570384+00:00",
      "updated_at": "2025-09-26T16:18:00.450814+00:00",
      "metadata": {
        "processed_at": "2025-09-26T16:18:00.450816+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "cec62a849a78c5db7bd2b42c51569555",
      "url": "https://arxiv.org/abs/2509.20799",
      "title": "AuthGlass: Enhancing Voice Authentication on Smart Glasses via Air-Bone Acoustic Features",
      "content": "arXiv:2509.20799v1 Announce Type: new \nAbstract: With the rapid advancement of smart glasses, voice interaction has become widely deployed due to its naturalness and convenience. However, its practicality is often undermined by the vulnerability to spoofing attacks and interference from surrounding sounds, making seamless voice authentication crucial for smart glasses usage. To address this challenge, we propose AuthGlass, a voice authentication approach that leverages both air- and bone-conducted speech features to enhance accuracy and liveness detection. Aiming to gain comprehensive knowledge on speech-related acoustic and vibration features, we built a smart glasses prototype with redundant synchronized microphones: 14 air-conductive microphones and 2 bone-conductive units. In a study with 42 participants, we validated that combining sound-field and vibration features significantly improves authentication robustness and attack resistance. Furthermore, experiments demonstrated that AuthGlass maintains competitive accuracy even under various practical scenarios, highlighting its applicability and scalability for real-world deployment.",
      "author": "Weiye Xu, Zhang Jiang, Siqi Zheng, Xiyuxing Zhang, Yankai Zhao, Changhao Zhang, Jian Liu, Weiqiang Wang, Yuntao Wang",
      "published_date": "2025-09-26T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 145,
      "reading_time": 1,
      "created_at": "2025-09-26T15:40:32.570350+00:00",
      "updated_at": "2025-09-26T16:18:00.450818+00:00",
      "metadata": {
        "processed_at": "2025-09-26T16:18:00.450822+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "372ea349d8718616fceaed45039bfae9",
      "url": "https://www.nature.com/articles/s41593-025-02053-7",
      "title": "Rapid learning of neural circuitry from holographic ensemble stimulation enabled by model-based compressed sensing",
      "content": "<p>Nature Neuroscience, Published online: 17 September 2025; <a href=\"https://www.nature.com/articles/s41593-025-02053-7\">doi:10.1038/s41593-025-02053-7</a></p>The authors develop a new computational system for high-throughput mapping of synaptic connectivity using two-photon holographic optogenetics and intracellular recordings.",
      "author": "Liam Paninski",
      "published_date": "2025-09-17T00:00:00+00:00",
      "source": "Nature Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 28,
      "reading_time": 1,
      "created_at": "2025-09-26T13:45:37.666135+00:00",
      "updated_at": "2025-09-26T14:14:25.488084+00:00",
      "metadata": {
        "processed_at": "2025-09-26T14:14:25.488095+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2c4f560bc459b5ad73b82304c0a12a55",
      "url": "https://www.nature.com/articles/s41593-025-02024-y",
      "title": "High-throughput synaptic connectivity mapping using in vivo two-photon holographic optogenetics and compressive sensing",
      "content": "<p>Nature Neuroscience, Published online: 17 September 2025; <a href=\"https://www.nature.com/articles/s41593-025-02024-y\">doi:10.1038/s41593-025-02024-y</a></p>Using two-photon optogenetics, electrical recordings and sparse signal reconstruction, the authors demonstrate in vivo synaptic connectivity mapping in the mouse visual cortex.",
      "author": "Valentina Emiliani",
      "published_date": "2025-09-17T00:00:00+00:00",
      "source": "Nature Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 30,
      "reading_time": 1,
      "created_at": "2025-09-26T13:45:37.666114+00:00",
      "updated_at": "2025-09-26T14:14:25.488100+00:00",
      "metadata": {
        "processed_at": "2025-09-26T14:14:25.488102+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "d546df6e7b8fd041779e374f9f13ac75",
      "url": "https://www.nature.com/articles/s41593-025-02060-8",
      "title": "Temporal integration in human auditory cortex is predominantly yoked to absolute time",
      "content": "<p>Nature Neuroscience, Published online: 18 September 2025; <a href=\"https://www.nature.com/articles/s41593-025-02060-8\">doi:10.1038/s41593-025-02060-8</a></p>Temporal integration throughout the human auditory cortex is predominantly locked to absolute time and does not vary with the duration of speech structures such as phonemes or words.",
      "author": "Nima Mesgarani",
      "published_date": "2025-09-18T00:00:00+00:00",
      "source": "Nature Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 36,
      "reading_time": 1,
      "created_at": "2025-09-26T13:45:37.666092+00:00",
      "updated_at": "2025-09-26T14:14:25.488104+00:00",
      "metadata": {
        "processed_at": "2025-09-26T14:14:25.488105+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "62a8af781972e2c2ecd9bd4dfc86c43a",
      "url": "http://www.jneurosci.org/cgi/content/short/45/35/etwij45352025?rss=1",
      "title": "This Week in The Journal",
      "content": "",
      "author": "McKeon, P.",
      "published_date": "2025-08-27T16:30:24+00:00",
      "source": "Journal Neuroscience This Week",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-09-26T13:45:36.509730+00:00",
      "updated_at": "2025-09-26T14:14:25.488108+00:00",
      "metadata": {
        "processed_at": "2025-09-26T14:14:25.488109+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "1fbf2a88ffa5c6d723541c65448df2d3",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1553035",
      "title": "VAE deep learning model with domain adaptation, transfer learning and harmonization for diagnostic classification from multi-site neuroimaging data",
      "content": "In large public multi-site fMRI datasets, the sample characteristics, data acquisition methods, and MRI scanner models vary across sites and datasets. This non-neural variability obscures neural differences between groups and leads to poor machine learning based diagnostic classification of neurodevelopmental conditions. This could be potentially addressed by domain adaptation, which aims to improve classification performance in a given target domain by utilizing the knowledge learned from a different source domain by making data distributions of the two domains as similar as possible. In order to demonstrate the utility of domain adaptation for multi-site fMRI data, this research developed a variational autoencoder\u2014maximum mean discrepancy (VAE-MMD) deep learning model for three-way diagnostic classification: (i) Autism, (ii) Asperger's syndrome, and (iii) typically developing controls. This study chooses ABIDE-II (Autism Brain Imaging Data Exchange) dataset as the target domain and ABIDE-I as the source domain. The results show that domain adaptation from ABIDE-I to ABIDE-II provides superior test accuracy of ABIDE-II compared to just using ABIDE-II for classification. Further, augmenting the source domain with additional healthy control subjects from Healthy Brain Network (HBN) and Amsterdam Open MRI Collection (AOMIC) datasets enables transfer learning and improves ABIDE-II classification performance. Finally, a comparison with statistical data harmonization techniques, such as ComBat, reveals that domain adaptation using VAE-MMD achieves comparable performance, and incorporating transfer learning (TL) with additional healthy control data substantially improves classification accuracy beyond that achieved by statistical methods (such as ComBat) alone. The dataset and the model used in this study are publicly available. The neuroimaging community can explore the possibility of further improving the model by utilizing the ever-increasing amount of healthy control fMRI data in the public domain.",
      "author": "D. Rangaprakash",
      "published_date": "2025-09-11T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 276,
      "reading_time": 1,
      "created_at": "2025-09-26T13:45:32.435139+00:00",
      "updated_at": "2025-09-26T14:14:25.488111+00:00",
      "metadata": {
        "processed_at": "2025-09-26T14:14:25.488113+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}