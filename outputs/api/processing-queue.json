{
  "last_updated": "2026-01-07T06:26:38.978933+00:00",
  "pending_count": 678,
  "processed_count": 322,
  "pending_articles": [
    {
      "id": "cf9da45a23e11181d435a5e294a98d67",
      "url": "https://arxiv.org/abs/2601.03223",
      "title": "Are eHMIs always helpful? Investigating how eHMIs interfere with pedestrian behavior on multi-lane streets: An eye-tracking virtual reality experiment",
      "content": "arXiv:2601.03223v1 Announce Type: new \nAbstract: Appropriate communication is crucial for efficient and safe interactions between pedestrians and autonomous vehicles (AVs). External human-machine interfaces (eHMIs) on AVs, which can be categorized as allocentric or egocentric, are considered a promising solution. While the effectiveness of eHMIs has been extensively studied, in complex environments, such as unsignalized multi-lane streets, their potential to interfere with pedestrian crossing behavior remains underexplored. Hence, a virtual reality-based experiment was conducted to examine how different types of eHMIs displayed on AVs affect the crossing behavior of pedestrians in multi-lane streets environments, with a focus on the gaze patterns of pedestrians during crossing. The results revealed that the presence of eHMIs significantly influenced the cognitive load on pedestrians and increased the possibility of distraction, even misleading pedestrians in cases involving multiple AVs on multi-lane streets. Notably, allocentric eHMIs induced higher cognitive loads and greater distraction in pedestrians than egocentric eHMIs. This was primarily evidenced by longer gaze time and higher proportions of attention for the eHMI on the interacting vehicle, as well as a broader distribution of gaze toward vehicles in the non-interacting lane. However, misleading behavior was mainly triggered by eHMI signals from yielding vehicles in the non-interacting lane. Under such asymmetric signal configurations, egocentric eHMIs resulted in a higher misjudgment rate than allocentric eHMIs. These findings highlight the importance of enhancing eHMI designs to balance the clarity and consistency of the displayed information across different perspectives, especially in complex multi-lane traffic scenarios. This study provides valuable insights regarding the application and standardization of future eHMI systems for AVs.",
      "author": "Yun Ye, Zexuan Li, Panagiotis Angeloudis, S. C. Wong, Jian Sun, Haoyang Liang",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 261,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.899089+00:00",
      "updated_at": "2026-01-07T05:27:39.899091+00:00"
    },
    {
      "id": "ba70e609cdbb9e9b847625ac540af753",
      "url": "https://arxiv.org/abs/2601.03218",
      "title": "Enhancing Safety in Automated Ports: A Virtual Reality Study of Pedestrian-Autonomous Vehicle Interactions under Time Pressure, Visual Constraints, and Varying Vehicle Size",
      "content": "arXiv:2601.03218v1 Announce Type: new \nAbstract: Autonomous driving improves traffic efficiency but presents safety challenges in complex port environments. This study investigates how environmental factors, traffic factors, and pedestrian characteristics influence interaction safety between autonomous vehicles and pedestrians in ports. Using virtual reality (VR) simulations of typical port scenarios, 33 participants completed pedestrian crossing tasks under varying visibility, vehicle sizes, and time pressure conditions. Results indicate that low-visibility conditions, partial occlusions and larger vehicle sizes significantly increase perceived risk, prompting pedestrians to wait longer and accept larger gaps. Specifically, pedestrians tended to accept larger gaps and waited longer when interacting with large autonomous truck platoons, reflecting heightened caution due to their perceived threat. However, local obstructions also reduce post-encroachment time, compressing safety margins. Individual attributes such as age, gender, and driving experience further shape decision-making, while time pressure undermines compensatory behaviors and increases risk. Based on these findings, safety strategies are proposed, including installing wide-angle cameras at multiple viewpoints, enabling real-time vehicle-infrastructure communication, enhancing port lighting and signage, and strengthening pedestrian safety training. This study offers practical recommendations for improving the safety and deployment of vision-based autonomous systems in port settings.",
      "author": "Yuan Che, Mun On Wong, Xiaowei Gao, Haoyang Liang, Yun Ye",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 190,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.899046+00:00",
      "updated_at": "2026-01-07T05:27:39.899048+00:00"
    },
    {
      "id": "9921f689332702cb1d19174ed30e5465",
      "url": "https://arxiv.org/abs/2601.02829",
      "title": "Resolution deficits drive simulator sickness and compromise reading performance in virtual environments",
      "content": "arXiv:2601.02829v1 Announce Type: new \nAbstract: Extended reality (XR) is evolving into a general-purpose computing platform, yet its adoption for productivity is hindered by visual fatigue and simulator sickness. While these symptoms are often attributed to latency or motion conflicts, the precise impact of textual clarity on physiological comfort remains undefined. Here we show that sub-optimal effective resolution, the clarity that reaches the eye after the full display-optics-rendering pipeline, is a primary driver of simulator sickness during reading tasks in both virtual reality and video see-through environments. By systematically manipulating end-to-end effective resolution on a unified logMAR scale, we measured reading psychophysics and sickness symptoms in a controlled within-subjects study. We find that reading performance and user comfort degrade exponentially as resolution drops below 0 logMAR (normal visual acuity). Notably, our results reveal 0 logMAR as a key physiological tipping point: resolutions better than this threshold yield naked-eye-level performance with minimal sickness, whereas poorer resolutions trigger rapid, non-linear increases in nausea and oculomotor strain. These findings suggest that the cognitive and perceptual effort required to resolve blurry text directly compromises user comfort, establishing human-eye resolution as a critical baseline for the design of future ergonomic XR systems.",
      "author": "Jialin Wang, Xinru Cheng, Boyong Hou, Hai-Ning Liang",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 196,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.899014+00:00",
      "updated_at": "2026-01-07T05:27:39.899016+00:00"
    },
    {
      "id": "34e1ea5169e2abf33f079220674bbede",
      "url": "https://arxiv.org/abs/2601.02805",
      "title": "The perceptual gap between video see-through displays and natural human vision",
      "content": "arXiv:2601.02805v1 Announce Type: new \nAbstract: Video see-through (VST) technology aims to seamlessly blend virtual and physical worlds by reconstructing reality through cameras. While manufacturers promise perceptual fidelity, it remains unclear how close these systems are to replicating natural human vision across varying environmental conditions. In this work, we quantify the perceptual gap between the human eye and different popular VST headsets (Apple Vision Pro, Meta Quest 3, Quest Pro) using psychophysical measures of visual acuity, contrast sensitivity, and color vision. We show that despite hardware advancements, all tested VST systems fail to match the dynamic range and adaptability of the naked eye. While high-end devices approach human performance in ideal lighting, they exhibit significant degradation in low-light conditions, particularly in contrast sensitivity and acuity. Our results map the physiological limitations of digital reality reconstruction, establishing a specific perceptual gap that defines the roadmap for achieving indistinguishable VST experiences.",
      "author": "Jialin Wang, Songming Ping, Kemu Xu, Yue Li, Hai-Ning Liang",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 148,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.898981+00:00",
      "updated_at": "2026-01-07T05:27:39.898983+00:00"
    },
    {
      "id": "a12e713837be186c37dd387c2c8c647f",
      "url": "https://arxiv.org/abs/2601.02775",
      "title": "Experience and Adaptation in AI-mediated Hiring Systems: A Combined Analysis of Online Discourse and Interface Design",
      "content": "arXiv:2601.02775v1 Announce Type: new \nAbstract: Automated interviewing tools are now widely adopted to manage recruitment at scale, often replacing early human screening with algorithmic assessments. While these systems are promoted as efficient and consistent, they also generate new forms of uncertainty for applicants. Efforts to soften these experiences through human-like design features have only partially addressed underlying concerns. To understand how candidates interpret and cope with such systems, we conducted a mixed empirical investigation that combined analysis of online discussions, responses from more than one hundred and fifty survey participants, and follow-up conversations with seventeen interviewees. The findings point to several recurring problems, including unclear evaluation criteria, limited organizational responsibility for automated outcomes, and a lack of practical support for preparation. Many participants described the technology as far less advanced than advertised, leading them to infer how decisions might be made in the absence of guidance. This speculation often intensified stress and emotional strain. Furthermore, the minimal sense of interpersonal engagement contributed to feelings of detachment and disposability. Based on these observations, we propose design directions aimed at improving clarity, accountability, and candidate support in AI-mediated hiring processes.",
      "author": "Md Nazmus Sakib, Naga Manogna Rayasam, Sanorita Dey",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 188,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.898951+00:00",
      "updated_at": "2026-01-07T05:27:39.898953+00:00"
    },
    {
      "id": "4316d71e191b33fa3329beeac4986686",
      "url": "https://arxiv.org/abs/2601.02363",
      "title": "Acceptance of cybernetic avatars for capability enhancement: a large-scale survey",
      "content": "arXiv:2601.02363v1 Announce Type: new \nAbstract: Avatar embodiment experiences have the potential to enhance human capabilities by extending human senses, body, and mind. This study investigates social acceptance of robotic and virtual avatars as enablers of capability enhancement in six domains: identity exploration, well-being and behavioral transformation, expanded travel capabilities, expanded bodily and sensory abilities, cognitive augmentation, and immortality. We conducted a large-scale survey (n = 1001) in Dubai to explore acceptance of sixteen capability enhancement scenarios within these domains. The highest levels of agreement were observed for multilingual communication (77.5%) and learning capabilities (68.7%), followed by assisting individuals with reduced mobility (64.5%) and behavioral transformation (59.5%). Scenarios involving immortality through consciousness transfer received the least support (34.9%). These findings contribute to a deeper understanding of public attitudes toward avatar-based human enhancement and offer practical guidance for the responsible design, development, and integration of cybernetic avatars in the society, ensuring their societal acceptance and fostering a harmonious human-avatar coexistence.",
      "author": "Laura Aymerich-Franch, Tarek Taha, Hiroko Kamide, Takahiro Miyashita, Hiroshi Ishiguro, Paolo Dario",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.898913+00:00",
      "updated_at": "2026-01-07T05:27:39.898917+00:00"
    },
    {
      "id": "b2e792b730ab429d9fdf9feeb50ff49c",
      "url": "https://arxiv.org/abs/2506.02164",
      "title": "Quantifying task-relevant representational similarity using decision variable correlation",
      "content": "arXiv:2506.02164v3 Announce Type: replace-cross \nAbstract: Previous studies have compared neural activities in the visual cortex to representations in deep neural networks trained on image classification. Interestingly, while some suggest that their representations are highly similar, others argued the opposite. Here, we propose a new approach to characterize the similarity of the decision strategies of two observers (models or brains) using decision variable correlation (DVC). DVC quantifies the image-by-image correlation between the decoded decisions based on the internal neural representations in a classification task. Thus, it can capture task-relevant information rather than general representational alignment. We evaluate DVC using monkey V4/IT recordings and network models trained on image classification tasks. We find that model-model similarity is comparable to monkey-monkey similarity, whereas model-monkey similarity is consistently lower. Strikingly, DVC decreases with increasing network performance on ImageNet-1k. Adversarial training does not improve model-monkey similarity in task-relevant dimensions assessed using DVC, although it markedly increases the model-model similarity. Similarly, pre-training on larger datasets does not improve model-monkey similarity. These results suggest a divergence between the task-relevant representations in monkey V4/IT and those learned by models trained on image classification tasks.",
      "author": "Yu (Eric),  Qian, Wilson S. Geisler, Xue-Xin Wei",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 186,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:38.809731+00:00",
      "updated_at": "2026-01-07T05:27:38.809733+00:00"
    },
    {
      "id": "a6e11413e33658edf09eb0f59609b931",
      "url": "https://arxiv.org/abs/2509.25453",
      "title": "Neural Receptive Fields, Stimulus Space Embedding and Effective Geometry of Scale-Free Networks",
      "content": "arXiv:2509.25453v2 Announce Type: replace \nAbstract: Understanding how receptive fields emerge and organize within brain networks and how neural dynamics couple with stimuli space is fundamental to neuroscience. Models often rely on fine-tuning connectivity to match empirical data, which may limit biological plausibility. Here we propose a physiologically grounded alternative where receptive fields and population-level attractor dynamics arise naturally from the effective hyperbolic geometry of scale-free networks. By associating stimulus space with the boundary of a hyperbolic embedding, we simulate neural dynamics using rate-based and spiking models, revealing localized activity patterns that reflect stimulus space structure without synaptic fine-tuning. The resulting receptive fields follow experimentally observed statistics and properties, and their sizes depends on neuron's connectivity degree. The model generalizes across stimuli dimensionalities and various modalities, such as orientation and place selectivity. Experimental analyses of hippocampal place fields recorded on a linear track support these findings. This framework offers a novel organizing principle linking network structure, stimulus space encoding, and neural dynamics, providing insights into receptive field formation across diverse brain areas.",
      "author": "Vasilii Tiselko, Alexander Gorsky, Yuri Dabaghian",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:38.809700+00:00",
      "updated_at": "2026-01-07T05:27:38.809701+00:00"
    },
    {
      "id": "b265ec185f5d0cf089bdbca251c12183",
      "url": "https://arxiv.org/abs/2601.02885",
      "title": "A Mathematical Formalization of Self-Determining Agency",
      "content": "arXiv:2601.02885v1 Announce Type: cross \nAbstract: Defining agency is an extremely important challenge for cognitive science and artificial intelligence. Physics generally describes mechanical happenings, but there remains an unbridgeable gap between them and the acts of agents. To discuss the morality and responsibility of agents, it is necessary to model acts; whether such responsible acts can be fully explained by physical determinism has been debated. Although we have already proposed a physical \"agent determinism\" model that appears to go beyond mere mechanical happenings, we have not yet established a strict mathematical formalism to eliminate ambiguity. Here, we explain why a physical system can follow coarse-graining agent-level determination without violating physical laws by formulating supervenient causation. Generally, supervenience including coarse graining does not change without a change in its lower base; therefore, a single supervenience alone cannot define supervenient causation. We define supervenient causation as the causal efficacy from the supervenience level to its lower base level. Although an algebraic expression composed of the multiple supervenient functions does supervenes on the base, a sequence of indices that determines the algebraic expression does not supervene on the base. Therefore, the sequence can possess unique dynamical laws that are independent of the lower base level. This independent dynamics creates the possibility for temporally preceding changes at the supervenience level to cause changes at the lower base level. Such a dual-laws system is considered useful for modeling self-determining agents such as humans.",
      "author": "Yoshiyuki Ohmura, Earnest Kota Carr, Yasuo Kuniyoshi",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 237,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:38.809670+00:00",
      "updated_at": "2026-01-07T05:27:38.809671+00:00"
    },
    {
      "id": "2587e42874902193001b7c6568e6fd0a",
      "url": "https://arxiv.org/abs/2601.02636",
      "title": "Credit Assignment via Neural Manifold Noise Correlation",
      "content": "arXiv:2601.02636v1 Announce Type: cross \nAbstract: Credit assignment--how changes in individual neurons and synapses affect a network's output--is central to learning in brains and machines. Noise correlation, which estimates gradients by correlating perturbations of activity with changes in output, provides a biologically plausible solution to credit assignment but scales poorly as accurately estimating the Jacobian requires that the number of perturbations scale with network size. Moreover, isotropic noise conflicts with neurobiological observations that neural activity lies on a low-dimensional manifold. To address these drawbacks, we propose neural manifold noise correlation (NMNC), which performs credit assignment using perturbations restricted to the neural manifold. We show theoretically and empirically that the Jacobian row space aligns with the neural manifold in trained networks, and that manifold dimensionality scales slowly with network size. NMNC substantially improves performance and sample efficiency over vanilla noise correlation in convolutional networks trained on CIFAR-10, ImageNet-scale models, and recurrent networks. NMNC also yields representations more similar to the primate visual system than vanilla noise correlation. These findings offer a mechanistic hypothesis for how biological circuits could support credit assignment, and suggest that biologically inspired constraints may enable, rather than limit, effective learning at scale.",
      "author": "Byungwoo Kang, Maceo Richards, Bernardo Sabatini",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 194,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:38.809636+00:00",
      "updated_at": "2026-01-07T05:27:38.809637+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "e11c3f1f030ef02fcd2dff69c338e159",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1702902",
      "title": "GAME-Net: an ensemble deep learning framework integrating Generative Autoencoders and attention mechanisms for automated brain tumor segmentation in MRI",
      "content": "IntroductionAccurate and early identification of brain tumors is essential for improving therapeutic planning and clinical outcomes. Manual segmentation of Magnetic Resonance Imaging (MRI) remains time-consuming and subject to inter-observer variability. Computational models that combine Artificial Intelligence and biomedical imaging offer a pathway toward objective and efficient tumor delineation. The present study introduces a deep learning framework designed to enhance brain tumor segmentation performance.MethodsA comprehensive ensemble architecture was developed by integrating Generative Autoencoders with Attention Mechanisms (GAME), Convolutional Neural Networks, and attention-augmented U-Net segmentation modules. The dataset comprised 5,880 MRI images sourced from the BraTS 2023 benchmark distribution accessed via Kaggle, partitioned into training, validation, and testing subsets. Preprocessing included intensity normalization, augmentation, and unsupervised feature extraction. Tumor segmentation employed an attention-based U-Net, while tumor classification utilized a CNN coupled with Transformer-style self-attention. The Generative Autoencoder performed unsupervised representation learning to refine feature separability and enhance robustness to MRI variability.ResultsThe proposed framework achieved notable performance improvements across multiple evaluation metrics. The segmentation module produced a Dice Coefficient of 0.85 and a Jaccard Index of 0.78. The classification component yielded an accuracy of 87.18 percent, sensitivity of 88.3 percent, specificity of 86.5 percent, and an AUC-ROC of 0.91. The combined use of generative modeling, attention mechanisms, and ensemble learning improved tumor localization, boundary delineation, and false positive suppression compared with conventional architectures.DiscussionThe findings indicate that enriched representation learning and attention-driven feature refinement substantially elevate segmentation accuracy on heterogeneous MRI data. The integration of unsupervised learning within the pipeline supported improved generalization across variable imaging conditions. The demonstrated performance suggests strong potential for clinical utility, although broader validation across external datasets is recommended to further substantiate generalizability.",
      "author": "Mohammed Al-Naeem",
      "published_date": "2025-12-08T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 274,
      "reading_time": 1,
      "created_at": "2026-01-07T05:50:05.142714+00:00",
      "updated_at": "2026-01-07T06:26:38.874104+00:00",
      "metadata": {
        "processed_at": "2026-01-07T06:26:38.874113+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "5060b486247a8b02a29f1ce4d914ba31",
      "url": "https://arxiv.org/abs/2601.03023",
      "title": "MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models",
      "content": "arXiv:2601.03023v1 Announce Type: cross \nAbstract: Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items (\"must-ask\" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.",
      "author": "Lecheng Gong, Weimin Fang, Ting Yang, Dongjie Tao, Chunxiao Guo, Peng Wei, Bo Xie, Jinqun Guan, Zixiao Chen, Fang Shi, Jinjie Gu, Junwei Liu",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 237,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.899244+00:00",
      "updated_at": "2026-01-07T06:26:38.874117+00:00",
      "metadata": {
        "processed_at": "2026-01-07T06:26:38.874119+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "16b24e1d0aef577558fbdd2ed5fc9a2b",
      "url": "https://arxiv.org/abs/2601.02933",
      "title": "Pearmut: Human Evaluation of Translation Made Trivial",
      "content": "arXiv:2601.02933v1 Announce Type: cross \nAbstract: Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.",
      "author": "Vil\\'em Zouhar, Tom Kocmi",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 144,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.899205+00:00",
      "updated_at": "2026-01-07T06:26:38.874122+00:00",
      "metadata": {
        "processed_at": "2026-01-07T06:26:38.874123+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "1515fb53c15ca3c0b3c48ea92fc29397",
      "url": "https://arxiv.org/abs/2601.02559",
      "title": "PerspectiveCoach: Exploring LLMs for Developer Reflection",
      "content": "arXiv:2601.02559v1 Announce Type: cross \nAbstract: Despite growing awareness of ethical challenges in software development, practitioners still lack structured tools that help them critically engage with the lived experiences of marginalized users. This paper presents PerspectiveCoach, a large language model (LLM)-powered conversational tool designed to guide developers through structured perspective-taking exercises and deepen critical reflection on how software design decisions affect marginalized communities. Through a controlled study with 18 front-end developers (balanced by sex), who interacted with the tool using a real case of online gender-based harassment, we examine how PerspectiveCoach supports ethical reasoning and engagement with user perspectives. Qualitative analysis revealed increased self-awareness, broadened perspectives, and more nuanced ethical articulation, while a complementary human-human study contextualized these findings. Text similarity analyses demonstrated that participants in the human-PerspectiveCoach study improved the fidelity of their restatements over multiple attempts, capturing both surface-level and semantic aspects of user concerns. However, human-PerspectiveCoach's restatements had a lower baseline than the human-human conversations, highlighting contextual differences in impersonal and interpersonal perspective-taking. Across the study, participants rated the tool highly for usability and relevance. This work contributes an exploratory design for LLM-powered end-user perspective-taking that supports critical, ethical self-reflection and offers empirical insights (i.e., enhancing adaptivity, centering plurality) into how such tools can help practitioners build more inclusive and socially responsive technologies.",
      "author": "Lauren Olson, Emitz\\'a Guzm\\'an, Florian Kunneman",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 215,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.899176+00:00",
      "updated_at": "2026-01-07T06:26:38.874125+00:00",
      "metadata": {
        "processed_at": "2026-01-07T06:26:38.874127+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7c71dc13542ebc105f05045fcae1db16",
      "url": "https://arxiv.org/abs/2601.03225",
      "title": "Wait or cross? Understanding the influence of behavioral tendency, trust, and risk perception on pedestrian gap-acceptance of automated truck platoons",
      "content": "arXiv:2601.03225v1 Announce Type: new \nAbstract: Although automated trucks have the potential to improve freight efficiency, reduce costs, and address driver shortages, organizing two or more trucks in a convoy has raised considerable concerns for pedestrian safety. This study conducted a controlled experiment to examine the influence of behavioral tendency, trust, and risk perception on pedestrian intention to cross in front of an automated truck platoon. A total of 603 subjects participated in the virtual reality video-based questionnaire survey. By fusing the merits of structural equation modeling and artificial neural networks, a two-stage, hybrid model was developed to examine complex relationships between latent variables and gap-acceptance behaviors. Our results indicated that subjects watched an average of five vehicle gaps before starting crossing and the average time gap accepted was about 5.35 seconds. Risk perception not only played the most dominant role in shaping pedestrian crossing decisions, but also served as the strong bone, mediating the effects of behavioral tendency and trust on gap-acceptance. Participants who frequently violated traffic rules were more likely to accept a smaller time gap, while those who showed positive behaviors to other road users tended to wait for a larger time gap. Participants who often committed errors, showed aggressive behaviors, and held greater trust in the safety of automated trucks generally reported a lower level of risk for road-crossing in front of automated truck platoons. Built on these findings, a range of tailored countermeasures were proposed to ensure safer and smother interactions between pedestrians and automated truck platoons.",
      "author": "Yun Ye, Yuan Che, Haoyang Liang, Yingheng Zhang, Pengpeng Xu",
      "published_date": "2026-01-07T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 251,
      "reading_time": 1,
      "created_at": "2026-01-07T05:27:39.899138+00:00",
      "updated_at": "2026-01-07T06:26:38.874129+00:00",
      "metadata": {
        "processed_at": "2026-01-07T06:26:38.874131+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7061f61b99ac28c2a7d9232a8747edd6",
      "url": "https://fmhy.net/posts/WWCO",
      "title": "What We're Capable Of",
      "content": "<p>There are 8.2 billion humans on earth.</p>\n<p>We're extremely dynamic. You will meet people all through your life with different personalities, skills, and interests. This individual uniqueness not only makes life more interesting, but it also increases our collective ability to handle a wide range of ideas and problems.</p>\n<p>Despite these capabilities, our differences can also lead to an overwhelming feeling of division in some. Differences often lead to disagreement, anger, hostility, or violence. To say humans don't often inflict cruelty on one another would be a lie. We absolutely have shown our teeth. In innumerable ways, we've caused horror to our own species. We've dehumanized, caused suffering, and in many cases killed one another.</p>\n<p>In many ways, we're still learning. Humanity is still growing. A lot of people are trying to sort right from wrong, fair from unfair, true from untrue. This leads to a lot of confusion, a lot of chaos, and a lot of violence. To see humans go after other humans in such efficient and relentless manners is terrifying and beyond disheartening. It can be easy to lose hope, but the truth is... <em>none of it is necessary.</em></p>\n<p><em><strong>None of the violence, none of the hate, none of the inequality is mandatory.</strong></em></p>\n<p>We are allowed to go against the grain of things we know are wrong. <em>We are allowed to improve our world.</em> Never be scared to do what you know is right, even when its hard, even when it might benefit someone else more than yourself. Match relentless selfishness with your own relentless generosity. Resist chaos and confusion with serenity and understanding. Engulf violence with endless harmony.</p>\n<p>Remember that truth doesn't rely on anyone. Truth doesn't ask for our opinions. Truth is eternal, unwavering, and it will always be there, waiting for us to realize what we're capable of together.</p>\n<hr />\n<ul>\n<li>&quot;<em><strong>In this world there\u2019s room for everyone and the good earth is rich, and can provide for everyone. The way of life can be free and beautiful, but we have lost the way.</strong></em> Greed has poisoned men's souls, has barricaded the world with hate, has goose-stepped us into misery and bloodshed. We have developed speed, but we have shut ourselves in. Machinery that gives abundance has left us in want. Our knowledge has made us cynical. Our cleverness hard and unkind. We think too much, and feel too little. <em><strong>More than machinery, we need humanity. More than cleverness, we need kindness and gentleness. Without these qualities life will be violent, and all will be lost.</strong></em>&quot; - Charlie Chaplin</li>\n</ul>\n<p><strong>Discussion</strong>: <a href=\"https://redd.it/1q63cme\" rel=\"noreferrer\" target=\"_blank\">https://redd.it/1q63cme</a></p>",
      "author": "",
      "published_date": "2026-01-06T00:00:00+00:00",
      "source": "Fmhy",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 432,
      "reading_time": 2,
      "created_at": "2026-01-07T03:40:02.928907+00:00",
      "updated_at": "2026-01-07T04:36:42.432286+00:00",
      "metadata": {
        "processed_at": "2026-01-07T04:36:42.432295+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "dd36282b4463b321c81cb5aa0ec73f82",
      "url": "https://jasoneckert.github.io/myblog/how-microsoft-killed-my-snapdragon-devkit/",
      "title": "Microsoft probably killed my Snapdragon Dev Kit",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46521860\">Comments</a>",
      "author": "",
      "published_date": "2026-01-07T02:37:09+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-07T03:40:00.341418+00:00",
      "updated_at": "2026-01-07T04:36:42.432299+00:00",
      "metadata": {
        "processed_at": "2026-01-07T04:36:42.432301+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "dd36282b4463b321c81cb5aa0ec73f82",
      "url": "https://jasoneckert.github.io/myblog/how-microsoft-killed-my-snapdragon-devkit/",
      "title": "Microsoft probably killed my Snapdragon Dev Kit",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46521860\">Comments</a>",
      "author": "",
      "published_date": "2026-01-07T02:37:09+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-07T03:40:00.341418+00:00",
      "updated_at": "2026-01-07T04:36:42.432299+00:00",
      "metadata": {
        "processed_at": "2026-01-07T04:36:42.432301+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "0bcb3f008e6f5bc32d99176a49ccd10d",
      "url": "https://csef.it/wp-content/uploads/M_Ronchi.pdf",
      "title": "Study: Managers with first-born daughters, hire more women and pay more equal [pdf]",
      "content": "<p>Article URL: <a href=\"https://csef.it/wp-content/uploads/M_Ronchi.pdf\">https://csef.it/wp-content/uploads/M_Ronchi.pdf</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46520587\">https://news.ycombinator.com/item?id=46520587</a></p>\n<p>Points: 10</p>\n<p># Comments: 1</p>",
      "author": "pploug",
      "published_date": "2026-01-06T23:48:38+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-07T03:39:58.960434+00:00",
      "updated_at": "2026-01-07T04:36:42.432303+00:00",
      "metadata": {
        "processed_at": "2026-01-07T04:36:42.432304+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "57924fed8dab6a642b3c46ab1f1a24d5",
      "url": "https://blog.adafruit.com/2026/01/06/we-recreated-steve-jobss-1975-atari-horoscope-program-and-you-can-run-it/",
      "title": "We Recreated Steve Jobs's 1975 Atari Horoscope Program and You Can Run It",
      "content": "<p>Article URL: <a href=\"https://blog.adafruit.com/2026/01/06/we-recreated-steve-jobss-1975-atari-horoscope-program-and-you-can-run-it/\">https://blog.adafruit.com/2026/01/06/we-recreated-steve-jobss-1975-atari-horoscope-program-and-you-can-run-it/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46521029\">https://news.ycombinator.com/item?id=46521029</a></p>\n<p>Points: 12</p>\n<p># Comments: 0</p>",
      "author": "ptorrone",
      "published_date": "2026-01-07T00:44:43+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-07T03:39:58.960368+00:00",
      "updated_at": "2026-01-07T04:36:42.432307+00:00",
      "metadata": {
        "processed_at": "2026-01-07T04:36:42.432308+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}