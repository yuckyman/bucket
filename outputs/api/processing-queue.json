{
  "last_updated": "2025-10-29T18:22:29.801836+00:00",
  "pending_count": 985,
  "processed_count": 15,
  "pending_articles": [
    {
      "id": "f175d9396f70671d66971e35b3191918",
      "url": "https://arxiv.org/abs/2510.24057",
      "title": "VR-Assisted Guide Dog Training: A 360{\\deg} PanoHaptic System for Right-Hand Commands Analysis",
      "content": "arXiv:2510.24057v1 Announce Type: new \nAbstract: This paper presents a VR-based guide dog training system designed to assist novice trainers in understanding guide dog behavior and issuing appropriate training commands. Guide dogs play a vital role in supporting independent mobility for visually impaired individuals, yet the limited number of skilled trainers restricts their availability. Training is highly demanding, requiring accurate observation of the dog's status and precise command issuance, especially through right-hand gestures. While the trainer's left hand holds the harness to perceive haptic cues, the right hand is used to indicate directions, maintain attention, and provide comfort, with motion patterns varying by scenario and the dog's progress. Currently, novices learn mainly by observing experts or watching videos, which lacks immersion and makes it difficult to adopt the trainer's perspective for understanding behavior or synchronizing command timing.\n  To address these limitations, the proposed system introduces a VR-based assistive platform integrating panoramic visuals and haptic feedback to create an immersive training environment. The visual module provides contextual guidance, including cues for command execution and real-time comparison of the user's posture with standard actions, while the haptic module delivers tactile feedback for command gestures. Users can re-experience training sessions across diverse scenarios and dog proficiency levels, allowing independent and repeated practice. By improving the timing, accuracy, and expressiveness of right-hand commands, the system aims to accelerate skill acquisition, enhance training quality, and mitigate the shortage of qualified trainers, ultimately increasing the availability of guide dogs for visually impaired individuals.",
      "author": "Qirong Zhu, Ansheng Wang, Shinji Tanaka, Yasutoshi Makino, Hiroyuki Shinoda",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 246,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982245+00:00",
      "updated_at": "2025-10-29T17:43:43.982247+00:00"
    },
    {
      "id": "5208b19e2c070d96ec42cf953c682da0",
      "url": "https://arxiv.org/abs/2510.24011",
      "title": "Understanding Reader Perception Shifts upon Disclosure of AI Authorship",
      "content": "arXiv:2510.24011v1 Announce Type: new \nAbstract: As AI writing support becomes ubiquitous, how disclosing its use affects reader perception remains a critical, underexplored question. We conducted a study with 261 participants to examine how revealing varying levels of AI involvement shifts author impressions across six distinct communicative acts. Our analysis of 990 responses shows that disclosure generally erodes perceptions of trustworthiness, caring, competence, and likability, with the sharpest declines in social and interpersonal writing. A thematic analysis of participants' feedback links these negative shifts to a perceived loss of human sincerity, diminished author effort, and the contextual inappropriateness of AI. Conversely, we find that higher AI literacy mitigates these negative perceptions, leading to greater tolerance or even appreciation for AI use. Our results highlight the nuanced social dynamics of AI-mediated authorship and inform design implications for creating transparent, context-sensitive writing systems that better preserve trust and authenticity.",
      "author": "Hiroki Nakano, Jo Takezawa, Fabrice Matulic, Chi-Lan Yang, Koji Yatani",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982206+00:00",
      "updated_at": "2025-10-29T17:43:43.982207+00:00"
    },
    {
      "id": "fe77731744ed82caa4735c9ad41fae23",
      "url": "https://arxiv.org/abs/2510.24004",
      "title": "Modeling Object Attention in Mobile AR for Intrinsic Cognitive Security",
      "content": "arXiv:2510.24004v1 Announce Type: new \nAbstract: We study attention in mobile Augmented Reality (AR) using object recall as a proxy outcome. We observe that the ability to recall an object (physical or virtual) that was encountered in a mobile AR experience depends on many possible impact factors and attributes, with some objects being readily recalled while others are not, and some people recalling objects overall much better or worse than others. This opens up a potential cognitive attack in which adversaries might create conditions that make an AR user not recall certain potentially mission-critical objects. We explore whether a calibrated predictor of object recall can help shield against such cognitive attacks. We pool data from four mobile AR studies (with a total of 1,152 object recall probes) and fit a Partial Least Squares Structural Equation Model (PLS-SEM) with formative Object, Scene, and User State composites predicting recall, also benchmarking against Random Forest and multilayer perceptron classifiers. PLS-SEM attains the best F1 score in three of four studies. Additionally, path estimates identify lighting, augmentation density, AR registration stability, cognitive load, and AR familiarity as primary drivers. The model outputs per-object recall probabilities that can drive interface adjustments when predicted recall falls. Overall, PLS-SEM provides competitive accuracy with interpretable levers for design and evaluation in mobile AR.",
      "author": "Shane Dirksen, Radha Kumaran, You-Jin Kim, Yilin Wang, Tobias H\\\"ollerer",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982178+00:00",
      "updated_at": "2025-10-29T17:43:43.982179+00:00"
    },
    {
      "id": "5f426c07174c242eb0eabd439995bbcb",
      "url": "https://arxiv.org/abs/2510.23947",
      "title": "Toward Socially-Aware LLMs: A Survey of Multimodal Approaches to Human Behavior Understanding",
      "content": "arXiv:2510.23947v1 Announce Type: new \nAbstract: LLM-powered multimodal systems are increasingly used to interpret human social behavior, yet how researchers apply the models' 'social competence' remains poorly understood. This paper presents a systematic literature review of 176 publications across different application domains (e.g., healthcare, education, and entertainment). Using a four-dimensional coding framework (application, technical, evaluative, and ethical), we find (1) frequent use of pattern recognition and information extraction from multimodal sources, but limited support for adaptive, interactive reasoning; (2) a dominant 'modality-to-text' pipeline that privileges language over rich audiovisual cues, striping away nuanced social cues; (3) evaluation practices reliant on static benchmarks, with socially grounded, human-centered assessments rare; and (4) Ethical discussions focused mainly on legal and rights-related risks (e.g., privacy), leaving societal risks (e.g., deception) overlooked--or at best acknowledged but left unaddressed. We outline a research agenda for evaluating socially competent, ethically informed, and interaction-aware multi-modal systems.",
      "author": "Zihan Liu, Parisa Rabbani, Veda Duddu, Kyle Fan, Madison Lee, Yun Huang",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 147,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982144+00:00",
      "updated_at": "2025-10-29T17:43:43.982146+00:00"
    },
    {
      "id": "13ef90c3c003f11b1f3aba6eac08ea82",
      "url": "https://arxiv.org/abs/2510.23904",
      "title": "Towards AI as Colleagues: Multi-Agent System Improves Structured Professional Ideation",
      "content": "arXiv:2510.23904v1 Announce Type: new \nAbstract: Most AI systems today are designed to manage tasks and execute predefined steps. This makes them effective for process coordination but limited in their ability to engage in joint problem-solving with humans or contribute new ideas. We introduce MultiColleagues, a multi-agent conversational system that shows how AI agents can act as colleagues by conversing with each other, sharing new ideas, and actively involving users in collaborative ideation. In a within-subjects study with 20 participants, we compared MultiColleagues to a single-agent baseline. Results show that MultiColleagues fostered stronger perceptions of social presence, produced ideas rated significantly higher in quality and novelty, and encouraged deeper elaboration. These findings demonstrate the potential of AI agents to move beyond process partners toward colleagues that share intent, strengthen group dynamics, and collaborate with humans to advance ideas.",
      "author": "Kexin Quan, Dina Albassam, Mengke Wu, Zijian Ding, Jessie Chin",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 137,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982116+00:00",
      "updated_at": "2025-10-29T17:43:43.982117+00:00"
    },
    {
      "id": "7cfb090e0a1697e9501226d2edc7805c",
      "url": "https://arxiv.org/abs/2510.23887",
      "title": "MORA: AI-Mediated Story-Based practice for Speech Sound Disorder from Clinic to Home",
      "content": "arXiv:2510.23887v1 Announce Type: new \nAbstract: Speech sound disorder is among the most common communication challenges in preschool children. Home-based practice is essential for effective therapy and for acquiring generalization of target sounds, yet sustaining engaging and consistent practice remains difficult. Existing story-based activities, despite their potential for sound generalization and educational benefits, are often underutilized due to limited interactivity. Moreover, many practice tools fail to sufficiently integrate speech--language pathologists into the process, resulting in weak alignment with clinical treatment plans. To address these limitations, we present MORA, an interactive story-based practice system. MORA introduces three key innovations. First, it embeds target sounds and vocabulary into dynamic, character-driven conversational narratives, requiring children to actively produce speech to progress the story, thereby creating natural opportunities for exposure, repetition, and generalization. Second, it provides visual cues, explicit instruction, and feedback, allowing children to practice effectively either independently or with caregivers. Third, it supports an AI-in-the-loop workflow, enabling SLPs to configure target materials, review logged speech with phoneme-level scoring, and adapt therapy plans asynchronously -- bridging the gap between clinic and home practice while respecting professional expertise. A formative study with six licensed SLPs informed the system's design rationale, and an expert review with seven SLPs demonstrated strong alignment with established articulation-based treatments, as well as potential to enhance children's engagement and literacy. Furthermore, discussions highlight the design considerations for professional support and configurability, adaptive and multimodal child interaction, while highlighting MORA's broader applicability across speech and language disorders.",
      "author": "Sumin Hong, Xavier Briggs, Qingxiao Zheng, Yao Du, Jinjun Xiong, Toby Jia-jun Li",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 245,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982089+00:00",
      "updated_at": "2025-10-29T17:43:43.982090+00:00"
    },
    {
      "id": "d55991c08f4afda43133adc983f4f73b",
      "url": "https://arxiv.org/abs/2510.23875",
      "title": "Large Language Model Agent Personality and Response Appropriateness: Evaluation by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model",
      "content": "arXiv:2510.23875v1 Announce Type: new \nAbstract: While Large Language Model (LLM)-based agents can be used to create highly engaging interactive applications through prompting personality traits and contextual data, effectively assessing their personalities has proven challenging. This novel interdisciplinary approach addresses this gap by combining agent development and linguistic analysis to assess the prompted personality of LLM-based agents in a poetry explanation task. We developed a novel, flexible question bank, informed by linguistic assessment criteria and human cognitive learning levels, offering a more comprehensive evaluation than current methods. By evaluating agent responses with natural language processing models, other LLMs, and human experts, our findings illustrate the limitations of purely deep learning solutions and emphasize the critical role of interdisciplinary design in agent development.",
      "author": "Eswari Jayakumar, Niladri Sekhar Dash, Debasmita Mukherjee",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 121,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982052+00:00",
      "updated_at": "2025-10-29T17:43:43.982054+00:00"
    },
    {
      "id": "e5e7aee1c880c31a86f4a37ae90c8b37",
      "url": "https://arxiv.org/abs/2510.23848",
      "title": "Spatial Orchestra: Locomotion Music Instruments through Spatial Exploration",
      "content": "arXiv:2510.23848v1 Announce Type: new \nAbstract: Spatial Orchestra demonstrates how easy it is to play musical instruments using basic input like natural locomotion, which is accessible to most. Unlike many musical instruments, our work allows individuals of all skill levels to effortlessly create music by walking into virtual bubbles. Our Augmented Reality experience involves interacting with ever-shifting sound bubbles that the user engages with by stepping into color-coded bubbles within the assigned area using a standalone AR headset. Each bubble corresponds to a cello note, and omits sound from the center of the bubble, and lets the user hear and express in spatial audio, effectively transforming participants into musicians. This interactive element enables users to explore the intersection of spatial awareness, musical rhythm that extends to bodily expression through playful movements and dance-like gestures within the bubble-filled environment. This unique experience illuminates the intricate relationship between spatial awareness and the art of musical performance.",
      "author": "You-Jin Kim, Myungin Lee, Marko Peljhan, JoAnn Kuchera-Morin, Tobias H\\\"ollerer",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982023+00:00",
      "updated_at": "2025-10-29T17:43:43.982025+00:00"
    },
    {
      "id": "83b369db88083c78c05ecfb53d7b290a",
      "url": "https://arxiv.org/abs/2510.23840",
      "title": "Reality Distortion Room: A Study of User Locomotion Responses to Spatial Augmented Reality Effects",
      "content": "arXiv:2510.23840v1 Announce Type: new \nAbstract: Reality Distortion Room (RDR) is a proof-of-concept augmented reality system using projection mapping and unencumbered interaction with the Microsoft RoomAlive system to study a user's locomotive response to visual effects that seemingly transform the physical room the user is in. This study presents five effects that augment the appearance of a physical room to subtly encourage user motion. Our experiment demonstrates users' reactions to the different distortion and augmentation effects in a standard living room, with the distortion effects projected as wall grids, furniture holograms, and small particles in the air. The augmented living room can give the impression of becoming elongated, wrapped, shifted, elevated, and enlarged. The study results support the implementation of AR experiences in limited physical spaces by providing an initial understanding of how users can be subtly encouraged to move throughout a room.",
      "author": "You-Jin Kim, Andrew D. Wilson, Jennifer Jacobs, Tobias H\\\"ollerer",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 142,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.981987+00:00",
      "updated_at": "2025-10-29T17:43:43.981990+00:00"
    },
    {
      "id": "eec868a6e2fc777b5af395ead81d36b3",
      "url": "https://arxiv.org/abs/2510.20958",
      "title": "NeuroPilot: A Realtime Brain-Computer Interface system to enhance concentration of students in online learning",
      "content": "arXiv:2510.20958v2 Announce Type: replace-cross \nAbstract: The prevalence of online learning poses a vital challenge in real-time monitoring of students' concentration. Traditional methods such as questionnaire assessments require manual intervention, and webcam-based monitoring fails to provide accurate insights about learners' mental focus as it is deceived by mere screen fixation without cognitive engagement. Existing BCI-based approaches lack real-time validation and evaluation procedures. To address these limitations, a Brain-Computer Interface (BCI) system is developed using a non-invasive Electroencephalogram (EEG) headband, FocusCalm, to record brainwave activity under attentive and non-attentive states. 20 minutes of data were collected from each of 20 participants watching a pre-recorded educational video. The data validation employed a novel intra-video questionnaire assessment. Subsequently, collected signals were segmented (sliding window), filtered (Butterworth bandpass), and cleaned (removal of high- amplitude and EOG artifacts such as eye blinks). Time, frequency, wavelet, and statistical features were extracted, followed by recursive feature elimination (RFE) with support vector machines (SVMs) to classify attention and non-attention states. The leave-one-subject-out (LOSO) cross-validation accuracy was found to be 88.77%. The system provides feedback alerts upon detection of a non-attention state and maintains focus profile logs. A pilot study was conducted to evaluate the effectiveness of real-time feedback. Five participants underwent a 10-minute session comprising a 5-minute baseline phase devoid of feedback, succeeded by a 5-minute feedback phase, during which alerts were activated if participants exhibited inattention for approximately 8 consecutive seconds. A paired t-test (t = 5.73, p = 0.007) indicated a statistically significant improvement in concentration during the feedback phase.",
      "author": "Asif Islam, Farhan Ishtiaque, Md. Muhyminul Haque, Farhana Sarker, Ravi Vaidyanathan, Khondaker A. Mamun",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 253,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:42.905984+00:00",
      "updated_at": "2025-10-29T17:43:42.905985+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "69a981fc0bd4f82123c01313f3e8ff3d",
      "url": "http://ieeexplore.ieee.org/document/11210870",
      "title": "IEEE Transactions on Biomedical Engineering Information for Authors",
      "content": "null",
      "author": "",
      "published_date": "2025-10-21T13:16:30+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:49.109706+00:00",
      "updated_at": "2025-10-29T18:22:29.693002+00:00",
      "metadata": {
        "processed_at": "2025-10-29T18:22:29.693014+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "c6f0d0b84e49febca3da2525cc567a0d",
      "url": "http://ieeexplore.ieee.org/document/11210864",
      "title": "IEEE Engineering in Medicine and Biology Society Publication Information",
      "content": "null",
      "author": "",
      "published_date": "2025-10-21T13:16:29+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:49.109685+00:00",
      "updated_at": "2025-10-29T18:22:29.693042+00:00",
      "metadata": {
        "processed_at": "2025-10-29T18:22:29.693044+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f81ce0f85283a86d6b8262d8422a9d77",
      "url": "http://ieeexplore.ieee.org/document/11210866",
      "title": "Front Cover",
      "content": "null",
      "author": "",
      "published_date": "2025-10-21T13:16:29+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:49.109660+00:00",
      "updated_at": "2025-10-29T18:22:29.693047+00:00",
      "metadata": {
        "processed_at": "2025-10-29T18:22:29.693052+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "1a98eae853ecfd8f8f7641114ac67d2d",
      "url": "http://ieeexplore.ieee.org/document/10764720",
      "title": "Immunomechanobiology: Engineering the Activation and Function of Immune Cells With the Mechanical Signal of Fluid Shear Stress",
      "content": "Immunomechanobiology, the study of how physical forces influence the behavior and function of immune cells, is a rapidly growing area of research. It is becoming increasingly recognized that mechanical stimuli, such as fluid shear forces, are a critical determinant of immune cell regulation. In this review, we discuss the principles and significance of various mechanical forces present within the human body, with a focus on fluid shear flow and its impact on immune cell activation and function. Moreover, we discuss engineering approaches used to study immune cell mechanobiology, and their implications in health and diseases such as cancer, autoimmune disorders, and infectious disease.",
      "author": "",
      "published_date": "2024-11-22T13:16:46+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 103,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:47.980288+00:00",
      "updated_at": "2025-10-29T18:22:29.693055+00:00",
      "metadata": {
        "processed_at": "2025-10-29T18:22:29.693056+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "34b996c0e0c43288b9a47eaba2f43783",
      "url": "https://arxiv.org/abs/2510.24070",
      "title": "Building AI Literacy at Home: How Families Navigate Children's Self-Directed Learning with AI",
      "content": "arXiv:2510.24070v1 Announce Type: new \nAbstract: As generative AI becomes embedded in children's learning spaces, families face new challenges in guiding its use. Middle childhood (ages 7-13) is a critical stage where children seek autonomy even as parental influence remains strong. Using self-directed learning (SDL) as a lens, we examine how parents perceive and support children's developing AI literacy through focus groups with 13 parent-child pairs. Parents described evolving phases of engagement driven by screen time, self-motivation, and growing knowledge. While many framed AI primarily as a study tool, few considered its non-educational roles or risks, such as privacy and infrastructural embedding. Parents also noted gaps in their own AI understanding, often turning to joint exploration and engagement as a form of co-learning. Our findings reveal how families co-construct children's AI literacy, exposing tensions between practical expectations and critical literacies, and provide design implications that foster SDL while balancing autonomy and oversight.",
      "author": "Jingyi Xie, Chuhao Wu, Ge Wang, Rui Yu, He Zhang, Ronald Metoyer, Si Chen",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-29T17:43:43.982273+00:00",
      "updated_at": "2025-10-29T18:22:29.693059+00:00",
      "metadata": {
        "processed_at": "2025-10-29T18:22:29.693060+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "c26d89d902991a3e13b4738807303d24",
      "url": "http://www.jneurosci.org/cgi/content/short/45/39/etwij45392025?rss=1",
      "title": "This Week in The Journal",
      "content": "",
      "author": "McKeon, P.",
      "published_date": "2025-09-24T16:30:28+00:00",
      "source": "Journal Neuroscience This Week",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-10-29T15:46:08.327294+00:00",
      "updated_at": "2025-10-29T16:20:03.508443+00:00",
      "metadata": {
        "processed_at": "2025-10-29T16:20:03.508454+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "fc7bc6dd862b31cbe580ad5f4da30838",
      "url": "http://www.jneurosci.org/cgi/content/short/45/40/etwij45402025?rss=1",
      "title": "This Week in The Journal",
      "content": "",
      "author": "McKeon, P.",
      "published_date": "2025-10-01T16:30:31+00:00",
      "source": "Journal Neuroscience This Week",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-10-29T15:46:08.327275+00:00",
      "updated_at": "2025-10-29T16:20:03.508458+00:00",
      "metadata": {
        "processed_at": "2025-10-29T16:20:03.508460+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a42022e7aa2272bd80755edaade3bafd",
      "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2512855122?af=R",
      "title": "Neuronal plasticity at puberty in mouse hypothalamic Kiss1 neurons that control fertility",
      "content": "Proceedings of the National Academy of Sciences, Volume 122, Issue 43, October 2025. <br />SignificancePuberty is required for mammals to achieve fertility, which ensures continuation of the species. By the end of puberty, a regular cycle in females allows the periodic release of eggs. This is regulated by hormonal feedback loops between the ...",
      "author": "Yuanxin ZhangLeonie M. PakulatSzabolcs Tak\u00e1csLauren CampbellElisa GallianoErik HrabovszkyWilliam H. ColledgeSusan JonesaDepartment of Physiology, Development and Neuroscience, University of Cambridge, Cambridge CB2 3EG, United KingdombLaboratory of Reproductive Neurobiology, Hungarian Research Network, Institute of Experimental Medicine, Budapest 1083, Hungary",
      "published_date": "2025-10-21T07:00:00+00:00",
      "source": "Pnas Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 54,
      "reading_time": 1,
      "created_at": "2025-10-29T15:46:05.475587+00:00",
      "updated_at": "2025-10-29T16:20:03.508463+00:00",
      "metadata": {
        "processed_at": "2025-10-29T16:20:03.508464+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a778033399372a2d88d49c00984d5fa1",
      "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2507291122?af=R",
      "title": "From retinotopic to ordinal coding: Dissecting the cortical stages of visual word recognition",
      "content": "Proceedings of the National Academy of Sciences, Volume 122, Issue 43, October 2025. <br />SignificanceFluent reading requires that the brain encodes the order of letters within a word, yet without regard for where they appear on the retina. Information must be transformed from a retinotopic code into a position-invariant word form. Here, we ...",
      "author": "Aakash AgrawalStanislas DehaeneaCognitive Neuroimaging Unit, Commissariat \u00e0 l\u2019\u00e9nergie atomique et aux \u00e9nergies alternatives, INSERM, Universit\u00e9 Paris-Saclay, NeuroSpin Center, Gif-sur-Yvette 91191, FrancebColl\u00e8ge de France, Universit\u00e9 Paris-Sciences-Lettres, Paris 75005, France",
      "published_date": "2025-10-21T07:00:00+00:00",
      "source": "Pnas Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 54,
      "reading_time": 1,
      "created_at": "2025-10-29T15:46:05.475518+00:00",
      "updated_at": "2025-10-29T16:20:03.508467+00:00",
      "metadata": {
        "processed_at": "2025-10-29T16:20:03.508468+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "1eae4f6e9da31aa451320cb4b5e6f020",
      "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2503576122?af=R",
      "title": "Descattering and image restoration with a transformer-based neural network in deep tissue imaging",
      "content": "Proceedings of the National Academy of Sciences, Volume 122, Issue 43, October 2025. <br />SignificanceThe significant contributions of this work are threefold. First, it leverages deep learning to extend in vivo imaging depth of two-photon excitation fluorescence microscopy, far beyond the depths achieved by state-of-the-art methods. Second, ...",
      "author": "Xiangcong XuRenlong ZhangChenggui LuoChi ZhangYanping LiDanying LinBin YuLiwei LiuXiaoyu WengYiping WangLingjie KongJia LiJunle QuaState Key Laboratory of Radio Frequency Heterogeneous Integration, Key Laboratory of Optoelectronic Devices and Systems of Ministry of Education and Guangdong Province, College of Physics and Optoelectronic Engineering, Shenzhen University, Shenzhen 518060, ChinabState Key Laboratory of Precision Measurement Technology and Instruments, Department of Precision Instrument, Tsinghua University, Beijing 100084, China",
      "published_date": "2025-10-21T07:00:00+00:00",
      "source": "Pnas Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 48,
      "reading_time": 1,
      "created_at": "2025-10-29T15:46:05.475495+00:00",
      "updated_at": "2025-10-29T16:20:03.508470+00:00",
      "metadata": {
        "processed_at": "2025-10-29T16:20:03.508472+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}