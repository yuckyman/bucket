{
  "last_updated": "2025-09-12T01:03:09.220238+00:00",
  "pending_count": 985,
  "processed_count": 15,
  "pending_articles": [
    {
      "id": "1821104b44f4b8d0a9e10c3a0142d919",
      "url": "http://ieeexplore.ieee.org/document/10912489",
      "title": "A Receive-Only Frequency Translation System With Automatic Phase Correction for Simultaneous Multi-Nuclear MRI/MRS",
      "content": "Objective: Receive-only frequency translation enables MRI scanners with X-nuclear capabilities to perform simultaneous/interleaved multi-nuclear experiments. Mixing only on the receive side avoids modifying the transmit path, which often has narrow-band components. However, phase incoherence is introduced at the radio frequency mixer due to differing local oscillator frequencies between transmit and receive, necessitating phase correction. This paper presents a hardware solution for automatic phase correction during scans, eliminating the need for retrospective correction and allowing flexible scan parameter adjustments. Methods: The hardware solution detects phase changes in the system LO (local oscillator) between transmit and receive, calculates, and applies phase correction in the translator LO in real time. Programming spare TTL signals and accessing the scanner system LO are required to implement the phase correction method. Results: Phase correction accuracy was evaluated via averaged 31P spectroscopy and 23Na imaging. On top of the noise introduced by the additional mixer, the imperfect phase correction resulted in approximately 3% SNR loss at both frequencies. The corrected 23Na signal exhibited approximately an 8-degree phase standard deviation, compared to 6 degrees in the reference signal. Conclusion: The proposed hardware solution effectively corrects phase incoherence introduced by receive-only frequency translation. While minor imperfection exists, future upgrades are expected to improve the phase correction accuracy. Significance: This approach eliminates the need for retrospective phase correction when using receive-only frequency translation techniques for multi-nuclear acquisition, enabling real-time data acquisition and greater flexibility in scan parameter adjustment for simultaneous/interleaved multi-nuclear experiments.",
      "author": "",
      "published_date": "2025-03-05T13:18:14+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 242,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917797+00:00",
      "updated_at": "2025-09-11T23:37:56.917799+00:00"
    },
    {
      "id": "4d6fae8d345f1313c5810859c66c8593",
      "url": "http://ieeexplore.ieee.org/document/10910234",
      "title": "Deep Learning-Based Saturation Compensation for High Dynamic Range Multispectral Fluorescence Lifetime Imaging",
      "content": "In multispectral fluorescence lifetime imaging (FLIm), achieving consistent imaging quality across all spectral channels is crucial for accurately identifying a wide range of fluorophores. However, these essential measurements are frequently compromised by saturation artifacts due to the inherently limited dynamic range of detection systems. To address this issue, we present SatCompFLImNet, a deep learning-based network specifically designed to correct saturation artifacts in multispectral FLIm, facilitating high dynamic range applications. Leveraging generative adversarial networks, SatCompFLImNet effectively compensates for saturated fluorescence signals, ensuring accurate lifetime measurements across various levels of saturation. Extensively validated with simulated and real-world data, SatCompFLImNet demonstrates remarkable capability in correcting saturation artifacts, improving signal-to-noise ratios, and maintaining fidelity of lifetime measurements. By enabling reliable fluorescence lifetime measurements under a variety of saturation conditions, SatCompFLImNet paves the way for improved diagnostic tools and a deeper understanding of biological processes, making it a pivotal advancement for research and clinical diagnostics in tissue characterization and disease pathogenesis.",
      "author": "",
      "published_date": "2025-03-05T13:18:14+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 156,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917760+00:00",
      "updated_at": "2025-09-11T23:37:56.917761+00:00"
    },
    {
      "id": "06973eac53b05e4d571fb78c4dd20a03",
      "url": "http://ieeexplore.ieee.org/document/10910246",
      "title": "mmWave Radar for Sit-to-Stand Analysis: A Comparative Study With Wearables and Kinect",
      "content": "This study investigates a novel approach for analyzing Sit-to-Stand (STS) movements using millimeter-wave (mmWave) radar technology, aiming to develop a non-contact, privacy-preserving, and all-day operational solution for healthcare applications. A 60 GHz mmWave radar system was employed to collect radar point cloud data from 45 participants performing STS motions. Using a deep learning-based pose estimation model and Inverse Kinematics (IK), we calculated joint angles, segmented STS motions, and extracted clinically relevant features for fall risk assessment. The extracted features were compared with those obtained from Kinect and wearable sensors. While Kinect provided a reference for motion capture, we acknowledge its limitations compared to the gold-standard VICON system, which is planned for future validation. The results demonstrated that mmWave radar effectively captures general motion patterns and large joint movements (e.g., trunk), though challenges remain for more fine-grained motion analysis. This study highlights the unique advantages and limitations of mmWave radar and other sensors, emphasizing the potential of integrated sensor technologies to enhance the accuracy and reliability of motion analysis in clinical and biomedical research. Future work will expand the scope to more complex movements and incorporate high-precision motion capture systems to further validate the findings.",
      "author": "",
      "published_date": "2025-03-05T13:18:14+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 194,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917728+00:00",
      "updated_at": "2025-09-11T23:37:56.917730+00:00"
    },
    {
      "id": "8235c9d6533d87d72c2b2e83aee99f8c",
      "url": "http://ieeexplore.ieee.org/document/10932717",
      "title": "Camera Seismocardiogram Based Monitoring of Left Ventricular Ejection Time",
      "content": "Left Ventricular Ejection Time (LVET), reflecting the duration from the onset to the end of blood ejection by the left ventricle during each heartbeat, is a critical parameter for measuring cardiac pumping efficiency. Continuous and regular monitoring of LVET is particularly crucial in assessing cardiac health, valvular function, and myocardial contractility. Seismocardiogram (SCG) signals can be utilized for LVET monitoring, as the temporal distance between the aortic valve opening (AO) and aortic valve closure (AC) in SCG signals can accurately depict LVET. This study proposes a novel way to extract LVET from laser speckle videos recorded by a remote camera based on the principle of defocused speckle imaging, thereby enabling non-contact monitoring of LVET. We extract both the low-frequency components of laser speckle motion (LSM-LF), regarded as SCG signals, and the high-frequency components of laser speckle motion (LSM-HF) from recorded videos. We utilize LSM-HF to assist the detection of AO and AC markers in LSM-LF. We validated the effectiveness of our AO and AC detection algorithm on a self-made dataset comprising 21 participants with 9616 SCG cycles. The benchmark shows that the detection accuracy for AO and AC reached 98.16% and 97.94%, respectively, with an mean absolute error of 0.5571 ms for LVET estimation. The results demonstrate that camera-SCG has strong potential for cardiac health monitoring.",
      "author": "",
      "published_date": "2025-03-18T13:19:56+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 216,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917695+00:00",
      "updated_at": "2025-09-11T23:37:56.917697+00:00"
    },
    {
      "id": "4a5f7d3749b3c2e953669a61a2f468e2",
      "url": "http://ieeexplore.ieee.org/document/10909585",
      "title": "Biophysical Modeling of Capacitive Electro-Quasistatic Human Body Powering",
      "content": "The increasing demand for wearables necessitates efficient energy harvesting and wireless power transfer solutions. Capacitive Electro-Quasistatic Human Body Powering (EQS-HBP) is a promising technology for wirelessly powering on-body devices, offering enhanced received power ($P_{rx}$) with full-body coverage. Unlike EQS Human Body Communication (EQS-HBC), which optimizes channel capacity, EQS-HBP focuses on maximizing ${\\rm P_{rx}}$, requiring a distinct biophysical model tailored to lower termination impedance ranges where ${\\rm P_{rx}}$ peaks. This paper presents comprehensive simulations\u2014finite element method (FEM), distributed circuit modeling\u2014and in-vivo experiments to characterize the body channel as a finite impedance wire, with impedance determined by body dimensions. Contact impedance between the body and receiver, inversely related to contact area, significantly affects ${\\rm P_{rx}}$, necessitating careful design for devices with small contact areas. Furthermore, the body cross-sectional area influences voltage recovery after the point of load, with smaller cross-sections yielding reduced recovery. A lumped circuit model is developed to encapsulate these findings with circuit techniques to maximize ${\\rm P_{rx}}$, demonstrating that series resonance in a ground-floated receiver reduces input impedance by over 65x and improves ${\\rm P_{rx}}$ by more than 25\u00d7 over parallel resonance. We also propose a method to approximate optimal loading impedance for various receiver configurations and analyze the impact of inductor Q factor. We prove that neither series nor parallel resonance can mitigate the transmitter return path capacitance. These insights enable the development of a much higher on-body wireless power transfer method, advancing wearable device technology for applications in healthcare, fitness, and beyond.",
      "author": "",
      "published_date": "2025-03-04T13:17:42+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 245,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917657+00:00",
      "updated_at": "2025-09-11T23:37:56.917659+00:00"
    },
    {
      "id": "9b44787c5e2702b4fdc91e130b20f366",
      "url": "http://ieeexplore.ieee.org/document/11133578",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-08-21T13:16:36+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917603+00:00",
      "updated_at": "2025-09-11T23:37:56.917604+00:00"
    },
    {
      "id": "a4b47c7385c6d3578aa6dd12bdf4d533",
      "url": "http://ieeexplore.ieee.org/document/11133580",
      "title": "IEEE Transactions on Biomedical Engineering Handling Editors Information",
      "content": "null",
      "author": "",
      "published_date": "2025-08-21T13:16:36+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917584+00:00",
      "updated_at": "2025-09-11T23:37:56.917586+00:00"
    },
    {
      "id": "2cfd0cedafa67901906c1ebc99c10bc6",
      "url": "http://ieeexplore.ieee.org/document/11133584",
      "title": "IEEE Transactions on Biomedical Engineering Information for Authors",
      "content": "null",
      "author": "",
      "published_date": "2025-08-21T13:16:36+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917565+00:00",
      "updated_at": "2025-09-11T23:37:56.917567+00:00"
    },
    {
      "id": "a8434a15ba395c5b190d3e891a9e4c81",
      "url": "http://ieeexplore.ieee.org/document/11133581",
      "title": "IEEE Engineering in Medicine and Biology Society Publication Information",
      "content": "null",
      "author": "",
      "published_date": "2025-08-21T13:16:36+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917544+00:00",
      "updated_at": "2025-09-11T23:37:56.917546+00:00"
    },
    {
      "id": "0799111e2bad0cabd757c51fb0a9dfe8",
      "url": "http://ieeexplore.ieee.org/document/11133582",
      "title": "Front Cover",
      "content": "null",
      "author": "",
      "published_date": "2025-08-21T13:16:36+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:56.917519+00:00",
      "updated_at": "2025-09-11T23:37:56.917522+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "6fd093bbcffe4c4c83b3f4e66b81ed55",
      "url": "http://ieeexplore.ieee.org/document/10854900",
      "title": "Enhancing Human Navigation Ability Using Force-Feedback From a Lower-Limb Exoskeleton",
      "content": "Humans operating in dynamic environments with limited visibility are susceptible to collisions with moving objects, occupational hazards, and/or other agents, which can result in personal injuries or fatalities. Most existing research has focused on using vibrotactile cues to address this challenge. In this work, we propose a fundamentally new approach that utilizes variable impedance on an active exoskeleton to guide humans away from hazards and towards safe areas. This framework combines artificial potential fields with current impedance-based theories of exoskeleton control to provide a comprehensive navigational system that is intuitive for human operators. First, we present the mathematical framework to encode information about the locations of obstacles and the safest direction in which to move. Next, we optimize controller parameters in a series of human-subject experiments. Finally, we evaluate the framework in virtual reality on a set of randomly generated obstacle fields in environments where vision is either fully or partially occluded. Our results suggest that the exoskeleton provides significant separation from obstacles and reduced collisions compared to vision alone in conditions where visibility was limited to less than 1.3 m. Our work demonstrates that force-feedback in parallel with a human can improve overall navigation ability in low visibility conditions.",
      "author": "",
      "published_date": "2025-01-27T13:17:36+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:59.199807+00:00",
      "updated_at": "2025-09-12T01:03:09.120396+00:00",
      "metadata": {
        "processed_at": "2025-09-12T01:03:09.120406+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "17db5ad14cc374d9748b814b27ea2e66",
      "url": "http://ieeexplore.ieee.org/document/10858441",
      "title": "Human-in-the-Loop Optimization of Perceived Realism of Multi-Modal Haptic Rendering Under Conflicting Sensory Cues",
      "content": "During haptic rendering, a visual display and a haptic interface are commonly utilized together to elicit multi-sensory perception of a virtual object, through a combination and integration of force-related and movement-related cues. In this study, we explore visual-haptic cue integration during multi-modal haptic rendering under conflicting cues and propose a systematic means to determine the optimal visual scaling for haptic manipulation that maximizes the perceived realism of spring rendering for a given haptic interface. We show that the parameters affecting visual-haptic congruency can be effectively optimized through a qualitative feedback-based human-in-the-loop (HiL) optimization to ensure a consistently high rating of perceived realism. Accordingly, the multi-modal perception of users can be successfully enhanced by solely modulating the visual feedback without altering the haptic feedback, to make virtual environments feel stiffer or more compliant, significantly extending the range of perceived stiffness levels for a haptic interface. We extend our results to a group of individuals to capture the multi-dimensional psychometric field that characterizes the cumulative effect of feedback modalities utilized during sensory cue integration under conflicts. Our results not only provide reliable estimates of just noticeable difference thresholds for stiffness with and without visual scaling but also capture all the prominent features of sensory cue integration, indicating weights that are proportional to the congruency level of manipulated visual signals. Overall, preference-based HiL optimization excels as a systematic and efficient method of studying multi-modal perception under conflicts.",
      "author": "",
      "published_date": "2025-01-30T13:17:17+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:59.199773+00:00",
      "updated_at": "2025-09-12T01:03:09.120410+00:00",
      "metadata": {
        "processed_at": "2025-09-12T01:03:09.120412+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "420f9b78683bd639b58044e9d92b9f8d",
      "url": "http://ieeexplore.ieee.org/document/10829673",
      "title": "The Impact of Stimulation Parameters on Reaction Times Following Transcutaneous Electrical Stimulation in the Lower Leg",
      "content": "The growing need for human-machine interfaces (HMIs) underscores the importance of sensory feedback, with electrical stimulation offering efficient interaction in various applications. While its sensory effects are extensively studied, investigations into the reaction time (RT) following transcutaneous electrical stimulation (TES) remain limited. This study aimed to evaluate how stimulation parameters influence RT. We examined RT and RT variability among twenty healthy participants aged 21 to 61 years. Participants underwent 16 stimulation patterns (10 repetitions per pattern) with combinations of four pulse frequencies (4, 26, 48, 70 Hz) and four pulse amplitudes (1.5, 2.0, 2.5, 3.0 times of sensory threshold) on four skin locations in the lower leg above peroneal nerve, tibial nerve, tibialis anterior muscle, and a lateral shank control site. RT was assessed as participants dorsiflexed their foot in response to electrical stimulation. Results revealed that both RT and its variability decreased as pulse frequency and amplitude increased, and there was an interaction effect between pulse frequency and amplitude. However, no significant difference was found in RT across stimulation locations. These findings demonstrate how stimulation parameters affect the speed and efficiency of communication between the user and the stimulator, showing promises for augmenting real-time feedback HMIs.",
      "author": "",
      "published_date": "2025-01-06T13:18:28+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 197,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:59.199733+00:00",
      "updated_at": "2025-09-12T01:03:09.120414+00:00",
      "metadata": {
        "processed_at": "2025-09-12T01:03:09.120415+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a2894e94b4b446654a990f5bfdaf5c94",
      "url": "http://ieeexplore.ieee.org/document/11049834",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-06-24T13:47:09+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:59.199689+00:00",
      "updated_at": "2025-09-12T01:03:09.120418+00:00",
      "metadata": {
        "processed_at": "2025-09-12T01:03:09.120419+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "21b80613927d441f07edfa4c79226a4d",
      "url": "http://ieeexplore.ieee.org/document/11049812",
      "title": "Front Cover",
      "content": "null",
      "author": "",
      "published_date": "2025-06-24T13:47:09+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-09-11T23:37:59.199664+00:00",
      "updated_at": "2025-09-12T01:03:09.120421+00:00",
      "metadata": {
        "processed_at": "2025-09-12T01:03:09.120423+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "6a78ace4c736083f37ec2330b4f03b5f",
      "url": "https://arxiv.org/abs/2509.08539",
      "title": "Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning",
      "content": "arXiv:2509.08539v1 Announce Type: new \nAbstract: This paper examines the generalization capacity of two state-of-the-art classification and similarity learning models in reliably identifying users based on their motions in various Extended Reality (XR) applications. We developed a novel dataset containing a wide range of motion data from 49 users in five different XR applications: four XR games with distinct tasks and action patterns, and an additional social XR application with no predefined task sets. The dataset is used to evaluate the performance and, in particular, the generalization capacity of the two models across applications. Our results indicate that while the models can accurately identify individuals within the same application, their ability to identify users across different XR applications remains limited. Overall, our results provide insight into current models generalization capabilities and suitability as biometric methods for user verification and identification. The results also serve as a much-needed risk assessment of hazardous and unwanted user identification in XR and Metaverse applications. Our cross-application XR motion dataset and code are made available to the public to encourage similar research on the generalization of motion-based user identification in typical Metaverse application use cases.",
      "author": "Lukas Schach, Christian Rack, Ryan P. McMahan, Marc Erich Latoschik",
      "published_date": "2025-09-11T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2025-09-11T21:38:02.791223+00:00",
      "updated_at": "2025-09-11T22:12:58.318569+00:00",
      "metadata": {
        "processed_at": "2025-09-11T22:12:58.318578+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "cbca39a58874b315212cdaef9205bfc0",
      "url": "https://arxiv.org/abs/2509.08514",
      "title": "Bias in the Loop: How Humans Evaluate AI-Generated Suggestions",
      "content": "arXiv:2509.08514v1 Announce Type: new \nAbstract: Human-AI collaboration increasingly drives decision-making across industries, from medical diagnosis to content moderation. While AI systems promise efficiency gains by providing automated suggestions for human review, these workflows can trigger cognitive biases that degrade performance. We know little about the psychological factors that determine when these collaborations succeed or fail. We conducted a randomized experiment with 2,784 participants to examine how task design and individual characteristics shape human responses to AI-generated suggestions. Using a controlled annotation task, we manipulated three factors: AI suggestion quality in the first three instances, task burden through required corrections, and performance-based financial incentives. We collected demographics, attitudes toward AI, and behavioral data to assess four performance metrics: accuracy, correction activity, overcorrection, and undercorrection. Two patterns emerged that challenge conventional assumptions about human-AI collaboration. First, requiring corrections for flagged AI errors reduced engagement and increased the tendency to accept incorrect suggestions, demonstrating how cognitive shortcuts influence collaborative outcomes. Second, individual attitudes toward AI emerged as the strongest predictor of performance, surpassing demographic factors. Participants skeptical of AI detected errors more reliably and achieved higher accuracy, while those favorable toward automation exhibited dangerous overreliance on algorithmic suggestions. The findings reveal that successful human-AI collaboration depends not only on algorithmic performance but also on who reviews AI outputs and how review processes are structured. Effective human-AI collaborations require consideration of human psychology: selecting diverse evaluator samples, measuring attitudes, and designing workflows that counteract cognitive biases.",
      "author": "Jacob Beck, Stephanie Eckman, Christoph Kern, Frauke Kreuter",
      "published_date": "2025-09-11T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 242,
      "reading_time": 1,
      "created_at": "2025-09-11T21:38:02.791178+00:00",
      "updated_at": "2025-09-11T22:12:58.318583+00:00",
      "metadata": {
        "processed_at": "2025-09-11T22:12:58.318585+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7c61ff75c8a5a742af2ce4e8c7c0f204",
      "url": "https://arxiv.org/abs/2509.08459",
      "title": "Printegrated Circuits: Personal Fabrication of 3D Printed Devices with Embedded PCBs",
      "content": "arXiv:2509.08459v1 Announce Type: new \nAbstract: Consumer-level multi-material 3D printing with conductive thermoplastics enables fabrication of interactive elements for bespoke tangible devices. However, large feature sizes, high resistance materials, and limitations of printable control circuitry mean that deployable devices cannot be printed without post-print assembly steps. To address these challenges, we present Printegrated Circuits, a technique that uses traditional electronics as material to 3D print self-contained interactive objects. Embedded PCBs are placed into recesses during a pause in the print, and through a process we term \\textit{Prinjection}, conductive filament is injected into their plated-through holes. This automatically creates reliable electrical and mechanical contact, eliminating the need for manual wiring or bespoke connectors. We describe the custom machine code generation that supports our approach, and characterise its electrical and mechanical properties. With our 6 demonstrations, we highlight how the Printegrated Circuits process fits into existing design and prototyping workflows as well as informs future research agendas.",
      "author": "Oliver Child, Ollie Hanton, Jack Dawson, Steve Hodges, Mike Fraser",
      "published_date": "2025-09-11T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-09-11T21:38:02.791137+00:00",
      "updated_at": "2025-09-11T22:12:58.318587+00:00",
      "metadata": {
        "processed_at": "2025-09-11T22:12:58.318589+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "dd9ecfa353dbf5c36757a8d7874f519c",
      "url": "https://arxiv.org/abs/2509.08444",
      "title": "GlyphWeaver: Unlocking Glyph Design Creativity with Uniform Glyph DSL and AI",
      "content": "arXiv:2509.08444v1 Announce Type: new \nAbstract: Expressive glyph visualizations provide a powerful and versatile means to represent complex multivariate data through compact visual encodings, but creating custom glyphs remains challenging due to the gap between design creativity and technical implementation. We present GlyphWeaver, a novel interactive system to enable an easy creation of expressive glyph visualizations. Our system comprises three key components: a glyph domain-specific language (GDSL), a GDSL operation management mechanism, and a multimodal interaction interface. The GDSL is a hierarchical container model, where each container is independent and composable, providing a rigorous yet practical foundation for complex glyph visualizations. The operation management mechanism restricts modifications of the GDSL to atomic operations, making it accessible without requiring direct coding. The multimodal interaction interface enables direct manipulation, natural language commands, and parameter adjustments. A multimodal large language model acts as a translator, converting these inputs into GDSL operations. GlyphWeaver significantly lowers the barrier for designers, who often do not have extensive programming skills, to create sophisticated glyph visualizations. A case study and user interviews with 13 participants confirm its substantial gains in design efficiency and effectiveness of producing creative glyph visualizations.",
      "author": "Can Liu, Shiwei Chen, Zhibang Jiang, Yong Wang",
      "published_date": "2025-09-11T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 190,
      "reading_time": 1,
      "created_at": "2025-09-11T21:38:02.791108+00:00",
      "updated_at": "2025-09-11T22:12:58.318591+00:00",
      "metadata": {
        "processed_at": "2025-09-11T22:12:58.318593+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "d465e969e01804609fdff1762cfcc021",
      "url": "https://arxiv.org/abs/2509.08404",
      "title": "HyperMOOC: Augmenting MOOC Videos with Concept-based Embedded Visualizations",
      "content": "arXiv:2509.08404v1 Announce Type: new \nAbstract: Massive Open Online Courses (MOOCs) have become increasingly popular worldwide. However, learners primarily rely on watching videos, easily losing knowledge context and reducing learning effectiveness. We propose HyperMOOC, a novel approach augmenting MOOC videos with concept-based embedded visualizations to help learners maintain knowledge context. Informed by expert interviews and literature review, HyperMOOC employs multi-glyph designs for different knowledge types and multi-stage interactions for deeper understanding. Using a timeline-based radial visualization, learners can grasp cognitive paths of concepts and navigate courses through hyperlink-based interactions. We evaluated HyperMOOC through a user study with 36 MOOC learners and interviews with two instructors. Results demonstrate that HyperMOOC enhances learners' learning effect and efficiency on MOOCs, with participants showing higher satisfaction and improved course understanding compared to traditional video-based learning approaches.",
      "author": "Li Ye, Lei Wang, Lihong Cai, Ruiqi Yu, Yong Wang, Yigang Wang, Wei Chen, Zhiguang Zhou",
      "published_date": "2025-09-11T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 131,
      "reading_time": 1,
      "created_at": "2025-09-11T21:38:02.791073+00:00",
      "updated_at": "2025-09-11T22:12:58.318595+00:00",
      "metadata": {
        "processed_at": "2025-09-11T22:12:58.318599+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}