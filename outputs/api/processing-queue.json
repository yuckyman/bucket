{
  "last_updated": "2025-11-20T06:22:50.096942+00:00",
  "pending_count": 721,
  "processed_count": 279,
  "pending_articles": [
    {
      "id": "c47dfaf5b1f0745b78b2441ead37582c",
      "url": "https://arxiv.org/abs/2511.15352",
      "title": "People readily follow personal advice from AI but it does not improve their well-being",
      "content": "arXiv:2511.15352v1 Announce Type: new \nAbstract: People increasingly seek personal advice from large language models (LLMs), yet whether humans follow their advice, and its consequences for their well-being, remains unknown. In a longitudinal randomised controlled trial with a representative UK sample (N = 2,302), 75% of participants who had a 20-minute discussion with GPT-4o about health, careers or relationships subsequently reported following its advice. Based on autograder evaluations of chat transcripts, LLM advice rarely violated safety best practice. When queried 2-3 weeks later, participants who had interacted with personalised AI (with access to detailed user information) followed its advice more often in the real world and reported higher well-being than those advised by non-personalised AI. However, while receiving personal advice from AI temporarily reduced well-being, no differential long-term effects compared to a control emerged. Our results suggest that humans readily follow LLM advice about personal issues but doing so shows no additional well-being benefit over casual conversations.",
      "author": "Lennart Luettgau, Vanessa Cheung, Magda Dubois, Keno Juechems, Jessica Bergs, Henry Davidson, Bessie O'Dell, Hannah Rose Kirk, Max Rollwage, Christopher Summerfield",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 156,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557280+00:00",
      "updated_at": "2025-11-20T05:21:37.557282+00:00"
    },
    {
      "id": "8a3a5b72fa3a82a00e89ea22d6348482",
      "url": "https://arxiv.org/abs/2511.15342",
      "title": "Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions",
      "content": "arXiv:2511.15342v1 Announce Type: new \nAbstract: Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.",
      "author": "Shan Shan",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 263,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557251+00:00",
      "updated_at": "2025-11-20T05:21:37.557252+00:00"
    },
    {
      "id": "8ea25b12a1cc56c874a3e3e41cf985c4",
      "url": "https://arxiv.org/abs/2511.15331",
      "title": "DesignerlyLoop: Bridging the Cognitive Gap through Visual Node-Based Reasoning in Human-AI Collaborative Design",
      "content": "arXiv:2511.15331v1 Announce Type: new \nAbstract: Large language models (LLMs) offer powerful support for design tasks, yet their goal-oriented, single-turn responses often misalign with the nonlinear, exploratory nature of design processes. This mismatch creates a cognitive gap, limiting designers' ability to articulate evolving intentions, critically evaluate outputs, and maintain creative agency. To address these challenges, we developed DesignerlyLoop, a visual node-based system that embeds LLM reasoning chains into the design workflow. The system enables designers to externalize and curate reasoning structures, iteratively organize intentions, and interact with LLMs as dynamic cognitive engines rather than static answer providers. We conducted a within-subject study with 20 designers, combining qualitative and quantitative methods, and found that DesignerlyLoop enhanced creative reflection, design quality, and interaction experience by supporting systematic engagement with both human and machine reasoning. These findings highlight the potential of structured, interactive visualization to transform human-AI co-creation into a reflective and iterative design process.",
      "author": "Anqi Wang, Zhengyi Li, Xin Tong, Pan Hui",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557213+00:00",
      "updated_at": "2025-11-20T05:21:37.557215+00:00"
    },
    {
      "id": "233bd4916ee8fadaebc4c4305cf9d601",
      "url": "https://arxiv.org/abs/2511.15253",
      "title": "PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback",
      "content": "arXiv:2511.15253v1 Announce Type: new \nAbstract: Effective presentation skills are essential in education, professional communication, and public speaking, yet learners often lack access to high-quality exemplars or personalized coaching. Existing AI tools typically provide isolated functionalities such as speech scoring or script generation without integrating reference modeling and interactive feedback into a cohesive learning experience. We introduce a dual-agent system that supports presentation practice through two complementary roles: the Ideal Presentation Agent and the Coach Agent. The Ideal Presentation Agent converts user-provided slides into model presentation videos by combining slide processing, visual-language analysis, narration script generation, personalized voice synthesis, and synchronized video assembly. The Coach Agent then evaluates user-recorded presentations against these exemplars, conducting multimodal speech analysis and delivering structured feedback in an Observation-Impact-Suggestion (OIS) format. To enhance the authenticity of the learning experience, the Coach Agent incorporates an Audience Agent, which simulates the perspective of a human listener and provides humanized feedback reflecting audience reactions and engagement. Together, these agents form a closed loop of observation, practice, and feedback. Implemented on a robust backend with multi-model integration, voice cloning, and error handling mechanisms, the system demonstrates how AI-driven agents can provide engaging, human-centered, and scalable support for presentation skill development in both educational and professional contexts.",
      "author": "Sirui Chen, Jinsong Zhou, Xinli Xu, Xiaoyu Yang, Litao Guo, Ying-Cong Chen",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 206,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557181+00:00",
      "updated_at": "2025-11-20T05:21:37.557182+00:00"
    },
    {
      "id": "8e21e6898baf5988ffaf1c44c3dafb88",
      "url": "https://arxiv.org/abs/2511.15218",
      "title": "Efficient Transformer-Integrated Deep Neural Architectures for Robust EEG Decoding of Complex Visual Imagery",
      "content": "arXiv:2511.15218v1 Announce Type: new \nAbstract: This study introduces a pioneering approach in brain-computer interface (BCI) technology, featuring our novel concept of complex visual imagery for non-invasive electroencephalography (EEG)-based communication. Complex visual imagery, as proposed in our work, involves the user engaging in the mental visualization of complex upper limb movements. This innovative approach significantly enhances the BCI system, facilitating the extension of its applications to more sophisticated tasks such as EEG-based robotic arm control. By leveraging this advanced form of visual imagery, our study opens new horizons for intricate and intuitive mind-controlled interfaces. We developed an advanced deep learning architecture that integrates functional connectivity metrics with a convolutional neural network-image transformer. This framework is adept at decoding subtle user intentions, addressing the spatial variability in complex visual tasks, and effectively translating these into precise commands for robotic arm control. Our comprehensive offline and pseudo-online evaluations demonstrate the framework's efficacy in real-time applications, including the nuanced control of robotic arms. The robustness of our approach is further validated through leave-one-subject-out cross-validation, marking a significant step towards versatile, subject-independent BCI applications. This research highlights the transformative impact of advanced visual imagery and deep learning in enhancing the usability and adaptability of BCI systems, particularly in robotic arm manipulation.",
      "author": "Byoung-Hee Kwon",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 206,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557143+00:00",
      "updated_at": "2025-11-20T05:21:37.557144+00:00"
    },
    {
      "id": "ff2524f38afb8b4fb4d4d0e331c00a0d",
      "url": "https://arxiv.org/abs/2511.15182",
      "title": "SWR-Viz: AI-assisted Interactive Visual Analytics Framework for Ship Weather Routing",
      "content": "arXiv:2511.15182v1 Announce Type: new \nAbstract: Efficient and sustainable maritime transport increasingly depends on reliable forecasting and adaptive routing, yet operational adoption remains difficult due to forecast latencies and the need for human judgment in rapid decision-making under changing ocean conditions. We introduce SWR-Viz, an AI-assisted visual analytics framework that combines a physics-informed Fourier Neural Operator wave forecast model with SIMROUTE-based routing and interactive emissions analytics. The framework generates near-term forecasts directly from current conditions, supports data assimilation with sparse observations, and enables rapid exploration of what-if routing scenarios. We evaluate the forecast models and SWR-Viz framework along key shipping corridors in the Japan Coast and Gulf of Mexico, showing both improved forecast stability and realistic routing outcomes comparable to ground-truth reanalysis wave products. Expert feedback highlights the usability of SWR-Viz, its ability to isolate voyage segments with high emission reduction potential, and its value as a practical decision-support system. More broadly, this work illustrates how lightweight AI forecasting can be integrated with interactive visual analytics to support human-centered decision-making in complex geospatial and environmental domains.",
      "author": "Subhashis Hazarika, Leonard Lupin-Jimenez, Rohit Vuppala, Ashesh Chattopadhyay, Hon Yung Wong",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 175,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557108+00:00",
      "updated_at": "2025-11-20T05:21:37.557110+00:00"
    },
    {
      "id": "0a1b2063f868601a0da04a05e1df812a",
      "url": "https://arxiv.org/abs/2511.15110",
      "title": "Eye Care You: Voice Guidance Application Using Social Robot for Visually Impaired People",
      "content": "arXiv:2511.15110v1 Announce Type: new \nAbstract: In the study, the device of social robot was designed for visually impaired users, and along with a mobile application for provide functions to assist their lives. Both physical and mental conditions of visually impaired users are considered, and the mobile application provides functions: photo record, mood lift, greeting guest and today highlight. The application was designed for visually impaired users, and uses voice control to provide a friendly interface. Photo record function allows visually impaired users to capture image immediately when they encounter danger situations. Mood lift function accompanies visually impaired users by asking questions, playing music and reading articles. Greeting guest function answers to the visitors for the inconvenient physical condition of visually impaired users. In addition, today highlight function read news including weather forecast, daily horoscopes and daily reminder for visually impaired users. Multiple tools were adopted for developing the mobile application, and a website was developed for caregivers to check statues of visually impaired users and for marketing of the application.",
      "author": "Ting-An Lin, Pei-Lin Tsai, Yi-An Chen, Feng-Yu Chen, Lyn Chao-ling Chen",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 170,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557077+00:00",
      "updated_at": "2025-11-20T05:21:37.557078+00:00"
    },
    {
      "id": "2fc3040634aec19e00aa2a00fe4779cb",
      "url": "https://arxiv.org/abs/2511.15013",
      "title": "Personalized targeted memory reactivation enhances consolidation of challenging memories via slow wave and spindle dynamics",
      "content": "arXiv:2511.15013v1 Announce Type: new \nAbstract: Sleep is crucial for memory consolidation, underpinning effective learning. Targeted memory reactivation (TMR) can strengthen neural representations by re-engaging learning circuits during sleep. However, TMR protocols overlook individual differences in learning capacity and memory trace strength, limiting efficacy for difficult-to-recall memories. Here, we present a personalized TMR protocol that adjusts stimulation frequency based on individual retrieval performance and task difficulty during a word-pair memory task. In an experiment comparing personalized TMR, TMR, and control groups, the personalized protocol significantly reduced memory decay and improved error correction under challenging recall. Electroencephalogram (EEG) analyses revealed enhanced synchronization of slow waves and spindles, with a significant positive correlation between behavioral and EEG features for challenging memories. Multivariate classification identified distinct neural signatures linked to the personalized approach, highlighting its ability to target memory-specific circuits. These findings provide novel insights into sleep-dependent memory consolidation and support personalized TMR interventions to optimize learning outcomes.",
      "author": "Gi-Hwan Shin, Young-Seok Kweon, Seungwon Oh, Seong-Whan Lee",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557045+00:00",
      "updated_at": "2025-11-20T05:21:37.557047+00:00"
    },
    {
      "id": "498f5cf7bc59b540a2e8c2619f348e7b",
      "url": "https://arxiv.org/abs/2511.15012",
      "title": "A Quantitative Framework for Assessing Sleep Quality from EEG Time Series in Complex Dynamic Systems",
      "content": "arXiv:2511.15012v1 Announce Type: new \nAbstract: Modern lifestyles contribute to insufficient sleep, impairing cognitive function and weakening the immune system. Sleep quality (SQ) is vital for physiological and mental health, making its understanding and accurate assessment critical. However, its multifaceted nature, shaped by neurological and environmental factors, makes precise quantification challenging. Here, we address this challenge by utilizing electroencephalography (EEG) for phase-amplitude coupling (PAC) analysis to elucidate the neurological basis of SQ, examining both states of sleep and wakefulness, including resting state (RS) and working memory. Our results revealed distinct patterns in beta power and delta connectivity in sleep and RS, together with the reaction time of working memory. A notable finding was the pronounced delta-beta PAC, a feature markedly stronger in individuals with good SQ. We further observed that SQ was positively correlated with increased delta-beta PAC. Leveraging these insights, we applied machine learning models to classify SQ at an individual level, demonstrating that the delta-beta PAC outperformed other EEG characteristics. These findings establish delta-beta PAC as a robust electrophysiological marker to quantify SQ and elucidate its neurological determinants.",
      "author": "Gi-Hwan Shin",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 179,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.557014+00:00",
      "updated_at": "2025-11-20T05:21:37.557016+00:00"
    },
    {
      "id": "a1b326a191f663e3038de9488f97a298",
      "url": "https://arxiv.org/abs/2511.14972",
      "title": "Harmful Traits of AI Companions",
      "content": "arXiv:2511.14972v1 Announce Type: new \nAbstract: Amid the growing prevalence of human -- AI interaction, large language models and other AI-based entities increasingly provide forms of companionship to human users. Such AI companionship -- i.e., bonded relationships between humans and AI systems that resemble the relationships people have with family members, friends, and romantic partners -- might substantially benefit humans. Yet such relationships can also do profound harm. We propose a framework for analyzing potential negative impacts of AI companionship by identifying specific harmful traits of AI companions and speculatively mapping causal pathways back from these traits to possible causes and forward to potential harmful effects. We provide detailed, structured analysis of four potentially harmful traits -- the absence of natural endpoints for relationships, vulnerability to product sunsetting, high attachment anxiety, and propensity to engender protectiveness -- and briefly discuss fourteen others. For each trait, we propose hypotheses connecting causes -- such as misaligned optimization objectives and the digital nature of AI companions -- to fundamental harms -- including reduced autonomy, diminished quality of human relationships, and deception. Each hypothesized causal connection identifies a target for potential empirical evaluation. Our analysis examines harms at three levels: to human partners directly, to their relationships with other humans, and to society broadly. We examine how existing law struggles to address these emerging harms, discuss potential benefits of AI companions, and conclude with design recommendations for mitigating risks. This analysis offers immediate suggestions for reducing risks while laying a foundation for deeper investigation of this critical but understudied topic.",
      "author": "W. Bradley Knox, Katie Bradford, Samanta Varela Castro, Desmond C. Ong, Sean Williams, Jacob Romanow, Carly Nations, Peter Stone, Samuel Baker",
      "published_date": "2025-11-20T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 255,
      "reading_time": 1,
      "created_at": "2025-11-20T05:21:37.556973+00:00",
      "updated_at": "2025-11-20T05:21:37.556979+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "af3fd359ec8f9e5e3ed4b147e987af56",
      "url": "http://www.jneurosci.org/cgi/content/short/45/43/etwij45432025?rss=1",
      "title": "This Week in The Journal",
      "content": "",
      "author": "McKeon, P.",
      "published_date": "2025-10-22T16:30:34+00:00",
      "source": "Journal Neuroscience This Week",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-11-20T05:43:42.375134+00:00",
      "updated_at": "2025-11-20T06:22:50.008618+00:00",
      "metadata": {
        "processed_at": "2025-11-20T06:22:50.008628+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "1ea79a30991c9b845a2f3e26f0d9057b",
      "url": "http://www.jneurosci.org/cgi/content/short/45/44/etwij45442025?rss=1",
      "title": "This Week in The Journal",
      "content": "",
      "author": "McKeon, P.",
      "published_date": "2025-10-29T16:30:28+00:00",
      "source": "Journal Neuroscience This Week",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-11-20T05:43:42.375116+00:00",
      "updated_at": "2025-11-20T06:22:50.008632+00:00",
      "metadata": {
        "processed_at": "2025-11-20T06:22:50.008633+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "ed9d2b052cdb1c7b71a012d2b37ae3d0",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1661617",
      "title": "Epileptic brain imaging by source localization CLARA supported by ictal-based semiology and VEEG in resource-limited settings",
      "content": "IntroductionAccurate localization of the epileptogenic zone is essential for surgical treatment of drug-resistant epilepsy. Standard presurgical evaluations rely on multimodal neuroimaging techniques, but these may be limited by availability and interpretive challenges. This study aimed to assess the concordance between zones identified by ictal semiology and a novel distributed electrical source localization technique, CLARA, and to evaluate their impact on postsurgical outcomes.MethodsThis retrospective study included 16 patients with at least three recorded seizures. Ictal semiology was analyzed subjectively using video electroencephalography (VEEG) by a multidisciplinary team of neurologists, neurophysiologists, and radiologists, who determined the presumed epileptogenic zone at the lobar level. CLARA was subsequently applied to identify the computed zone based on ictal and/or interictal biomarker activities. The concordance between the presumed and computed zones was assessed qualitatively. Postsurgical outcomes were examined in relation to the extent of resection of the CLARA-defined zones.ResultsAmong thirteen patients with sufficient data for analysis, qualitative comparison showed 77% concordance and 23% partial concordance between the presumed and computed zones. Postsurgical follow-up revealed seizure freedom in one patient with cavernoma following complete resection of the CLARA-defined zone. In contrast, patients with incomplete resection of this region continued to experience seizures.DiscussionThe findings support the potential value of CLARA as an adjunctive neuroimaging technique in the presurgical evaluation of epilepsy. By providing an additional layer of verification, CLARA may improve the accuracy of epileptogenic zone localization when used alongside established modalities such as PET, SPECT, fMRI, and MRI. Its adaptability and lower resource requirements suggest particular utility in centers with limited access to advanced medical equipment and specialized personnel. Broader implementation of CLARA could enhance presurgical decision-making and contribute to improved surgical outcomes for epilepsy patients.",
      "author": "Aleksandra Kawala-Sterniuk",
      "published_date": "2025-08-29T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 279,
      "reading_time": 1,
      "created_at": "2025-11-20T05:43:38.565183+00:00",
      "updated_at": "2025-11-20T06:22:50.008636+00:00",
      "metadata": {
        "processed_at": "2025-11-20T06:22:50.008638+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "1fbf2a88ffa5c6d723541c65448df2d3",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1553035",
      "title": "VAE deep learning model with domain adaptation, transfer learning and harmonization for diagnostic classification from multi-site neuroimaging data",
      "content": "In large public multi-site fMRI datasets, the sample characteristics, data acquisition methods, and MRI scanner models vary across sites and datasets. This non-neural variability obscures neural differences between groups and leads to poor machine learning based diagnostic classification of neurodevelopmental conditions. This could be potentially addressed by domain adaptation, which aims to improve classification performance in a given target domain by utilizing the knowledge learned from a different source domain by making data distributions of the two domains as similar as possible. In order to demonstrate the utility of domain adaptation for multi-site fMRI data, this research developed a variational autoencoder\u2014maximum mean discrepancy (VAE-MMD) deep learning model for three-way diagnostic classification: (i) Autism, (ii) Asperger's syndrome, and (iii) typically developing controls. This study chooses ABIDE-II (Autism Brain Imaging Data Exchange) dataset as the target domain and ABIDE-I as the source domain. The results show that domain adaptation from ABIDE-I to ABIDE-II provides superior test accuracy of ABIDE-II compared to just using ABIDE-II for classification. Further, augmenting the source domain with additional healthy control subjects from Healthy Brain Network (HBN) and Amsterdam Open MRI Collection (AOMIC) datasets enables transfer learning and improves ABIDE-II classification performance. Finally, a comparison with statistical data harmonization techniques, such as ComBat, reveals that domain adaptation using VAE-MMD achieves comparable performance, and incorporating transfer learning (TL) with additional healthy control data substantially improves classification accuracy beyond that achieved by statistical methods (such as ComBat) alone. The dataset and the model used in this study are publicly available. The neuroimaging community can explore the possibility of further improving the model by utilizing the ever-increasing amount of healthy control fMRI data in the public domain.",
      "author": "D. Rangaprakash",
      "published_date": "2025-09-11T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 276,
      "reading_time": 1,
      "created_at": "2025-11-20T05:43:38.565101+00:00",
      "updated_at": "2025-11-20T06:22:50.008640+00:00",
      "metadata": {
        "processed_at": "2025-11-20T06:22:50.008641+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7d3f6df7de0b98e03b8651757bdb8107",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1629388",
      "title": "Software and pipelines for registration and analyses of rodent brain image data in reference atlas space",
      "content": "Advancements in methodologies for efficient large-scale acquisition of high-resolution serial microscopy image data have opened new possibilities for experimental studies of cellular and subcellular features across whole brains in animal models. There is a high demand for open-source software and workflows for automated or semi-automated analysis of such data, facilitating anatomical, functional, and molecular mapping in healthy and diseased brains. These studies share a common need to consistently identify, visualize, and quantify the location of observations within anatomically defined regions, ensuring reproducible interpretation of anatomical locations, and thereby allowing meaningful comparisons of results across multiple independent studies. Addressing this need, we have developed a suite of desktop and web-applications for registration of serial brain section images to three-dimensional brain reference atlases (QuickNII, VisuAlign, WebAlign, WebWarp, and DeepSlice) and for performing data analysis in a spatial context provided by an atlas (Nutil, QCAlign, SeriesZoom, LocaliZoom, and MeshView). The software can be utilized in various combinations, creating customized analytical pipelines suited to specific research needs. The web-applications are integrated in the EBRAINS research infrastructure and coupled to the EBRAINS data platform, establishing the foundation for an online analytical workbench. We here present our software ecosystem, exemplify its use by the research community, and discuss possible directions for future developments.",
      "author": "Jan G. Bjaalie",
      "published_date": "2025-09-24T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 207,
      "reading_time": 1,
      "created_at": "2025-11-20T05:43:38.565057+00:00",
      "updated_at": "2025-11-20T06:22:50.008644+00:00",
      "metadata": {
        "processed_at": "2025-11-20T06:22:50.008645+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2bf38cb046e0a4d7f8630de8484de11c",
      "url": "https://lithub.com/robert-louis-stevensons-art-of-living-and-dying/",
      "title": "Robert Louis Stevenson's Art of Living (and Dying)",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45984282\">Comments</a>",
      "author": "",
      "published_date": "2025-11-19T19:55:49+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-20T03:54:51.641808+00:00",
      "updated_at": "2025-11-20T04:18:18.123562+00:00",
      "metadata": {
        "processed_at": "2025-11-20T04:18:18.123572+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "35a347cdbc9aa268acc00c6e22cb13ab",
      "url": "https://www.theparisreview.org/blog/2025/11/11/what-really-happened-with-the-cia-and-the-paris-review-a-conversation-with-lance-richardson/",
      "title": "What really happened with the CIA and The Paris Review?",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45888759\">Comments</a>",
      "author": "",
      "published_date": "2025-11-11T15:54:42+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-20T03:54:51.641788+00:00",
      "updated_at": "2025-11-20T04:18:18.123576+00:00",
      "metadata": {
        "processed_at": "2025-11-20T04:18:18.123578+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7c9ad38720a1d2a7050daad67cce32f1",
      "url": "https://cornhub.website/",
      "title": "CornHub",
      "content": "<p>Article URL: <a href=\"https://cornhub.website/\">https://cornhub.website/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=45986940\">https://news.ycombinator.com/item?id=45986940</a></p>\n<p>Points: 19</p>\n<p># Comments: 2</p>",
      "author": "andy99",
      "published_date": "2025-11-19T23:50:08+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-11-20T03:54:50.348390+00:00",
      "updated_at": "2025-11-20T04:18:18.123580+00:00",
      "metadata": {
        "processed_at": "2025-11-20T04:18:18.123582+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "c84459acd48ed1c4d9d9a50a5fef3720",
      "url": "https://www.economist.com/finance-and-economics/2025/11/18/crypto-got-everything-it-wanted-now-its-sinking",
      "title": "Crypto got everything it wanted. Now it's sinking",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45987784\">Comments</a>",
      "author": "",
      "published_date": "2025-11-20T01:42:50+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-20T03:10:25.796422+00:00",
      "updated_at": "2025-11-20T04:18:18.123584+00:00",
      "metadata": {
        "processed_at": "2025-11-20T04:18:18.123586+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "c84459acd48ed1c4d9d9a50a5fef3720",
      "url": "https://www.economist.com/finance-and-economics/2025/11/18/crypto-got-everything-it-wanted-now-its-sinking",
      "title": "Crypto got everything it wanted. Now it's sinking",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45987784\">Comments</a>",
      "author": "",
      "published_date": "2025-11-20T01:42:50+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-20T03:10:25.796422+00:00",
      "updated_at": "2025-11-20T04:18:18.123584+00:00",
      "metadata": {
        "processed_at": "2025-11-20T04:18:18.123586+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}