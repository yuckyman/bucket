{
  "last_updated": "2025-12-22T08:23:52.358425+00:00",
  "pending_count": 669,
  "processed_count": 331,
  "pending_articles": [
    {
      "id": "3f1ebb488349fb06c49746e031fb0bba",
      "url": "https://www.nature.com/articles/s41746-025-02226-5",
      "title": "Distinct visual biases affect humans and artificial intelligence in medical imaging diagnoses",
      "content": "",
      "author": "",
      "published_date": "2025-12-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-22T06:37:22.372032+00:00",
      "updated_at": "2025-12-22T06:37:22.372037+00:00"
    },
    {
      "id": "667ffa1c9a9374ed8129f366c85a234d",
      "url": "https://lcamtuf.substack.com/p/cursed-circuits-3-true-mathematics",
      "title": "Cursed circuits #3: true mathematics",
      "content": "<p>Article URL: <a href=\"https://lcamtuf.substack.com/p/cursed-circuits-3-true-mathematics\">https://lcamtuf.substack.com/p/cursed-circuits-3-true-mathematics</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46351345\">https://news.ycombinator.com/item?id=46351345</a></p>\n<p>Points: 6</p>\n<p># Comments: 0</p>",
      "author": "zdw",
      "published_date": "2025-12-22T04:34:36+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-12-22T06:36:45.575583+00:00",
      "updated_at": "2025-12-22T06:36:45.575591+00:00"
    },
    {
      "id": "27cddd2396e31165b8889ccc71b78dd2",
      "url": "https://arxiv.org/abs/2512.17117",
      "title": "Alignment, Exploration, and Novelty in Human-AI Interaction",
      "content": "arXiv:2512.17117v1 Announce Type: new \nAbstract: Human-AI interactions are increasingly part of everyday life, yet the interpersonal dynamics that unfold during such exchanges remain underexplored. This study investigates how emotional alignment, semantic exploration, and linguistic innovation emerge within a collaborative storytelling paradigm that paired human participants with a large language model (LLM) in a turn-taking setup. Over nine days, more than 3,000 museum visitors contributed to 27 evolving narratives, co-authored with an LLM in a naturalistic, public installation. To isolate the dynamics specific to human involvement, we compared the resulting dataset with a simulated baseline where two LLMs completed the same task. Using sentiment analysis, semantic embeddings, and information-theoretic measures of novelty and resonance, we trace how humans and models co-construct stories over time. Our results reveal that affective alignment is primarily driven by the model, with limited mutual convergence in human-AI interaction. At the same time, human participants explored a broader semantic space and introduced more novel, narratively influential contributions. These patterns were significantly reduced in the simulated AI-AI condition. Together, these findings highlight the unique role of human input in shaping narrative direction and creative divergence in co-authored texts. The methods developed here provide a scalable framework for analysing dyadic interaction and offer a new lens on creativity, emotional dynamics, and semantic coordination in human-AI collaboration.",
      "author": "Halfdan Nordahl Fundal, Johannes Eide Ramb{\\o}ll, Karsten Olsen",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245137+00:00",
      "updated_at": "2025-12-22T05:26:36.245138+00:00"
    },
    {
      "id": "9b2d2f406adac24dc3fa50266db3342c",
      "url": "https://arxiv.org/abs/2512.17081",
      "title": "Virtual Reality in Service Design for Plastics Recycling: Two Application Cases",
      "content": "arXiv:2512.17081v1 Announce Type: new \nAbstract: Plastics recycling depends on everyday sorting practices and on how recycling services are communicated and experienced. Virtual reality (VR) can present these practices and services in situated, interactive form, yet its role in service design for plastics recycling is still emerging. This paper examines how VR tools can contribute to designing plastics recycling services through two application cases that address different stages of the recycling journey. The first case, Clean Cabin Escape, is a household scale VR escape room where players collect and sort waste items into locally relevant categories, with immediate feedback that supports practice with plastics recycling decisions. The second case is a VR simulation of a plastics recycling center that represents a real planned site and is used in service design workshops where stakeholders explore layout, signage and customer paths for plastics fractions. Across the cases, we analyse how VR supported learning, engagement and shared sensemaking, and how it interacted with other service design methods such as workshops, customer path mapping and physical artefacts. The findings show that VR can make domestic sorting tasks and complex recycling centers more concrete for both citizens and professionals, but also highlight trade offs related to hardware access, onboarding effort, visual fidelity and localisation of recycling rules. The paper concludes by outlining opportunities for integrating VR into broader service design toolsets for plastics recycling and circular economy services, and by pointing to directions for future research on long term impact and inclusive design.",
      "author": "Ashley Colley, Kuisma Hurtig, Juri Etto, Emma Kirjavainen, Pavlo Ivanov, Jonna H\\\"akkil\\\"a",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 247,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245103+00:00",
      "updated_at": "2025-12-22T05:26:36.245105+00:00"
    },
    {
      "id": "ffc925bd06390981e9812a9a9621f0b1",
      "url": "https://arxiv.org/abs/2512.17067",
      "title": "Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution",
      "content": "arXiv:2512.17067v1 Announce Type: new \nAbstract: Social bots are now deeply embedded in online platforms for promotion, persuasion, and manipulation. Most bot-detection systems still treat behavioural features as static, implicitly assuming bots behave stationarily over time. We test that assumption for promotional Twitter bots, analysing change in both individual behavioural signals and the relationships between them. Using 2,615 promotional bot accounts and 2.8M tweets, we build yearly time series for ten content-based meta-features. Augmented Dickey-Fuller and KPSS tests plus linear trends show all ten are non-stationary: nine increase over time, while language diversity declines slightly.\n  Stratifying by activation generation and account age reveals systematic differences: second-generation bots are most active and link-heavy; short-lived bots show intense, repetitive activity with heavy hashtag/URL use; long-lived bots are less active but more linguistically diverse and use emojis more variably. We then analyse co-occurrence across generations using 18 interpretable binary features spanning actions, topic similarity, URLs, hashtags, sentiment, emojis, and media (153 pairs). Chi-square tests indicate almost all pairs are dependent. Spearman correlations shift in strength and sometimes polarity: many links (e.g. multiple hashtags with media; sentiment with URLs) strengthen, while others flip from weakly positive to weakly or moderately negative. Later generations show more structured combinations of cues.\n  Taken together, these studies provide evidence that promotional social bots adapt over time at both the level of individual meta-features and the level of feature interdependencies, with direct implications for the design and evaluation of bot-detection systems trained on historical behavioural features.",
      "author": "Ohoud Alzahrani, Russell Beale, Bob Hendley",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 246,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245068+00:00",
      "updated_at": "2025-12-22T05:26:36.245070+00:00"
    },
    {
      "id": "5f0971618c27c8d4917f6715211323a8",
      "url": "https://arxiv.org/abs/2512.17025",
      "title": "Designing Virtual Reality Games for Grief: A Workshop Approach with Mental Health Professionals",
      "content": "arXiv:2512.17025v1 Announce Type: new \nAbstract: Although serious games have been increasingly used for mental health applications, few explicitly address coping with grief as a core mechanic and narrative experience for patients. Existing grief-related digital games often focus on clinical training for medical professionals rather than immersive storytelling and agency in emotional processing for the patient. In response, we designed Road to Acceptance, a VR game that presents grief through first-person narrative and gameplay. As the next phase of evaluation, we propose a workshop-based study with 12 licensed mental health professionals to assess the therapeutic impacts of the game and the alignment with best practices in grief education and interventions. This will inform iterative game design and patient evaluation methods, ensuring that the experience is clinically appropriate. Potential findings can contribute to the design principles of grief-related virtual reality experiences, bridging the gap between interactive media, mental health interventions, and immersive storytelling.",
      "author": "Amina Kobenova, Piper Stickler, Tha\\'is Alvarenga, Sri Kurniawan",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245025+00:00",
      "updated_at": "2025-12-22T05:26:36.245027+00:00"
    },
    {
      "id": "1f52363d5b24e009f0d6e69af7d05dca",
      "url": "https://arxiv.org/abs/2512.17017",
      "title": "Explorable Ideas: Externalizing Ideas as Explorable Environments",
      "content": "arXiv:2512.17017v1 Announce Type: new \nAbstract: Working with abstract information often relies on static, symbolic representations that constrain exploration. We introduce Explorable Ideas, a framework that externalizes abstract concepts into explorable environments where physical navigation coordinates conceptual exploration. To investigate its practical value, we designed Idea Islands, a VR probe for ideation tasks, and conducted two controlled studies with 19 participants. Results show that overview perspectives foster strategic breadth while immersion sustains engagement through embodied presence, and that seamless transitions enable flexible workflows combining both modes. These findings validate the framework's design considerations and yield design implications for building future systems that treat information as explorable territory across creative, educational, and knowledge-intensive domains.",
      "author": "Euijun Jung, Jingyu Lee, Minji Kim, Youngki Lee",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 112,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.244989+00:00",
      "updated_at": "2025-12-22T05:26:36.244993+00:00"
    },
    {
      "id": "d66c8cb69d06cb9135ea5516fd6c1233",
      "url": "https://arxiv.org/abs/2512.17655",
      "title": "Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos",
      "content": "arXiv:2512.17655v1 Announce Type: cross \nAbstract: Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.",
      "author": "Evangelos Sariyanidi, Gokul Nair, Lisa Yankowitz, Casey J. Zampella, Mohan Kashyap Pargi, Aashvi Manakiwala, Maya McNealis, John D. Herrington, Jeffrey Cohn, Robert T. Schultz, Birkan Tunc",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 270,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:35.092140+00:00",
      "updated_at": "2025-12-22T05:26:35.092141+00:00"
    },
    {
      "id": "a6cd0226e35aa9f773722a516e16df40",
      "url": "https://arxiv.org/abs/2512.17775",
      "title": "How Light Shapes Memory: Beta Synchrony in the Temporal-Parietal Cortex Predicts Cognitive Ergonomics for BCI Applications",
      "content": "arXiv:2512.17775v1 Announce Type: new \nAbstract: Working memory is a promising paradigm for assessing cognitive ergonomics of brain states in brain-computer interfaces(BCIs). This study decodes these states with a focus on environmental illumination effects via two distinct working memory tasks(Recall and Sequence) for mixed-recognition analysis. Leveraging nonlinear patterns in brain connectivity, we propose an innovative framework: multi-regional dynamic interplay patterns based on beta phase synchrony dynamics, to identify low-dimensional EEG regions (prefrontal, temporal, parietal) for state recognition. Based on nonlinear phase map analysis of the above three brain regions using beta-phase connectivity, we found that: (1)Temporal-parietal phase clustering outperforms other regional combinations in distinguishing memory states; (2)Illumination-enhanced environments optimize temporoparietal balance;(3) Machine learning confirms temporal-parietal synchrony as the dominant cross-task classification feature. These results provide a precise prediction algorithm, facilitating a low-dimensional system using temporal and parietal EEG channels with practical value for real-time cognitive ergonomics assessment in BCIs and optimized human-machine interaction.",
      "author": "Jiajia Li, Tian Guo, Fan Li, Huichao Ding, Guozheng Xu, Jian Song",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 152,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:35.092103+00:00",
      "updated_at": "2025-12-22T05:26:35.092105+00:00"
    },
    {
      "id": "9689f399d97b18eaed2370e5ef4d7cf8",
      "url": "https://arxiv.org/abs/2512.17735",
      "title": "Gravity Prior and Temporal Horizon Shape Interceptive Behavior under Active Inference",
      "content": "arXiv:2512.17735v1 Announce Type: new \nAbstract: Accurate interception of moving objects, such as catching a ball, requires the nervous system to overcome sensory delays, noise, and environmental dynamics. One key challenge is predicting future object motion in the presence of sensory uncertainty and inherent neural processing latencies. Theoretical frameworks such as internal models and optimal control have emphasized the role of predictive mechanisms in motor behavior. Active Inference extends these ideas by positing that perception and action arise from minimizing variational free energy under a generative model of the world. In this study, we investigate how different predictive strategies and the inclusion of environmental dynamics, specifically an internal model of gravity, influence interceptive control within an Active Inference agent. We simulate a simplified ball-catching task in which the agent moves a cursor horizontally to intercept a parabolically falling object. Four strategies are compared: short temporal horizon prediction of the next position or long horizon estimation of the interception point, each with or without a gravity prior. Performance is evaluated across diverse initial conditions using spatial and temporal error, action magnitude, and movement corrections. All strategies produce successful interception behavior, but those that incorporate gravity and longer temporal horizons outperform others. Including a gravity prior significantly improves spatial and temporal accuracy. Predicting the future interception point yields lower action values and smoother trajectories compared to short-horizon prediction. These findings suggest that internal models of physical dynamics and extended predictive horizons can enhance interceptive control, providing a unified computational account of how the brain may integrate sensory uncertainty, physical expectations, and motor planning.",
      "author": "Marta Russo, Antonella Maselli, Federico Maggiore, Giovanni Pezzulo",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 260,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:35.092073+00:00",
      "updated_at": "2025-12-22T05:26:35.092075+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "dcb065be3da7c045e76e71652520acf8",
      "url": "https://www.biorxiv.org/content/10.64898/2025.12.17.695034v1?rss=1",
      "title": "Musical training improves planning and robustness of sequence learning",
      "content": "Musicians demonstrate advantages in acquiring motor sequences, showing faster learning and better explicit sequence knowledge than non-musicians. However, it is unclear whether this advantage extends beyond acquisition to the consolidation phase, which is when newly learned skills stabilize and become resistant to interference. Additionally, while interference from executing competing motor tasks is well-established, less is known about whether purely sensory information presented after learning can disrupt consolidation of a bimodal motor sequence. We investigated how post-acquisition sensory interference affects performance of a learned audio-visual sequence, and whether musical training moderates this vulnerability. Participants first learned an explicit sequence in a serial reaction time task using synchronous, informative audio-visual cues. After a brief consolidation period, they were randomly assigned to one of four observational conditions that manipulated the relationship between auditory and visual streams. Motor performance was then reassessed. Post-acquisition sensory interference significantly impaired subsequent motor performance, but this effect was modality-specific: it was driven primarily by manipulations to the task-relevant visual stream, while auditory interference alone had no credible effect. Distributional analysis revealed that learning involved a strategic shift from reactive to anticipatory responding. Critically, participants with musical training made this shift significantly faster than those without, demonstrating a more rapid adoption of predictive motor control. These findings demonstrate that newly formed sensorimotor memories are selectively vulnerable to interference in task-relevant modalities. Furthermore, our work provides a mechanistic explanation for the musician advantage in sequence learning, linking it to faster development of predictive motor strategies during consolidation.",
      "author": "Leow, L.-A., Lum, J. A., Johnson, S., Corti, E., Marinovic, W.",
      "published_date": "2025-12-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 247,
      "reading_time": 1,
      "created_at": "2025-12-22T07:25:48.598519+00:00",
      "updated_at": "2025-12-22T08:23:52.249080+00:00",
      "metadata": {
        "processed_at": "2025-12-22T08:23:52.249091+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b787ddccf8a4f8e916e1a0c67499f068",
      "url": "https://www.nature.com/articles/s41467-025-67825-y",
      "title": "Distinct mechanisms of transcriptomic habituation to repeated stress in the mouse hippocampus",
      "content": "",
      "author": "",
      "published_date": "2025-12-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-22T07:25:47.414727+00:00",
      "updated_at": "2025-12-22T08:23:52.249095+00:00",
      "metadata": {
        "processed_at": "2025-12-22T08:23:52.249097+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2af4cf331b6c804bbcb1e1f360135f60",
      "url": "https://xania.org/202512/15-aliasing-in-general",
      "title": "Aliasing",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46286813\">Comments</a>",
      "author": "",
      "published_date": "2025-12-16T10:13:45+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-12-22T07:25:11.755043+00:00",
      "updated_at": "2025-12-22T08:23:52.249100+00:00",
      "metadata": {
        "processed_at": "2025-12-22T08:23:52.249102+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b2714e55c74af50dbb67d60618fa33ef",
      "url": "https://www.nature.com/articles/s41467-025-66124-w",
      "title": "Constructing the human brain metabolic connectome with MR spectroscopic imaging reveals cerebral biochemical organization",
      "content": "",
      "author": "",
      "published_date": "2025-12-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-22T06:37:22.372134+00:00",
      "updated_at": "2025-12-22T08:23:52.249104+00:00",
      "metadata": {
        "processed_at": "2025-12-22T08:23:52.249106+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "c546f5ca7486a3ee8561e9580fa4f35a",
      "url": "https://www.nature.com/articles/s41528-025-00516-2",
      "title": "Injectable eutectogel for high-quality scalp electroencephalogram monitoring",
      "content": "",
      "author": "",
      "published_date": "2025-12-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-22T06:37:22.372057+00:00",
      "updated_at": "2025-12-22T08:23:52.249108+00:00",
      "metadata": {
        "processed_at": "2025-12-22T08:23:52.249110+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3cfcf78b4885ae1adbeaf3e32af4d5ca",
      "url": "https://arxiv.org/abs/2512.17354",
      "title": "Implementation of Augmented Reality as an Educational Tool for Practice in Early Childhood",
      "content": "arXiv:2512.17354v1 Announce Type: new \nAbstract: Learning Wudhu for young children requires engaging and interactive media to foster a deep understanding of the worship procedures. This study aims to develop a Wudhu learning application based on Augmented Reality (AR) as an interactive and fun educational medium. The development method used includes the stages of needs analysis, system design, implementation, and testing using Black Box Testing. The system utilizes marker-based tracking to display 3D animations of Wudhu movements in real-time when the camera detects a marker on the printed media. The test results indicate that all main functions run well, and a limited trial on children aged 5-7 years showed an increase in learning interest and a better understanding of the Wudhu sequence. Thus, the application of AR technology is proven effective in improving the quality of basic worship instruction for young children.",
      "author": "Wisnu Uriawan, Muhammad Aditya Hafizh Zahran, Inayah Ayu Deswita, Muhammad Ahsani Taqwim, Ismail Muhammad Ahmadi, Marvi Yoga Pratama",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 141,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245302+00:00",
      "updated_at": "2025-12-22T06:27:58.227887+00:00",
      "metadata": {
        "processed_at": "2025-12-22T06:27:58.227899+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "742cd0e2c22cd02d6705e7728f0aa740",
      "url": "https://arxiv.org/abs/2512.17228",
      "title": "LUMIA: A Handheld Vision-to-Music System for Real-Time, Embodied Composition",
      "content": "arXiv:2512.17228v1 Announce Type: new \nAbstract: Most digital music tools emphasize precision and control, but often lack support for tactile, improvisational workflows grounded in environmental interaction. Lumia addresses this by enabling users to \"compose through looking\"--transforming visual scenes into musical phrases using a handheld, camera-based interface and large multimodal models. A vision-language model (GPT-4V) analyzes captured imagery to generate structured prompts, which, combined with user-selected instrumentation, guide a text-to-music pipeline (Stable Audio). This real-time process allows users to frame, capture, and layer audio interactively, producing loopable musical segments through embodied interaction. The system supports a co-creative workflow where human intent and model inference shape the musical outcome. By embedding generative AI within a physical device, Lumia bridges perception and composition, introducing a new modality for creative exploration that merges vision, language, and sound. It repositions generative music not as a task of parameter tuning, but as an improvisational practice driven by contextual, sensory engagement.",
      "author": "Chung-Ta Huang, Connie Cheng, Vealy Lai",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245272+00:00",
      "updated_at": "2025-12-22T06:27:58.227903+00:00",
      "metadata": {
        "processed_at": "2025-12-22T06:27:58.227905+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "133948a4560b370e5ecaacf4f19ecd0e",
      "url": "https://arxiv.org/abs/2512.17172",
      "title": "PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases",
      "content": "arXiv:2512.17172v1 Announce Type: new \nAbstract: Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.",
      "author": "Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 244,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245243+00:00",
      "updated_at": "2025-12-22T06:27:58.227907+00:00",
      "metadata": {
        "processed_at": "2025-12-22T06:27:58.227909+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "5cf3201691c4b376fab7b60c7ecd24a6",
      "url": "https://arxiv.org/abs/2512.17149",
      "title": "Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces",
      "content": "arXiv:2512.17149v1 Announce Type: new \nAbstract: This study investigates the task of dwell time prediction and proposes a Transformer framework based on interaction behavior modeling. The method first represents user interaction sequences on the interface by integrating dwell duration, click frequency, scrolling behavior, and contextual features, which are mapped into a unified latent space through embedding and positional encoding. On this basis, a multi-head self-attention mechanism is employed to capture long-range dependencies, while a feed-forward network performs deep nonlinear transformations to model the dynamic patterns of dwell time. Multiple comparative experiments are conducted with BILSTM, DRFormer, FedFormer, and iTransformer as baselines under the same conditions. The results show that the proposed method achieves the best performance in terms of MSE, RMSE, MAPE, and RMAE, and more accurately captures the complex patterns in interaction behavior. In addition, sensitivity experiments are carried out on hyperparameters and environments to examine the impact of the number of attention heads, sequence window length, and device environment on prediction performance, which further demonstrates the robustness and adaptability of the method. Overall, this study provides a new solution for dwell time prediction from both theoretical and methodological perspectives and verifies its effectiveness in multiple aspects.",
      "author": "Rui Liu, Runsheng Zhang, Shixiao Wang",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 197,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245208+00:00",
      "updated_at": "2025-12-22T06:27:58.227911+00:00",
      "metadata": {
        "processed_at": "2025-12-22T06:27:58.227912+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "ba4e0c936a7719d95ea75ad7b7e0b156",
      "url": "https://arxiv.org/abs/2512.17140",
      "title": "Bridging Psychometric and Content Development Practices with AI: A Community-Based Workflow for Augmenting Hawaiian Language Assessments",
      "content": "arXiv:2512.17140v1 Announce Type: new \nAbstract: This paper presents the design and evaluation of a community-based artificial intelligence (AI) workflow developed for the Kaiapuni Assessment of Educational Outcomes (K\\=A'EO) program, the only native language assessment used for federal accountability in the United States. The project explored whether document-grounded language models could ethically and effectively augment human analysis of item performance while preserving the cultural and linguistic integrity of the Hawaiian language. Operating under the K\\=A'EO AI Policy Framework, the workflow used NotebookLM for cross-document synthesis of psychometric data and Claude 3.5 Sonnet for developer-facing interpretation, with human oversight at every stage. Fifty-eight flagged items across Hawaiian Language Arts, Mathematics, and Science were reviewed during Round 2 of the AI Lab, producing six interpretive briefs that identified systemic design issues such as linguistic ambiguity, Depth-of-Knowledge (DOK) misalignment, and structural overload. The findings demonstrate that AI can serve as an ethically bounded amplifier of human expertise, accelerating analysis while simultaneously prioritizing fairness, human expertise, and cultural authority. This work offers a replicable model for responsible AI integration in Indigenous-language educational measurement.",
      "author": "P\\=ohai K\\=ukea-Shultz, Frank Brockmann",
      "published_date": "2025-12-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 178,
      "reading_time": 1,
      "created_at": "2025-12-22T05:26:36.245168+00:00",
      "updated_at": "2025-12-22T06:27:58.227915+00:00",
      "metadata": {
        "processed_at": "2025-12-22T06:27:58.227919+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}