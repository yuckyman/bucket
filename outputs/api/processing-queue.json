{
  "last_updated": "2025-10-01T10:15:53.664733+00:00",
  "pending_count": 985,
  "processed_count": 15,
  "pending_articles": [
    {
      "id": "b463bcc77eb4a19055464b7208d1af96",
      "url": "https://arxiv.org/abs/2509.25491",
      "title": "LLM-Assisted News Discovery in High-Volume Information Streams: A Case Study",
      "content": "arXiv:2509.25491v1 Announce Type: new \nAbstract: Journalists face mounting challenges in monitoring ever-expanding digital information streams to identify newsworthy content. While traditional automation tools gather information at scale, they struggle with the editorial judgment needed to assess newsworthiness. This paper investigates whether large language models (LLMs) can serve as effective first-pass filters for journalistic monitoring. We develop a prompt-based approach encoding journalistic news values - timeliness, impact, controversy, and generalizability - into LLM instructions to extract and evaluate potential story leads. We validate our approach across multiple models against expert-annotated ground truth, then deploy a real-world monitoring pipeline that processes trade press articles daily. Our evaluation reveals strong performance in extracting relevant leads from source material ($F1=0.94$) and in coarse newsworthiness assessment ($\\pm$1 accuracy up to 92%), but it consistently struggles with nuanced editorial judgments requiring beat expertise. The system proves most valuable as a hybrid tool combining automated monitoring with human review, successfully surfacing novel, high-value leads while filtering obvious noise. We conclude with practical recommendations for integrating LLM-powered monitoring into newsroom workflows that preserves editorial judgment while extending journalistic capacity.",
      "author": "Nick Hagar, Ethan Silver, Clare Spencer, Nicholas Diakopoulos",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 181,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.747057+00:00",
      "updated_at": "2025-10-01T09:42:28.747059+00:00"
    },
    {
      "id": "652dad4caaec29b92ff350a5d8368251",
      "url": "https://arxiv.org/abs/2509.25460",
      "title": "\"Where Can I Park?\" Understanding Human Perspectives and Scalably Detecting Disability Parking from Aerial Imagery",
      "content": "arXiv:2509.25460v1 Announce Type: new \nAbstract: Accessible parking is critical for people with disabilities (PwDs), allowing equitable access to destinations, independent mobility, and community participation. Despite mandates, there has been no large-scale investigation of the quality or allocation of disability parking in the US nor significant research on PwD perspectives and uses of disability parking. In this paper, we first present a semi-structured interview study with 11 PwDs to advance understanding of disability parking uses, concerns, and relevant technology tools. We find that PwDs often adapt to disability parking challenges according to their personal mobility needs and value reliable, real-time accessibility information. Informed by these findings, we then introduce a new deep learning pipeline, called AccessParkCV, and parking dataset for automatically detecting disability parking and inferring quality characteristics (e.g., width) from orthorectified aerial imagery. We achieve a micro-F1=0.89 and demonstrate how our pipeline can support new urban analytics and end-user tools. Together, we contribute new qualitative understandings of disability parking, a novel detection pipeline and open dataset, and design guidelines for future tools.",
      "author": "Jared Hwang, Chu Li, Hanbyul Kang, Maryam Hosseini, Jon E. Froehlich",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 172,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.747026+00:00",
      "updated_at": "2025-10-01T09:42:28.747027+00:00"
    },
    {
      "id": "a41507032d26f246243a270dec0c3ddb",
      "url": "https://arxiv.org/abs/2509.25457",
      "title": "Human vs. AI Safety Perception? Decoding Human Safety Perception with Eye-Tracking Systems, Street View Images, and Explainable AI",
      "content": "arXiv:2509.25457v1 Announce Type: new \nAbstract: The way residents perceive safety plays an important role in how they use public spaces. Studies have combined large-scale street view images and advanced computer vision techniques to measure the perception of safety of urban environments. Despite their success, such studies have often overlooked the specific environmental visual factors that draw human attention and trigger people's feelings of safety perceptions. In this study, we introduce a computational framework that enriches the existing body of literature on place perception by using eye-tracking systems with street view images and deep learning approaches. Eye-tracking systems quantify not only what users are looking at but also how long they engage with specific environmental elements. This allows us to explore the nuance of which visual environmental factors influence human safety perceptions. We conducted our research in Helsingborg, Sweden, where we recruited volunteers outfitted with eye-tracking systems. They were asked to indicate which of the two street view images appeared safer. By examining participants' focus on specific features using Mean Object Ratio in Highlighted Regions (MoRH) and Mean Object Hue (MoH), we identified key visual elements that attract human attention when perceiving safe environments. For instance, certain urban infrastructure and public space features draw more human attention while the sky is less relevant in influencing safety perceptions. These insights offer a more human-centered understanding of which urban features influence human safety perceptions. Furthermore, we compared the real human attention from eye-tracking systems with attention maps obtained from eXplainable Artificial Intelligence (XAI) results. Several XAI models were tested, and we observed that XGradCAM and EigenCAM most closely align with human safety perceptual patterns.",
      "author": "Yuhao Kang, Junda Chen, Liu Liu, Kshitij Sharmad, Martina Mazzarello, Simone Mora, Fabio Duarte, Carlo Ratti",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 271,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.746974+00:00",
      "updated_at": "2025-10-01T09:42:28.746990+00:00"
    },
    {
      "id": "62e8e142ac157eefa4e4017fbeb7b007",
      "url": "https://arxiv.org/abs/2509.25383",
      "title": "Beyond the Pocket: A Large-Scale International Study on User Preferences on Bodily Placements of Commercial Wearables",
      "content": "arXiv:2509.25383v1 Announce Type: new \nAbstract: As wearable technologies continue to evolve-becoming smaller, more powerful, and more deeply embedded in daily life-their integration into diverse user contexts raises critical design challenges. There remains a notable gap in large-scale empirical data on where users actually wear or carry these devices throughout the day, systematically examining user preferences for wearable placement across varied contexts and routines. In this work, we conducted a questionnaire in several countries aimed at capturing real-world habits related to wearable device placement. The results from n = 320 participants reveal how wearable usage patterns shift depending on time of day and context. We propose a set of practical, user-centered guidelines for sensor placement and discuss how they align or diverge from assumptions seen in existing ISWC work. This study contributes to ongoing efforts within the community to design more inclusive, adaptable, and context-aware wearable systems.",
      "author": "Joanna Sorysz, Lars Krupp, Dominique Nshimyimana, Meagan B. Loerakker, Bo Zhou, Paul Lukowicz, Jakob Karolus",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.746934+00:00",
      "updated_at": "2025-10-01T09:42:28.746936+00:00"
    },
    {
      "id": "5a4ef1c1671f3e751c3677c4a2a7e442",
      "url": "https://arxiv.org/abs/2509.25364",
      "title": "Assessing the Effectiveness of Driver Training Interventions in Improving Safe Engagement with Vehicle Automation Systems",
      "content": "arXiv:2509.25364v1 Announce Type: new \nAbstract: This study investigates how targeted training interventions can improve safe driver interaction with vehicle automation (VA) systems, focusing on Adaptive Cruise Control (ACC) and Lane Keeping Assist (LKA), both safety-critical advanced driver assistance systems (ADAS). Effective training reduces misuse and enhances road safety by promoting correct knowledge and application.\n  A review of multiple automakers' owners' manuals revealed inconsistencies in describing ACC and LKA functions. Three training formats were compared: (1) owners' manual (OM), (2) knowledge-based (KB) with summarized operational guidelines and visual aids, and (3) skill-based hands-on practice in a driving simulator (SIM). Thirty-six participants with no prior VA experience were randomly assigned to one group. Safety-relevant outcomes - system comprehension (quiz scores) and real-world engagement (frequency and duration of activations) - were analyzed using mixed-effects and negative binomial models.\n  KB training produced the greatest improvements in comprehension of system limitations, as well as safer engagement patterns. Compared with OM participants, KB participants achieved significantly higher quiz scores and engaged LKA and ACC more often (1.4 and 1.45 times, respectively); they also demonstrated greater awareness of scenarios requiring manual control, indicating reduced risk of inappropriate reliance. Older drivers exhibited longer activations overall, highlighting age-related differences in reliance and potential safety implications.\n  Short, targeted training can significantly improve safe and effective VA system use, particularly for senior drivers. These results highlight training as a proactive safety intervention to reduce human-automation mismatch and enhance system reliability in real-world driving.",
      "author": "Chengxin Zhang, Huizhong Guo, Zifei Wang, Fred Feng, Anuj Pradhan, Shan Bao",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 242,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.746899+00:00",
      "updated_at": "2025-10-01T09:42:28.746903+00:00"
    },
    {
      "id": "654b90fc64556ca4927915ffca506a91",
      "url": "https://arxiv.org/abs/2203.10602",
      "title": "A physical approach to qualia and the emergence of conscious observers in qualia space",
      "content": "arXiv:2203.10602v2 Announce Type: replace-cross \nAbstract: I propose that qualia are physical because they are directly observable, and revisit the contentious link between consciousness and quantum measurements from a new perspective -- one that does not rely on observers or wave function collapse but instead treats physical measurements as fundamental in a sense resonant with Wheeler's it-from-bit. Building on a mathematical definition of measurement space in physics, I reinterpret it as a model of qualia, effectively equating the measurement problem of quantum mechanics with the hard problem of consciousness. The resulting framework falls within panpsychism, and offers potential solutions to the combination problem. Moreover, some of the mathematical structure of measurement spaces, taken for granted in physics, needs justification for qualia, suggesting that the apparent solidity of physical reality is deeply rooted in how humans process information.",
      "author": "Pedro Resende",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 136,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:27.671467+00:00",
      "updated_at": "2025-10-01T09:42:27.671469+00:00"
    },
    {
      "id": "64696584d5ca1e17002742ef3f89f997",
      "url": "https://arxiv.org/abs/2509.13875",
      "title": "Personalized Detection of Stress via hdrEEG: Linking Neuro-markers to Cortisol, HRV, and Self-Report",
      "content": "arXiv:2509.13875v2 Announce Type: replace \nAbstract: Chronic stress is a risk factor for cognitive decline and illness, yet reliable individual markers remain limited. We tested whether two single channel high dynamic range EEG biomarkers, ST4 and T2, index stress responses by linking neural activity to validated physiological and subjective measures.\n  Study 1 included 101 adults between 22 and 82 years of age who completed questionnaires on stress, resilience, and burnout, provided salivary cortisol, and performed resting, cognitive load, emotional, and startle conditions. Study 2 included 82 adults between 19 and 42 years who completed the State Trait Anxiety Inventory, underwent heart rate variability monitoring, and performed auditory, stress inducing, and emotional conditions. Correlations were considered meaningful when r was at least 0.30. Results showed that ST4 reflected physiological arousal and cognitive strain. In Study 1, resting ST4 was positively related to cortisol and lower in more resilient participants. In Study 2, ST4 correlated negatively with heart rate variability during stress and recovery. T2 reflected emotional and autonomic regulation. In Study 1, T2 tracked higher cortisol and was lower with greater resilience. In Study 2, T2 was higher with trait anxiety and correlated negatively with heart rate variability during stress and emotional conditions. Together, ST4 and T2 provide complementary portable markers of stress, supporting individualized assessment in clinical and real world contexts.",
      "author": "N. B. Maimon, Ganit Baruchin, Itamar Grotto, Lior Molcho, Nathan Intrator, Talya Zeimer, Ofir Chibotero, Nardeen Murad, Yori Gidron, Efrat Danino",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 220,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:27.671440+00:00",
      "updated_at": "2025-10-01T09:42:27.671441+00:00"
    },
    {
      "id": "0ad9823547439cdcf20fc84d9845a4bb",
      "url": "https://arxiv.org/abs/2509.26560",
      "title": "Estimating Dimensionality of Neural Representations from Finite Samples",
      "content": "arXiv:2509.26560v1 Announce Type: cross \nAbstract: The global dimensionality of a neural representation manifold provides rich insight into the computational process underlying both artificial and biological neural networks. However, all existing measures of global dimensionality are sensitive to the number of samples, i.e., the number of rows and columns of the sample matrix. We show that, in particular, the participation ratio of eigenvalues, a popular measure of global dimensionality, is highly biased with small sample sizes, and propose a bias-corrected estimator that is more accurate with finite samples and with noise. On synthetic data examples, we demonstrate that our estimator can recover the true known dimensionality. We apply our estimator to neural brain recordings, including calcium imaging, electrophysiological recordings, and fMRI data, and to the neural activations in a large language model and show our estimator is invariant to the sample size. Finally, our estimators can additionally be used to measure the local dimensionalities of curved neural manifolds by weighting the finite samples appropriately.",
      "author": "Chanwoo Chun, Abdulkadir Canatar, SueYeon Chung, Daniel Lee",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 163,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:27.671407+00:00",
      "updated_at": "2025-10-01T09:42:27.671408+00:00"
    },
    {
      "id": "34fbe5382d4158c676f595530082a6ed",
      "url": "https://arxiv.org/abs/2509.26606",
      "title": "Beyond Suboptimality: Resource-Rationality and Task Demands Shape the Complexity of Perceptual Representations",
      "content": "arXiv:2509.26606v1 Announce Type: new \nAbstract: Early theories of perception as probabilistic inference propose that uncertainty about the interpretation of sensory input is represented as a probability distribution over many interpretations -- a relatively complex representation. However, critics argue that persistent demonstrations of suboptimal perceptual decision-making indicate limits in representational complexity. We contend that suboptimality arises not from genuine limits, but participants' resource-rational adaptations to task demands. For example, when tasks are solvable with minimal attention to stimuli, participants may neglect information needed for complex representations, relying instead on simpler ones that engender suboptimality. Across three experiments, we progressively reduced the efficacy of resource-rational strategies on a carefully controlled decision task. Model fits favored simple representations when resource-rational strategies were effective, and favored complex representations when ineffective, suggesting that perceptual representations can be simple or complex depending on task demands. We conclude that resource-rationality is an epistemic constraint for experimental design and essential to a complete theory of perception.",
      "author": "Andrew Jun Lee, Daniel Turek, Omer Daglar Tanrikulu",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:27.671375+00:00",
      "updated_at": "2025-10-01T09:42:27.671377+00:00"
    },
    {
      "id": "af3f2db0b38a9b85bf6e9cd438279243",
      "url": "https://arxiv.org/abs/2509.26090",
      "title": "Coexistence of two adaptation processes in a visuomotor rotation task",
      "content": "arXiv:2509.26090v1 Announce Type: new \nAbstract: Motor adaptation is a learning process that enables humans to regain proficiency when sensorimotor conditions are sustainably altered. Many studies have documented the properties of motor adaptation, yet the underlying mechanisms of motor adaptation remain imperfectly understood. In this study, we propose a computational analysis of adaptation to a visuomotor rotation task and examine it through an experiment. Our analysis suggests that two distinct processes contribute to produce adaptation: one which straightens trajectories, and another which redirects trajectories. We designed a visuomotor rotation task in a 3D virtual environment where human participants performed a pointing task using a head-mounted display controller represented by a cursor that was visually rotated by an angular deviation relative to its actual position. We observed that: (1) the trajectories were initially curved and misdirected, and became straighter and better directed with learning; (2) the straightening process occurred faster than the redirection process. These findings are consistent with our computational analysis and disclose a new and different perspective on motor adaptation.",
      "author": "Alexis Berland (ISIR, CAOR), Youssouf Ismail Cherifi (CAOR), Alexis Paljic (CAOR), Emmanuel Guigon (ISIR)",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 170,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:27.671345+00:00",
      "updated_at": "2025-10-01T09:42:27.671347+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "4f503efc209479f9b870bfb5bde19f97",
      "url": "https://arxiv.org/abs/2509.25537",
      "title": "Healthy Lifestyles and Self-Improvement Videos on YouTube: A Thematic Analysis of Teen-Targeted Social Media Content",
      "content": "arXiv:2509.25537v1 Announce Type: new \nAbstract: As teenagers increasingly turn to social media for health-related information, understanding the values of teen-targeted content has become important. Although videos on healthy lifestyles and self-improvement are gaining popularity on social media platforms like YouTube, little is known about how these videos benefit and engage with teenage viewers. To address this, we conducted a thematic analysis of 44 YouTube videos and 66,901 comments. We found that these videos provide various advice on teenagers' common challenges, use engaging narratives for authenticity, and foster teen-centered communities through comments. However, a few videos also gave misleading advice to adolescents that can be potentially harmful. Based on our findings, we discuss design implications for creating relatable and intriguing social media content for adolescents. Additionally, we suggest ways for social media platforms to promote healthier and safer experiences for teenagers.",
      "author": "Kyuha Jung, Tyler Kim, Yunan Chen",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 140,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.747203+00:00",
      "updated_at": "2025-10-01T10:15:53.564317+00:00",
      "metadata": {
        "processed_at": "2025-10-01T10:15:53.564326+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "ba103cc111df09142bb2ce13dfcbc549",
      "url": "https://arxiv.org/abs/2509.25513",
      "title": "User Prompting Strategies and ChatGPT Contextual Adaptation Shape Conversational Information-Seeking Experiences",
      "content": "arXiv:2509.25513v1 Announce Type: new \nAbstract: Conversational AI, such as ChatGPT, is increasingly used for information seeking. However, little is known about how ordinary users actually prompt and how ChatGPT adapts its responses in real-world conversational information seeking (CIS). In this study, a nationally representative sample of 937 U.S. adults engaged in multi-turn CIS with ChatGPT on both controversial and non-controversial topics across science, health, and policy contexts. We analyzed both user prompting strategies and the communication styles of ChatGPT responses. The findings revealed behavioral signals of digital divide: only 19.1% of users employed prompting strategies, and these users were disproportionately more educated and Democrat-leaning. Further, ChatGPT demonstrated contextual adaptation: responses to controversial topics contain more cognitive complexity and more external references than to non-controversial topics. Notably, cognitively complex responses were perceived as less favorable but produced more positive issue-relevant attitudes. This study highlights disparities in user prompting behaviors and shows how user prompts and AI responses together shape information-seeking with conversational AI.",
      "author": "Haoning Xue, Yoo Jung Oh, Xinyi Zhou, Xinyu Zhang, Berit Oxley",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 162,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.747175+00:00",
      "updated_at": "2025-10-01T10:15:53.564330+00:00",
      "metadata": {
        "processed_at": "2025-10-01T10:15:53.564332+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "12518e6b2b2e986bef078011dbe9c579",
      "url": "https://arxiv.org/abs/2509.25504",
      "title": "XR Blocks: Accelerating Human-centered AI + XR Innovation",
      "content": "arXiv:2509.25504v1 Announce Type: new \nAbstract: We are on the cusp where Artificial Intelligence (AI) and Extended Reality (XR) are converging to unlock new paradigms of interactive computing. However, a significant gap exists between the ecosystems of these two fields: while AI research and development is accelerated by mature frameworks like JAX and benchmarks like LMArena, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction. To bridge this gap, we present XR Blocks, a cross-platform framework designed to accelerate human-centered AI + XR innovation. XR Blocks strives to provide a modular architecture with plug-and-play components for core abstraction in AI + XR: user, world, peers; interface, context, and agents. Crucially, it is designed with the mission of \"reducing frictions from idea to reality\", thus accelerating rapid prototyping of AI + XR apps. Built upon accessible technologies (WebXR, three.js, TensorFlow, Gemini), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source templates, samples, and advanced demos, empowering the community to quickly move from concept to interactive XR prototype. Site: https://xrblocks.github.io",
      "author": "David Li, Nels Numan, Xun Qian, Yanhe Chen, Zhongyi Zhou, Evgenii Alekseev, Geonsun Lee, Alex Cooper, Min Xia, Scott Chung, Jeremy Nelson, Xiuxiu Yuan, Jolica Dias, Tim Bettridge, Benjamin Hersh, Michelle Huynh, Konrad Piascik, Ricardo Cabello, David Kim, Ruofei Du",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 191,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.747146+00:00",
      "updated_at": "2025-10-01T10:15:53.564335+00:00",
      "metadata": {
        "processed_at": "2025-10-01T10:15:53.564337+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "6751d4ba29952f47f6af983d84f172cd",
      "url": "https://arxiv.org/abs/2509.25499",
      "title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking",
      "content": "arXiv:2509.25499v1 Announce Type: new \nAbstract: Human-AI interaction researchers face an overwhelming challenge: synthesizing insights from thousands of empirical studies to understand how AI impacts people and inform effective design. Existing approach for literature reviews cluster papers by similarities, keywords or citations, missing the crucial cause-and-effect relationships that reveal how design decisions impact user outcomes. We introduce the Atlas of Human-AI Interaction, an interactive web interface that provides the first systematic mapping of empirical findings across 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach identifies causal relationships, and visualizes them through an AI-enabled interactive web interface as a navigable knowledge graph. We extracted 2,037 empirical findings, revealing research topic clusters, common themes, and disconnected areas. Expert evaluation with 20 researchers revealed the system's effectiveness for discovering research gaps. This work demonstrates how AI can transform literature synthesis itself, offering a scalable framework for evidence-based design, opening new possibilities for computational meta-science across HCI and beyond.",
      "author": "Chayapatr Archiwaranguprok, Awu Chen, Sheer Karny, Hiroshi Ishii, Pattie Maes, Pat Pataranutaporn",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.747115+00:00",
      "updated_at": "2025-10-01T10:15:53.564339+00:00",
      "metadata": {
        "processed_at": "2025-10-01T10:15:53.564341+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "863c3da472b0a1909355d8b7dcb7ee7d",
      "url": "https://arxiv.org/abs/2509.25492",
      "title": "Botender: Supporting Communities in Collaboratively Designing AI Agents through Case-Based Provocations",
      "content": "arXiv:2509.25492v1 Announce Type: new \nAbstract: AI agents, or bots, serve important roles in online communities. However, they are often designed by outsiders or a few tech-savvy members, leading to bots that may not align with the broader community's needs. How might communities collectively shape the behavior of community bots? We present Botender, a system that enables communities to collaboratively design LLM-powered bots without coding. With Botender, community members can directly propose, iterate on, and deploy custom bot behaviors tailored to community needs. Botender facilitates testing and iteration on bot behavior through case-based provocations: interaction scenarios generated to spark user reflection and discussion around desirable bot behavior. A validation study found these provocations more useful than standard test cases for revealing improvement opportunities and surfacing disagreements. During a five-day deployment across six Discord servers, Botender supported communities in tailoring bot behavior to their specific needs, showcasing the usefulness of case-based provocations in facilitating collaborative bot design.",
      "author": "Tzu-Sheng Kuo, Sophia Liu, Quan Ze Chen, Joseph Seering, Amy X. Zhang, Haiyi Zhu, Kenneth Holstein",
      "published_date": "2025-10-01T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-10-01T09:42:28.747086+00:00",
      "updated_at": "2025-10-01T10:15:53.564343+00:00",
      "metadata": {
        "processed_at": "2025-10-01T10:15:53.564347+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "eddcbadea083b0a5930cbd4a70baf32e",
      "url": "https://brain.ieee.org/publications/neuroethics-framework/education/references/education-references/",
      "title": "Education: References",
      "content": "[1] OECD \u201cNeurotechnology Toolkit To support policymakers in implementing the OECD Recommendation on Responsible Innovation in Neurotechnology,\u201d 2024.: https://www.oecd.org/content/dam/oecd/en/topics/policy-sub-issues/emerging-technologies/neurotech-toolkit.pdf. [2] van Kesteren and Meeter, 2020 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7339924/ [3]\u00a0 Bikson, M., Esmaeilpour, Z., Adair, D., Kronberg, G., Tyler, W. J., Antal, A., Datta, A., Sabel, B. A., Nitsche, M. A., Loo, C., Edwards, D., Ekhtiari, H., Knotkova, H., Woods, A. J., Hampstead, ...",
      "author": "Adriel Carridice",
      "published_date": "2025-02-13T19:57:58+00:00",
      "source": "Brain",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2025-10-01T07:40:52.403597+00:00",
      "updated_at": "2025-10-01T08:19:40.633994+00:00",
      "metadata": {
        "processed_at": "2025-10-01T08:19:40.634003+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "8bfe7b7f37af142b24b0fcd318868131",
      "url": "https://brain.ieee.org/braininsight-articles/ieee-brain-annual-flagship-workshop-a-success/",
      "title": "IEEE Brain Annual Flagship Workshop a Success",
      "content": "IEEE Brain once again hosted the IEEE Brain Discovery and Neurotechnology Workshop as a satellite event to the 2024 Society of Neuroscience Workshop (SfN). Approximately 180 attended the two-day event, which was held at the University of Illinois Chicago (UIC), October 3-4, 2024 (Figure 1). Groundbreaking solutions with the potential to improve quality of life and address neural disorders require ...",
      "author": "ieeebrain",
      "published_date": "2025-03-03T16:55:37+00:00",
      "source": "Brain",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2025-10-01T07:40:52.403572+00:00",
      "updated_at": "2025-10-01T08:19:40.634008+00:00",
      "metadata": {
        "processed_at": "2025-10-01T08:19:40.634009+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2eb57dbdbc858b57282cfad12fa8d826",
      "url": "https://brain.ieee.org/braininsight-articles/ieee-brain-workshop-on-ai-for-neurotechnology/",
      "title": "IEEE Brain Workshop on AI for Neurotechnology",
      "content": "The IEEE Brain Workshop on AI for Neurotechnology was held on June 30, 2024, at the Pacifico Yokohama Conference Center in Japan. This event was part of the World Congress on Computational Intelligence (WCCI 2024) and was conducted in association with the International Joint Conference on Neural Networks (IJCNN). The workshop focused on the application of artificial intelligence to neurotechnology, ...",
      "author": "ieeebrain",
      "published_date": "2025-03-03T17:05:59+00:00",
      "source": "Brain",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2025-10-01T07:40:52.403550+00:00",
      "updated_at": "2025-10-01T08:19:40.634012+00:00",
      "metadata": {
        "processed_at": "2025-10-01T08:19:40.634014+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3352595883ccb35de24e20a19b774dd2",
      "url": "https://brain.ieee.org/braininsight-articles/ieee-journal-on-flexible-electronics/",
      "title": "Call for Papers: IEEE Brain Special Issue",
      "content": "In a unique interdisciplinary collaboration with the IEEE\u2019s Society on Social Implications of Technology (SSIT) and IEEE Brain, J-FLEX is joining forces to explore both the technology of the Internet-of-Medical-Things (IoMT) solutions and medical wearables/implantables. &#160;",
      "author": "ieeebrain",
      "published_date": "2025-03-03T17:16:40+00:00",
      "source": "Brain",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 36,
      "reading_time": 1,
      "created_at": "2025-10-01T07:40:52.403527+00:00",
      "updated_at": "2025-10-01T08:19:40.634016+00:00",
      "metadata": {
        "processed_at": "2025-10-01T08:19:40.634017+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "626415b9abcab4a79433d23aa150042f",
      "url": "https://brain.ieee.org/braininsight-articles/ieee-brain-joins-the-american-brain-coalition-as-a-nonprofit-member/",
      "title": "IEEE Brain Joins the American Brain Coalition",
      "content": "IEEE Brain is pleased to announce its acceptance as a nonprofit member of the American Brain Coalition (ABC), a prestigious alliance of over 150 organizations dedicated to advancing brain research, advocacy, and improving treatments for individuals affected by brain conditions. The ABC Board has enthusiastically welcomed IEEE Brain into its network, reinforcing a shared commitment to fostering innovation and collaboration ...",
      "author": "ieeebrain",
      "published_date": "2025-03-03T17:22:08+00:00",
      "source": "Brain",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2025-10-01T07:40:52.403487+00:00",
      "updated_at": "2025-10-01T08:19:40.634019+00:00",
      "metadata": {
        "processed_at": "2025-10-01T08:19:40.634021+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}