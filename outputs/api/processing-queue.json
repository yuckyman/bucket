{
  "last_updated": "2025-11-02T08:17:02.462507+00:00",
  "pending_count": 980,
  "processed_count": 20,
  "pending_articles": [
    {
      "id": "11da1006ee59369caf1a8b22a800f02c",
      "url": "https://erpinfo.org/blog/2025/8/20/boot-camp-summer-2026",
      "title": "10-Day ERP Boot Camp to be held June 15-24, 2026 in Davis, California",
      "content": "<p class=\"\">We have received another 5 years of funding from the National Institute of Mental Health, so we plan to hold ERP Boot Camps in each of the next 5 summers. The next one will be June 15-24, 2026 in Davis, California. The application portal will open around January 1, 2026.</p><p class=\"\">As in most previous years, all attendees will receive a scholarship that covers most or all travel and lodging expenses. There will be no registration fee.</p><p class=\"\"><a href=\"https://erpinfo.org/summer-boot-camp\">Click here</a> for more information about the ERP Boot Camp.</p><p class=\"\">If you would like to receive announcements about upcoming boot camps, <a href=\"https://erpinfo.org/bootcamp-email-list\">join our email list</a>. If you have any questions after reading this page and the <a href=\"https://erpinfo.org/application-info\">application information</a> page, please email us at <a href=\"mailto:erpbootcamp@gmail.com\">erpbootcamp@gmail.com</a>.</p>",
      "author": "Steve Luck",
      "published_date": "2025-08-20T15:07:14+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 125,
      "reading_time": 1,
      "created_at": "2025-11-02T07:38:21.111476+00:00",
      "updated_at": "2025-11-02T07:38:21.111481+00:00"
    },
    {
      "id": "10e7cbd7a57cdb9572fe7ed61cfa90d5",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005610?dgcid=rss_sd_all",
      "title": "Test-retest reproducibility of structural and proxy estimates of brain connectivity at rest",
      "content": "<p>Publication date: 15 November 2025</p><p><b>Source:</b> NeuroImage, Volume 322</p><p>Author(s): Aldana Lizarraga, Arianna Sala, Kathrin Koch, Igor Yakushev</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 16,
      "reading_time": 1,
      "created_at": "2025-11-02T07:38:00.160382+00:00",
      "updated_at": "2025-11-02T07:38:00.160383+00:00"
    },
    {
      "id": "d0f334a39a923038fcb68d0cbee9dd73",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005592?dgcid=rss_sd_all",
      "title": "Neurocognitive mechanisms of age-related decline in global motion perception",
      "content": "<p>Publication date: 15 November 2025</p><p><b>Source:</b> NeuroImage, Volume 322</p><p>Author(s): Yaxi Hong, Ting Liu, Dan Luo, Ziliang Zhu, Shizhen Yan, Hua Jin</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 20,
      "reading_time": 1,
      "created_at": "2025-11-02T07:38:00.160363+00:00",
      "updated_at": "2025-11-02T07:38:00.160365+00:00"
    },
    {
      "id": "2e04813de5c8b36c4e83691d63f5fc4d",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005580?dgcid=rss_sd_all",
      "title": "Disrupted temporal structure of the M/EEG meta-states sequencing in Alzheimer\u2019s disease",
      "content": "<p>Publication date: 15 November 2025</p><p><b>Source:</b> NeuroImage, Volume 322</p><p>Author(s): Marina Sandon\u00eds-Fern\u00e1ndez, Pablo N\u00fa\u00f1ez, Miguel A. Tola-Arribas, M\u00f3nica Cano, Hideyuki Hoshi, Yoshihito Shigihara, Jes\u00fas Poza, Carlos G\u00f3mez</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 25,
      "reading_time": 1,
      "created_at": "2025-11-02T07:38:00.160344+00:00",
      "updated_at": "2025-11-02T07:38:00.160345+00:00"
    },
    {
      "id": "22408adb7ebf0bcd0820733cf6724f30",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005361?dgcid=rss_sd_all",
      "title": "Paired-pulse TMS of premotor cortex produces non-linear suppressive effects on neural activity in the targeted network - a TMS-fMRI study",
      "content": "<p>Publication date: 15 November 2025</p><p><b>Source:</b> NeuroImage, Volume 322</p><p>Author(s): Laerke Gebser Krohne, Sofus Nygaard, Maud Eline Ottenheijm, Marie Louise Liu, Axel Thielscher, Hartwig Roman Siebner, Kristoffer Hougaard Madsen</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 27,
      "reading_time": 1,
      "created_at": "2025-11-02T07:38:00.160321+00:00",
      "updated_at": "2025-11-02T07:38:00.160323+00:00"
    },
    {
      "id": "72dec2cc17b184d0ef2d8590e7a0714f",
      "url": "https://www.biorxiv.org/content/10.1101/2025.10.28.685236v1?rss=1",
      "title": "Individual-level contextual alignment and aperiodic slope reveal improved comprehension of unexpected language",
      "content": "In everyday language use, individuals are required to comprehend complex, continuous and often unexpected linguistic inputs to effectively communicate with others. Evidence from other domains of processing such as memory and learning suggest that individuals these unexpected inputs can often be remembered or learnt just as well as predictable inputs in follow up testing. However, language comprehension research often focuses on how individuals process these stimuli rather than how the information which is gathered can be used at a later time. It is proposed that, in line with memory and learning, individuals can utilise unexpected language inputs to support a deeper understanding of the information which is being communicated. Further, this will be related to the interaction between an individual's alignment to local context and inter-individual differences in information processing, such as aperiodic slope. The present study uses an alignment metric which measures the relationship between individual level N400 amplitudes (a neurophysiological metric of predictability) and surprisal (a text-collection-based metric of predictability), where higher alignment suggests the individual has a stronger preexisting schema. A steeper aperiodic slope has been suggested to reflect greater adaptation of predictive models in response to prediction errors; therefore, it is hypothesised that depth of language comprehension will be modulated by both alignment and aperiodic slope. To explore this, 30 participants (23F, 1 not reported; mean age: 22.2 years [SD:4.4]) listened to 12 short stories (3 genres, audio-visual presentation) while their electroencephalogram (EEG) was recorded. Comprehension was tested via 6 multiple choice questions per story. An additional combined analysis, including a reanalysis of 40 audio-modality participants, provided evidence across both modalities that comprehension was improved for more surprising stories. Further, when considering inter-individual differences, the relationship between predictability and comprehension was shown to be modulated by both alignment and aperiodic activity. Comprehension of unexpected stories (as measured by N400 amplitudes) was improved only for lower aligned, steeper aperiodic slope individuals. This finding is considered a reflection of complementary processing strategies which are weighted towards incoming stimuli, increasing the likelihood of model updating and therefore resulting in improved comprehension of unexpected stories.",
      "author": "Platt, A. L. M.",
      "published_date": "2025-11-01T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 345,
      "reading_time": 1,
      "created_at": "2025-11-02T07:37:58.915594+00:00",
      "updated_at": "2025-11-02T07:37:58.915596+00:00"
    },
    {
      "id": "1a18badc67c7309e5b2b5634bfda4744",
      "url": "https://www.biorxiv.org/content/10.1101/2025.10.25.684561v1?rss=1",
      "title": "Is poor dose selection undermining the translational validity of antidepressant research involving animal models?",
      "content": "Background: Behavioural studies in animal models represent a critical component of psychiatric drug development. Positive results in animal studies have identified novel therapeutic targets for major depressive disorder (MDD) but efficacy in humans has largely not been borne out in clinical trials. A possible reason for this failed translation is inappropriate dose selection and the engagement of mechanisms not directly relevant to antidepressant effects in patients. Methods: We first used PubMed to identify preclinical rodent studies in two assays used to assess antidepressants; the conventional forced swim test, (FST) and more recently developed affective bias test, (ABT). Dose ranges were extracted, as well as information about subjects, timing and route of administration, and justification and efficacy of dose(s). Dose ranges were compared against calculated animal equivalent doses. Results: The median FST dose across all antidepressants was 10mg/kg, with median doses for each drug exceeding the relevant animal equivalent dose by 1.5-25x. In contrast, effective doses in the ABT showed closer alignment to those used clinically. In the second study, 232 ketamine and 202 fluoxetine papers involving MDD-related research in rodents were reviewed. The median dose was 10mg/kg for both drugs, exceeding animal equivalent doses by 1.6-3.2x and 3.2-6.5x for ketamine and fluoxetine, respectively. Conclusions: The results indicate pervasive use of antidepressant doses in conventional models of MDD that may not correspond with doses used in clinical practice. We discuss the implications of using doses which exceed therapeutic levels and the potential to engage receptors and underlying mechanisms which are not relevant to clinical effects.",
      "author": "Anderson, D., Hinchcliffe, J., Jackson, M. G., Robinson, E.",
      "published_date": "2025-11-01T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 254,
      "reading_time": 1,
      "created_at": "2025-11-02T07:37:58.915537+00:00",
      "updated_at": "2025-11-02T07:37:58.915542+00:00"
    },
    {
      "id": "fc7bc6dd862b31cbe580ad5f4da30838",
      "url": "http://www.jneurosci.org/cgi/content/short/45/40/etwij45402025?rss=1",
      "title": "This Week in The Journal",
      "content": "",
      "author": "McKeon, P.",
      "published_date": "2025-10-01T16:30:31+00:00",
      "source": "Journal Neuroscience This Week",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-11-02T07:37:54.429433+00:00",
      "updated_at": "2025-11-02T07:37:54.429435+00:00"
    },
    {
      "id": "82bde25702e7ab6957f9f70d2d2860a3",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1655462",
      "title": "Neuron synchronization analyzed through spatial-temporal attention",
      "content": "Neuronal synchronization refers to the temporal coordination of activity across populations of neurons, a process that underlies coherent information processing, supports the encoding of diverse sensory stimuli, and facilitates adaptive behavior in dynamic environments. Previous studies of synchronization have predominantly emphasized rate coding and pairwise interactions between neurons, which have provided valuable insights into emergent network phenomena but remain insufficient for capturing the full complexity of temporal dynamics in spike trains, particularly the interspike interval. To address this limitation, we performed in vivo neural ensemble recording in the primary olfactory center\u2014the antennal lobe (AL) of the hawk moth Manduca sexta\u2014by stimulating with floral odor blends and systematically varying the concentration of an individual odorant within one of the mixtures. We then applied machine learning methods integrating modern attention mechanisms and generative normalizing flows, enabling the extraction of semi-interpretable attention weights that characterize dynamic neuronal interactions. These learned weights not only recapitulated the established principles of neuronal synchronization but also facilitated the functional classification of two major cell types in the antennal lobe (AL) [local interneurons (LNs) and projection neurons (PNs)]. Furthermore, by experimentally manipulating the excitation/inhibition balance within the circuit, our approach revealed the relationships between synchronization strength and odorant composition, providing new insight into the principles by which olfactory networks encode and integrate complex sensory inputs.",
      "author": "Jeffrey A. Riffell",
      "published_date": "2025-10-16T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-11-02T07:37:48.742212+00:00",
      "updated_at": "2025-11-02T07:37:48.742214+00:00"
    },
    {
      "id": "8fe0a8acff4ee28593be3c2d26e9cfb0",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1616472",
      "title": "Modeling cognition through adaptive neural synchronization: a multimodal framework using EEG, fMRI, and reinforcement learning",
      "content": "IntroductionUnderstanding the cognitive process of thinking as a neural phenomenon remains a central challenge in neuroscience and computational modeling. This study addresses this challenge by presenting a biologically grounded framework that simulates adaptive decision making across cognitive states.MethodsThe model integrates neuronal synchronization, metabolic energy consumption, and reinforcement learning. Neural synchronization is simulated using Kuramoto oscillators, while energy dynamics are constrained by multimodal activity profiles. Reinforcement learning agents\u2014Q-learning and Deep Q-Network (DQN)\u2014modulate external inputs to maintain optimal synchrony with minimal energy cost. The model is validated using real EEG and fMRI data, comparing simulated and empirical outputs across spectral power, phase synchrony, and BOLD activity.ResultsThe DQN agent achieved rapid convergence, stabilizing cumulative rewards within 200 episodes and reducing mean synchronization error by over 40%, outperforming Q-learning in speed and generalization. The model successfully reproduced canonical brain states\u2014focused attention, multitasking, and rest. Simulated EEG showed dominant alpha-band power (3.2\u202f\u00d7\u202f10\u22124\u202fa.u.), while real EEG exhibited beta-dominance (3.2\u202f\u00d7\u202f10\u22124\u202fa.u.), indicating accurate modeling of resting states and tunability for active tasks. Phase Locking Value (PLV) ranged from 0.9806 to 0.9926, with the focused condition yielding the lowest circular variance (0.0456) and a near significant phase shift compared to rest (t\u202f=\u202f\u22122.15, p\u202f=\u202f0.075). Cross-modal validation revealed moderate correlation between simulated and real BOLD signals (r\u202f=\u202f0.30, resting condition), with delayed inputs improving temporal alignment. General Linear Model (GLM) analysis of simulated BOLD data showed high region-specific prediction accuracy (R2\u202f=\u202f0.973\u20130.993, p\u202f<\u202f0.001), particularly in prefrontal, parietal, and anterior cingulate cortices. Voxel-wise correlation and ICA decomposition confirmed structured network dynamics.DiscussionThese findings demonstrate that the framework captures both electrophysiological and spatial aspects of brain activity, respects neuroenergetic constraints, and adaptively regulates brain-like states through reinforcement learning. The model offers a scalable platform for simulating cognition and developing biologically inspired neuroadaptive systems.ConclusionThis work provides a novel and testable approach to modeling thinking as a biologically constrained control problem and lays the groundwork for future applications in cognitive modeling and brain-computer interfaces.",
      "author": "Horace T. Crogman",
      "published_date": "2025-10-16T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 332,
      "reading_time": 1,
      "created_at": "2025-11-02T07:37:48.742175+00:00",
      "updated_at": "2025-11-02T07:37:48.742176+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "d1b3a64c1957f2b048e1e94f5d37c6e5",
      "url": "https://erpinfo.org/blog/2024/3/15/registration-full",
      "title": "Registration is now full for the 2024 ERP Boot Camp",
      "content": "<p class=\"\">The demand for the<a href=\"https://erpinfo.org/2024-erp-boot-camp\"> 2024 ERP Boot Camp</a> was far beyond our expectations, and we reached our maximum registration of 30 people within one day. We already have a waiting list of over 30 people, so we have closed the registration site.</p><p class=\"\">We realize that this is very disappointing to many people. We hope to offer another workshop like this next summer, or possibly earlier.</p><p class=\"\">If you would like to get announcements about upcoming boot camps and webinars, you should <a href=\"https://erpinfo.org/bootcamp-email-list\">join our email list</a>.</p><p class=\"\">You may also consider hosting a <a href=\"https://erpinfo.org/mini-erp-boot-camps\">Mini ERP Boot Camp</a> at your institution (in person or over Zoom).</p>",
      "author": "Steve Luck",
      "published_date": "2024-03-16T15:14:42+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 106,
      "reading_time": 1,
      "created_at": "2025-11-02T07:38:21.111899+00:00",
      "updated_at": "2025-11-02T08:17:02.359518+00:00",
      "metadata": {
        "processed_at": "2025-11-02T08:17:02.359527+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "e1385798428586a67ced89a895faeb47",
      "url": "https://erpinfo.org/blog/2024/6/10/erp-core-decoding-paper",
      "title": "New Paper: Using Multivariate Pattern Analysis to Increase Effect Sizes for ERP Amplitude Comparisons",
      "content": "<p class=\"\">Carrasco, C. D., Bahle, B., Simmons, A. M., &amp; Luck, S. J. (2024). Using multivariate pattern analysis to increase effect sizes for event-related potential analyses. Psychophysiology, 61, e14570. <a href=\"https://doi.org/10.1111/psyp.14570\">https://doi.org/10.1111/psyp.14570</a> [<a href=\"https://doi.org/10.1101/2023.11.07.566051\">preprint</a>]</p><p class=\"\">Multivariate pattern analysis (MVPA) can be used to \u201cdecode\u201d subtle information from ERP signals, such as which of several faces a participant is perceiving or the orientation that someone is holding in working memory (see <a href=\"https://erpinfo.org/blog/2018/9/16/decoding\">this previous blog post</a>). This approach is so powerful that we started wondering whether it might also give us greater statistical power in more typical experiments where the goal is to determine whether an ERP component differs in amplitude across experimental conditions. For example, might we more easily be able to tell if N400 amplitude is different between two different classes of words by using decoding? If so, that might make it possible to detect effects that would otherwise be too small to be significant.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"688\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/08f353c7-f484-4e87-b5d3-a256fe1206e2/N170_ES.png?format=1000w\" width=\"971\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">To address this question, we compared decoding with the conventional ERP analysis approach with using the 6 experimental paradigms in the <a href=\"https://doi.org/10.18115/D5JW4R\">ERP CORE</a>. In the conventional ERP analysis, we measured the mean amplitude during the standard measurement window from each participant in the two conditions of the paradigm (e.g., faces versus cars for N170, deviants versus standards for MMN). We quantified the magnitude of the difference between conditions using Cohen\u2019s <em>dz</em> (the variant of Cohen\u2019s <em>d</em> corresponding to a paired <em>t</em> test). For example, the effect size in the conventional ERP comparison of faces versus cars in the N170 paradigm was approximately 1.7 (see the figure).</p><p class=\"\">We also applied decoding to each paradigm. For example, in the N170 paradigm, we trained a support vector machine (SVM) to distinguish between ERPs elicited by faces and ERPs elicited by cars. This was done separately for each subject, and we converted the decoding accuracy into Cohen\u2019s <em>dz</em> so that it could be compared with the <em>dz</em> from the conventional ERP analysis. As you can see from the bar labeled SVM in the figure above, the effect size for the SVM-based decoding analysis was almost twice as large as the effect size for the conventional ERP analysis. That\u2019s a huge difference!</p><p class=\"\">We found a similar benefit for SVM-based decoding over conventional ERP analyses in 7 of the 10 cases we tested (see the figure below). In the other 3 cases, the ERP and SVM effects were approximately equivalent. So, there doesn\u2019t seem to be a downside to using decoding, at least in terms of effect size. But there can be a big benefit.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1371\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/d16f0782-7205-4d50-95e1-c6729cbc153e/All_Components.png?format=1000w\" width=\"4641\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">Because decoding has many possible benefits, we\u2019ve added it into <a href=\"ERPLAB Toolbox\">ERPLAB Toolbox</a>. It\u2019s super easy to use, and we\u2019ve created <a href=\"https://erpinfo.org/blog/2023/6/23/decoding-webinar\">detailed documentation and a video</a> to explain how it works at a conceptual level and to show you how to use it.</p><p class=\"\">We encourage you to apply it to your own data. It may give you the power to detect effects that are too small to be detected with conventional ERP analyses.</p>",
      "author": "Steve Luck",
      "published_date": "2024-06-10T18:01:45+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 525,
      "reading_time": 2,
      "created_at": "2025-11-02T07:38:21.111870+00:00",
      "updated_at": "2025-11-02T08:17:02.359531+00:00",
      "metadata": {
        "processed_at": "2025-11-02T08:17:02.359534+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "906f73f5c36ba087882a0ad17e01fc20",
      "url": "https://erpinfo.org/blog/2024/6/11/erplab-studio",
      "title": "New software package: ERPLAB Studio",
      "content": "<p class=\"\">We are excited to announce the release of a new EEG/ERP analysis package, <a href=\"https://github.com/ucdavis/erplab/releases\">ERPLAB Studio</a>. We think it\u2019s a huge improvement over the classic EEGLAB user interface. See our cheesy <a href=\"https://www.youtube.com/watch?v=lIaKVQ9DD6E\">\u201cadvertisement\u201d video</a> to get a quick overview. </p><p class=\"\">Rather than operating as an EEGLAB plugin, ERPLAB Studio is a standalone Matlab program that provides a more efficient and user-friendly interface to the most commonly used EEGLAB and ERPLAB routines.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/c874d4ec-5186-4de9-981b-58010c7a06e1/Interface.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">With ERPLAB Studio, you automatically see the EEG or ERP waveforms as soon as you load a file. And as soon as you perform an operation, you see what the new EEG/ERP looks like. For example, when you filter the data, you immediately see the filtered waveforms.</p><p class=\"\">You can even select multiple datasets and apply an operation like artifact detection on all of them in one step. And then you can immediately see the results, such as which EEG epochs have been marked with artifacts.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/b45f514d-2d21-4a5a-8be6-f3a8ff99c388/Artifacts.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">We give you access to EEGLAB\u2019s ICA-based artifact correction tools, but with a nice bonus. You can plot the ICA activations in the same window with the EEG data, making it easy to see which ICA components correspond to specific artifacts such as eyeblinks.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/8bc191da-9040-4042-ae9c-550cd98def7d/ICA.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">The program has an EEG tab for processing continuous and epoched EEG data, and an ERP tab for processing averaged ERPs.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/84bdd9df-b02e-4fc5-83b9-1139a91938f5/Tabs.jpg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">The automatic ERP plotting makes it easy for you to view the data laid out according to the electrode locations. And we have an Advanced Waveform Viewer that can make publication-quality plots.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/a932631f-fc30-415f-b11d-660d2bf90da5/ERP.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">ERPLAB Studio is mainly just a new user interface. Under the hood, we\u2019re running the same EEGLAB and ERPLAB routines you\u2019ve always used. And scripting is identical.</p><p class=\"\">ERPLAB Studio is included in <a href=\"https://github.com/ucdavis/erplab/releases\">version 11 and higher of ERPLAB</a>. You simply follow our <a href=\"https://github.com/ucdavis/erplab/wiki/installation\">download/installation instructions</a> and then type estudio from the Matlab command line. </p><p class=\"\">If you\u2019re new to ERPLAB, we strongly recommend that you go through our <a href=\"https://github.com/ucdavis/erplab/wiki/ERPLAB-Studio-Tutorial\" target=\"_blank\">tutorial</a> before starting to process your own data. </p><p class=\"\">If you already know how to use the original version of ERPLAB (which we now call ERPLAB Classic), you can quickly learn how to use ERPLAB Studio with our <a href=\"https://ucdavis.box.com/s/i4jfv22gv6rj9t5obctuk6yaruxqomcc\">Transition Guide</a>.</p><p class=\"\">We also have a <a href=\"https://github.com/ucdavis/erplab/wiki/ERPLAB-Studio-Manual\">manual</a> that describes every feature in detail. </p>",
      "author": "Steve Luck",
      "published_date": "2024-06-12T02:02:16+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 444,
      "reading_time": 2,
      "created_at": "2025-11-02T07:38:21.111796+00:00",
      "updated_at": "2025-11-02T08:17:02.359537+00:00",
      "metadata": {
        "processed_at": "2025-11-02T08:17:02.359538+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "79d603b3db5911be59b9e07e11acc674",
      "url": "https://erpinfo.org/blog/2024/6/28/recording-and-slides-now-available-for-erplab-studio-webinar",
      "title": "Recording and slides now available for ERPLAB Studio webinar",
      "content": "<p class=\"\">We held a webinar to demonstration ERPLAB Studio on 28 June 2024.</p><p class=\"\"><a href=\"https://youtu.be/k-nGv00rTP8\">Click here</a> to access a recording.</p><p class=\"\"><a href=\"https://ucdavis.box.com/s/4fseqz6327dtuouauj12rgvivy1d1nmo\">Click here </a>to access a PDF of the slides.</p>",
      "author": "Steve Luck",
      "published_date": "2024-06-28T22:21:45+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 30,
      "reading_time": 1,
      "created_at": "2025-11-02T07:38:21.111695+00:00",
      "updated_at": "2025-11-02T08:17:02.359540+00:00",
      "metadata": {
        "processed_at": "2025-11-02T08:17:02.359542+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f485a145c3b839418e3d039dc3a92ea6",
      "url": "https://erpinfo.org/blog/2025/3/20/new-paper-oddball",
      "title": "New Paper: Does the P3b component reflect working memory updating?",
      "content": "<p class=\"\">Carrasco, C. D., Simmons, A. M., Kiat, J. E., &amp; Luck, S. J. (in press). Enhanced working memory representations for rare events. <em>Psychophysiology</em>. <a href=\"https://doi.org/10.1111/psyp.70038\">https://doi.org/10.1111/psyp.70038</a> [<a href=\"https://doi.org/10.1101/2024.03.20.585952\">preprint</a>]</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n<hr />\n\n\n  <p class=\"\">For decades, many ERP researchers have believed that the P3b wave (sometimes called P300) is a scalp manifestation of a process that updates working memory. This idea originated with Manny Donchin\u2019s <em>context updating</em> hypothesis of the P3b (<a href=\"https://doi.org/10.1111/j.1469-8986.1981.tb01815.x\">Donchin, 1981</a>). Donchin\u2019s idea of <em>context</em> was pretty different from working memory, but as this hypothesis percolated through the field over time, it gradually morphed into the idea that the P3b reflects the updating of working memory.</p><p class=\"\">Rolf Verleger mounted a major attack on the original context updating hypothesis in a classic review article in BBS (<a href=\"https://doi.org/10.1017/S0140525X00058015\">Verleger, 1988</a>), which was followed by a vigorous rebuttal by <a href=\"https://doi.org/10.1017/S0140525X00058027\">Donchin and Coles (1988)</a>. These are interesting papers to read, but they did not settle the issue. In the ensuing years, as the field became more focused on working memory instead of context, I\u2019m aware of no studies that directly tested the hypothesis that the P3b reflects working memory updating. </p><p class=\"\">One reason for the lack of direct evidence is that the oddball paradigms typically used to elicit the P3b do not provide a sensitive assessment of working memory. In a typical paradigm, for example, participants would see a sequence of 90% Xs and 10% Os, and the task would be to press one button for X and another button for O. The responses are made immediately, so it is not necessary to store the stimuli in working memory. Even if participants were asked to make a delayed response, the Xs and Os are so easily discriminable that memory performance would likely be at ceiling.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"698\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/f1d6cee6-5eab-4240-bda0-1a2b7bf4bf88/Figure_1.png?format=1000w\" width=\"720\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 1</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">A few years ago, my lab (especially Carlos Carrasco, Aaron Simmons, and John Kiat) got interested in trying to test the working memory encoding hypothesis. We ran a couple of experiments, but we couldn\u2019t quite figure out the right design. Finally, we figured it out. We used a modified oddball paradigm in which a little dot appeared at one of many locations around an circle (see Figure 1). </p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1112\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/f68d38c2-c87c-4512-b0a7-fbecd75969ff/Figure_2.png?format=1000w\" width=\"1728\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 2</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">On each trial, participants pressed one button if the circle was close to one of the four cardinal axes (left, right, top, and bottom) and a different button if it was close to one of the four diagonals (upper left, upper right, lower left, and lower right). One of these two categories was rare (the <em>oddballs</em>) and the other was frequent (the <em>standards</em>; counterbalanced across trial blocks). As is usual in oddball paradigms, the P3b was much larger for trials in the rare category than for trials in the frequent category (see Figure 2).</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1256\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/89260e9e-be6e-48b1-adc4-58b77a418deb/Figure_3.png?format=1000w\" width=\"1996\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 3</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">The main question was whether the location of the dot was encoded in working memory better for the oddball trials than for the standard trials. To assess this, the experimental design contained occasional <em>probe</em> trials on which participants used the mouse to click on the exact location of the dot (see Figure 3). That is, after participants made the cardinal/diagonal buttonpress responses, they were sometimes then asked to click on the remembered location of the dot. This happened on only 12.5% of trials, selected at random, so that participants would mainly focus on the oddball task. </p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"706\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/166f7236-f77b-431b-8ebf-9633580dcf31/Figure_4.png?format=1000w\" width=\"1800\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 4</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">We looked at the accuracy of these probe responses, calculated as the (absolute value of the) angular distance between the true location and the reported location. As shown in Figure 4, the response error of the probe responses was reduced for the oddball trials relative to the standard trials. In other words, working memory was better for the P3b-eliciting oddball trials than for the standards. Moreover, we found that participants with large P3b amplitudes on oddball trials had smaller response errors on oddball trials (whereas this correlation was not present for standards).</p><p class=\"\">At first glance, these findings seem like support for the idea that the P3b reflects working memory updating. However, the story is not that simple. For example, when we looked at single-trial P3b amplitudes, response errors were not lower for trials with larger P3b amplitudes than for trials with smaller P3b amplitudes.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1122\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/a7a11955-3619-4499-9ef4-61f72a408561/Figure_5.png?format=1000w\" width=\"1378\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">We also used ERP decoding to test whether the exact location of the dot was better stored in working memory on oddball trials than on standard trials (<a href=\"https://erpinfo.org/blog/2023/6/23/decoding-webinar\">click here</a> for information about how ERP decoding works and how you can decode your own data using ERPLAB Toolbox). As shown in Figure 5, we could decode the location of the dot better on oddball trials than on standard trials during the period following the P3b component. Note, however, that this was a pretty small difference that only barely crossed the threshold for statistical significance (p = .048). I would really like to see this effect replicated before fully believing it. However, the behavioral effect was rock solid (and replicated in a follow-up experiment).</p><p class=\"\">What can we conclude from these findings? When we started the project, I knew that it would be difficult to draw any strong causal conclusions about the relationship between the P3b component and working memory updating. That is, even if we saw both a larger P3b and improved working memory on oddball trials, this would just be a correlation and could potentially be explained by a third variable such as attention. But if we saw a big difference in working memory between oddballs and standards, and if we found that working memory was better on trials with larger P3b amplitudes, this would be at least consistent with the idea that the process that produces the P3b on the scalp is also involved in working memory encoding.</p><p class=\"\">However, although we saw an enormous difference in P3b amplitude between oddball trials and standard trials, we saw only small differences in working memory between oddballs and standards, whether measured via behavioral response errors on probe trials or EEG decoding accuracy. If the process that generates the scalp P3b voltage plays a major role in working memory encoding, then we would have expected a much larger working memory difference between oddballs and standards. Moreover, although we found that participants with larger P3b amplitudes had smaller response errors, we did not find any evidence that working memory was any better on trials with larger P3b amplitudes (even though we looked very hard for such a relationship). The bottom line is that, although I was really hoping we would finally provide some direct evidence for the working memory encoding hypothesis, the results of this study have actually convinced me that the P3b is probably not related to working memory encoding.</p><p class=\"\">What, then, explains the small but statistically significant differences in working memory accuracy between oddballs and standards, along with the subjectwise correlation between P3b amplitude and behavioral response errors? A very plausible explanation is that both the P3b component and working memory encoding are facilitated by increased attention. That is, there are several sources of evidence that rare events trigger increased attention, and this could independently produce a larger P3b and improved working memory.</p><p class=\"\">Of course, this is just one experiment, so I wouldn\u2019t say that the working memory encoding hypothesis is completely dead. But given our new findings and the general lack of direct evidence for the hypothesis, it\u2019s on life support.</p><p class=\"\">If the P3b doesn\u2019t reflect working memory encoding, then what does it reflect? This seems like a significant question: the P3b is huge and is observed across a broad range of experimental paradigms, and it\u2019s reasonable to assume that the underlying process must be important for the brain to devote so many watts of energy to it. In fact, I find it embarrassing that our field has not answered this question in the 60 years since <a href=\"https://doi.org/10.1126/science.150.3700.1187\">the P3b was first discovered</a>.</p><p class=\"\">My best bet is that the P3b is related to the process of making decisions about actions (where the term <em>actions</em> is broadly construed to include the withholding of responses and mental actions such as counting). This is related to the fact that the amplitude of the P3b is related to the probability of a task-defined category, not the probability of a physical stimulus category. Rolf Verleger has a nice review of the evidence for this idea (<a href=\"https://doi.org/10.1111/psyp.13542\">Verleger, 2020</a>). But it is still not clear to me why the brain devotes so many watts of energy to creating a large P3b when a rare task-defined category occurs. Verleger notes that several hypotheses about the P3b are compatible with the finding of a larger P3b for oddballs than for standards, but in my view these hypotheses have a hard time explaining the enormous size of the P3b observed for oddballs. This is a longstanding mystery in need of a solution!</p>",
      "author": "Steve Luck",
      "published_date": "2025-03-21T03:42:26+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1547,
      "reading_time": 7,
      "created_at": "2025-11-02T07:38:21.111670+00:00",
      "updated_at": "2025-11-02T08:17:02.359544+00:00",
      "metadata": {
        "processed_at": "2025-11-02T08:17:02.359546+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "80d1f642eb4da0958753076107ac0cbe",
      "url": "http://daniellakens.blogspot.com/2025/07/easily-download-files-from-open-science.html",
      "title": "Easily download files from the Open Science Framework with Papercheck",
      "content": "<p>Researchers\nincreasingly use the <a href=\"https://osf.io/\">Open Science Framework</a> (OSF) to share files, such as data\nand code underlying scientific publications, or presentations and materials for\nscientific workshops. The OSF is an amazing service that has contributed\nimmensely to a changed research culture where psychologists share data, code, and\nmaterials. We are very grateful it exists.</p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">But it is\nnot always the most user-friendly. Specifically, downloading files from the OSF\nis a bigger hassle than we (<a href=\"https://debruine.github.io/\">Lisa DeBruine</a>\nand <a href=\"https://www.tue.nl/en/research/researchers/daniel-lakens/\">Daniel\nLakens</a>, the developers of Papercheck) would like it to be. Downloading individual\nfiles is so complex, <a href=\"https://bsky.app/profile/malte.the100.ci/post/3lpf4s6spns2i\">Malte Elson</a>\nrecently posted this meme on Bluesky. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"NL\"><!--[if gte vml 1]><v:shapetype\n id=\"_x0000_t75\" coordsize=\"21600,21600\" o:spt=\"75\" o:preferrelative=\"t\"\n path=\"m@4@5l@4@11@9@11@9@5xe\" filled=\"f\" stroked=\"f\">\n <v:stroke joinstyle=\"miter\"/>\n <v:formulas>\n  <v:f eqn=\"if lineDrawn pixelLineWidth 0\"/>\n  <v:f eqn=\"sum @0 1 0\"/>\n  <v:f eqn=\"sum 0 0 @1\"/>\n  <v:f eqn=\"prod @2 1 2\"/>\n  <v:f eqn=\"prod @3 21600 pixelWidth\"/>\n  <v:f eqn=\"prod @3 21600 pixelHeight\"/>\n  <v:f eqn=\"sum @0 0 1\"/>\n  <v:f eqn=\"prod @6 1 2\"/>\n  <v:f eqn=\"prod @7 21600 pixelWidth\"/>\n  <v:f eqn=\"sum @8 21600 0\"/>\n  <v:f eqn=\"prod @7 21600 pixelHeight\"/>\n  <v:f eqn=\"sum @10 21600 0\"/>\n </v:formulas>\n <v:path o:extrusionok=\"f\" gradientshapeok=\"t\" o:connecttype=\"rect\"/>\n <o:lock v:ext=\"edit\" aspectratio=\"t\"/>\n</v:shapetype><v:shape id=\"_x0000_i1026\" type=\"#_x0000_t75\" style='width:453.75pt;\n height:276pt;visibility:visible;mso-wrap-style:square'>\n <v:imagedata src=\"file:///C:/Users/DLakens/AppData/Local/Temp/msohtmlclip1/01/clip_image001.jpg\"\n  o:title=\"\"/>\n</v:shape><![endif]--><!--[if !vml]--><img border=\"0\" height=\"368\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEinS5tFoL5qX14Z2OcgT1APMZLGlghAD3BfBhSnJ9pleVbJyRFUFLShqReS2j7SXGozmLMcVQkoM62UhneJDtH4yx3NRnXOkVZZi-Dm-OPwbGaidxr2raF9wzjx5fU92nfPpE3jmGfwZEwHS1WGSB4gUfRfV3XWjOC2-lCjOYR108h3fwORIHOnqxIX9m8\" width=\"605\" /><!--[endif]--></span><span lang=\"EN-US\"></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span>&nbsp;</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Not only is\nthe download button for files difficult to find, but downloading all files\nrelated to a project can be surprisingly effortful. It is possible to download\nall files in a zip folder that will be called \u2018osfstorage-archive.zip\u2019 when\ndownloaded. But as the OSF supports a nested folder structure, you might miss a\nfolder, and you will quickly end up with \u2018osfstorage-archive (1).zip\u2019, \u2018osfstorage-archive\n(2).zip\u2019, etc. Unzipping these archives creates a lot of files without the\norganized folder structure, in folders with meaningless names, making it\ndifficult to understand where files are. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<h2 style=\"text-align: left;\"><b><span lang=\"EN-US\">The\nosf_file_download function in Papercheck </span></b></h2>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">We have added\na new function to our R package \u2018<a href=\"https://scienceverse.github.io/papercheck/\">Papercheck</a>\u2019 that will download all files and\nfolders in an OSF repository. It saves all files by recreating the folder\nstructure from the OSF in your download folder. Just install Papercheck, load\nthe library, and use the osf_file_download function to grab all files on the OSF:</span></p><p class=\"MsoNormal\"><span lang=\"EN-US\"><br /></span></p>\n\n<div style=\"text-align: left;\"><span style=\"font-family: courier;\"><b><span lang=\"EN-US\">devtools::install_github(\"scienceverse/papercheck\")<br /></span></b><b><span lang=\"EN-US\">library(papercheck)<br /></span></b><b><span lang=\"EN-US\">osf_file_download(\"6nt4v\")</span></b></span></div>\n\n\n\n\n\n<p class=\"MsoNormal\"><b><span lang=\"EN-US\">&nbsp;</span></b></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">All files\nwill be downloaded to your working directory. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Are you feeling\nFOMO for missing out on the <a href=\"https://www.kcl.ac.uk/events/open-research-summer-school-2025\">King Open\nResearch Summer School</a> that is going on these days, where <a href=\"https://research.tue.nl/en/persons/sajedeh-rasti\">Sajedeh Rasti</a> talked\nabout Preregistration, and <a href=\"https://research.tue.nl/en/persons/cristian-mesquida-caldentey\">Cristian\nMesquida</a> will give a workshop on using Papercheck? Well, at least it is\nvery easy to download all the files they have shared on the OSF and look at the\npresentations: </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"b7es8\")</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">In the\noutput, we see that by default large files (more than 10mb) are omitted. <span><!--[if gte vml 1]><v:shape id=\"Picture_x0020_1\"\n o:spid=\"_x0000_i1025\" type=\"#_x0000_t75\" alt=\"A screenshot of a computer&#10;&#10;AI-generated content may be incorrect.\"\n style='width:453.75pt;height:292.5pt;visibility:visible;mso-wrap-style:square'>\n <v:imagedata src=\"file:///C:/Users/DLakens/AppData/Local/Temp/msohtmlclip1/01/clip_image003.png\"\n  o:title=\"A screenshot of a computer&#10;&#10;AI-generated content may be incorrect\"/>\n</v:shape><![endif]--><!--[if !vml]--><img alt=\"A screenshot of a computer\n\nAI-generated content may be incorrect.\" border=\"0\" height=\"390\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEj2jH76ywmEeLzL43u6gJl7GKR2lEGlhYS0LOXRPMMovOsJEVdXyamYCvmq96ez3p_GXHLoFz4JDyAKHlARK9pSy8QfzVa7yt1_uEg_H9IJfKgunnYWUwlROXABNcU6TvrA0_hx0KPZfj00b3Cb-Wn_7Bf3sFu9JApZoClcWoh_Vfl5bk1GQVZUH1tuGao\" width=\"605\" /><!--[endif]--></span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">If you want\nto download all files, regardless of the size, then set the parameter to ignore\nthe maximum file size: </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"b7es8\",\nmax_file_size = NULL)</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Sometimes\nyou might want to download all files but ignore the file structure on the OSF,\nto just have all the files in one folder. Setting the parameter ignore_folder_structure\n= TRUE will give you all the files on the OSF in a single folder. By default, files will be downloaded into your working directory, but you can also specify\nwhere you want the files to be saved. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"6nt4v\",\nignore_folder_structure = TRUE, download_to = \"C:\\\\test_download\")</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">We hope\nthis function will make it easier for reviewers to access all supplementary\nfiles stored on the OSF during peer review, and for researchers who want to re-use\ndata, code, or materials shared on the OSF by downloading all the files they\nneed easily. Make sure to install the latest version of Papercheck (0.0.0.9050)\nto get access to this new function. Papercheck is still in active development,\nso report any bugs on <a href=\"https://github.com/scienceverse/papercheck\">GitHub</a>.</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>",
      "author": "noreply@blogger.com (Daniel Lakens)",
      "published_date": "2025-07-22T09:53:00+00:00",
      "source": "Twenty Percent Statistician",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 765,
      "reading_time": 3,
      "created_at": "2025-11-02T05:41:59.420940+00:00",
      "updated_at": "2025-11-02T06:21:19.630553+00:00",
      "metadata": {
        "processed_at": "2025-11-02T06:21:19.630564+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "5cf06dd1c8477abb17ef4e5c3b5426e0",
      "url": "https://erpinfo.org/blog/2021/12/22/applications-2023",
      "title": "Applications now being accepted for UC-Davis/SDSU ERP Boot Camp, July 31 \u2013 August 9, 2023",
      "content": "<p class=\"\">The next 10-day ERP Boot Camp will be held July 31 \u2013 August 9, 2023 in San Diego, California. We are now taking applications, which will be due by April 1, 2023. <a href=\"https://erpinfo.org/summer-boot-camp\">Click here</a> for more information.</p><p class=\"\">We are currently planning to hold this workshop as an in-person event. However, these plans are subject to change as the COVID-19 pandemic evolves. If the event is held in person, we will require that everyone is fully vaccinated, and we will also implement any other safety measures that are warranted at the time of the workshop.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"980\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/1609175691205-RTD3XM69YGOFMVP23U6T/Boot_Camp_Logo.png?format=1000w\" width=\"1148\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>",
      "author": "Steve Luck",
      "published_date": "2023-01-16T18:31:57+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 108,
      "reading_time": 1,
      "created_at": "2025-11-02T05:41:56.732103+00:00",
      "updated_at": "2025-11-02T06:21:19.630571+00:00",
      "metadata": {
        "processed_at": "2025-11-02T06:21:19.630573+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "bd7398ecbbd90ecd3269866b2fd3744f",
      "url": "https://erpinfo.org/blog/2023/6/23/decoding-webinar",
      "title": "ERP Decoding for Everyone: Software and Webinar",
      "content": "<p class=\"\"><strong>You can access the recording </strong><a href=\"https://video.ucdavis.edu/media/Virtual+ERP+Boot+CampA+Decoding+for+Everyone%2C+July+25+2023/1_lmwj6bu0\"><strong>here</strong></a><strong>.<br />You can access the final PDF of the slides </strong><a href=\"https://ucdavis.box.com/s/flf9gzeo12rz2jhxptih7xjl0omka2k7\"><strong>here</strong></a><strong>. <br />You can access the data </strong><a href=\"https://doi.org/10.18115/D5KS6S\"><strong>here</strong></a><strong>.</strong></p><p class=\"\">fMRI research has used decoding methods for over 20 years. These methods make it possible to decode what an individual is perceiving or holding in working memory on the basis of the pattern of BOLD activity across voxels. Remarkably, these methods can also be applied to ERP data, using the pattern of voltage across electrode sites rather than the pattern of activity across voxels to decode the information being represented by the brain (<a href=\"https://erpinfo.org/blog/2018/9/16/decoding\">see this previous blog post</a>). For example, ERPs can be used to decode the identity of a face that is being perceived, the emotional valence of a scene, the identity and semantic category of a word, and the features of an object that is being maintained in working memory. Moreover, decoding methods can be more sensitive than traditional methods for detecting conventional ERP effects (e.g., whether a word is semantically related or unrelated to a previous word in an N400 paradigm).</p><p class=\"\">So far, these methods have mainly been used by a small set of experts. We aim to change that with the upcoming Version 10 of <a href=\"https://erpinfo.org/erplab\">ERPLAB Toolbox</a>. This version of ERPLAB will contain an ERP decoding tool that makes it trivially easy for anyone who knows how to do conventional ERP processing to take advantage of the power of decoding. It should be available in mid-July at <a href=\"https://github.com/ucdavis/erplab/releases\">our GitHub site</a>. You can join the <a href=\"https://github.com/ucdavis/erplab/wiki/ERPLAB-email-list\">ERPLAB email list</a> to receive an announcement when this version is released. Please do not contact us with questions until it has been released and you have tried using it.</p><p class=\"\">On July 25, 2023, we will hold a 2-hour Zoom webinar to explain how decoding works at a conceptual level and show how to implement in ERPLAB Toolbox. The webinar will begin at 9:00 AM Pacific Time (California), 12:00 PM Eastern Time (New York), 5:00 PM British Summer Time (London), 6:00 PM Central European Summer Time (Berlin). </p><p class=\"\">The webinar is co-sponsored by the <a href=\"https://erpinfo.org/the-erp-boot-camp\">ERP Boot Camp</a> and the <a href=\"https://sprweb.org\">Society for Psychophysiological Research</a>. It is completely free, but you must register in advance at <a href=\"https://ucdavis.zoom.us/meeting/register/tJUrc-CtpzorEtBSmZXJINOlLJB9ZR0evpr4\">https://ucdavis.zoom.us/meeting/register/tJUrc-CtpzorEtBSmZXJINOlLJB9ZR0evpr4</a>. Once you register, you will receive an email with your own individual Zoom link. </p><p class=\"\">We will make a recording available a few days after the webinar on the <a href=\"https://erpinfo.org\">ERPinfo.org</a> web site.</p><p class=\"\">Please direct any questions about the webinar to <a href=\"mailto:erpbootcamp@gmail.com\">erpbootcamp@gmail.com</a>.</p>",
      "author": "Steve Luck",
      "published_date": "2023-06-23T21:05:26+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 420,
      "reading_time": 2,
      "created_at": "2025-11-02T05:41:56.732077+00:00",
      "updated_at": "2025-11-02T06:21:19.630576+00:00",
      "metadata": {
        "processed_at": "2025-11-02T06:21:19.630578+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3d370217cb4a351bb54e7854066e15c3",
      "url": "https://erpinfo.org/blog/2024/2/4/optimal-filters",
      "title": "New Papers: Optimal Filter Settings for ERP Research",
      "content": "<p class=\"\">Zhang, G., Garrett, D. R., &amp; Luck, S. J. (in press). Optimal filters for ERP research I: A general approach for selecting filter settings. <em>Psychophysiology</em>. <a href=\"https://doi.org/10.1111/psyp.14531\"><span>https://doi.org/10.1111/psyp.14531</span></a> [<a href=\"https://www.researchgate.net/publication/377773270_Optimal_filters_for_ERP_research_I_A_general_approach_for_selecting_filter_settings\"><span>preprint</span></a>]</p><p class=\"\">Zhang, G., Garrett, D. R., &amp; Luck, S. J. (in press). Optimal filters for ERP research II: Recommended settings for seven common ERP components. <em>Psychophysiology</em>. <a href=\"https://doi.org/10.1111/psyp.14530\"><span>https://doi.org/10.1111/psyp.14530</span></a> [<a href=\"https://www.researchgate.net/publication/377766656_Optimal_filters_for_ERP_research_II_Recommended_settings_for_seven_common_ERP_components\"><span>preprint</span></a>]</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1490\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/d64086cc-e3b4-457d-89df-08d9b3f96439/Filter_Table.png?format=1000w\" width=\"2062\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">What filter settings should you apply to your ERP data? If your filters are too weak to attenuate the noise in your data, your effects may not be statistically significant. If your filters are too strong, they may create artifactual peaks that lead you to draw bogus conclusions.</p><p class=\"\">For years, I have been recommending a bandpass of 0.1\u201330 Hz for most cognitive and affective research in neurotypical young adults. In this kind of research, I have found that filtering from 0.1\u201330 Hz usually does a good job of minimizing noise while creating minimal waveform distortion. </p><p class=\"\">However, this recommendation was based on a combination of informal observations from many experimental paradigms and a careful examination of a couple paradigms, so it was a bit hand-wavy. In addition, the optimal filter settings will depend on the waveshape of the ERP effects and the nature of the noise in a given study, so I couldn\u2019t make any specific recommendations about other experimental paradigms and participant populations. Moreover, different filter settings may be optimal for different scoring methods (e.g., mean amplitude vs. peak amplitude vs. peak latency).</p><p class=\"\">Guanghui Zhang, David Garrett, and I spent the last year focusing on this issue. First we developed a general method that can be used to determine the optimal filter settings for a given dataset and scoring method (see <a href=\"https://doi.org/10.1111/psyp.14531\"><span>this paper</span></a>). Then we applied this method to the <a href=\"https://doi.org/10.1016/j.neuroimage.2020.117465\"><span>ERP CORE</span></a> data to determine the optimal filter settings for the N170, MMN, P3b, N400, N2pc, LRP, and ERN components in neurotypical young adults (see <a href=\"https://doi.org/10.1111/psyp.14530\"><span>this paper</span></a> and the table above).</p><p class=\"\">If you are doing research with these components (or similar components) in neurotypical young adults, you can simply use the filter settings that we identified. If you are using a very different paradigm or testing a very different subject population, you can apply our method to your own data to find the optimal settings. We added some new tools to <a href=\"https://github.com/ucdavis/erplab\"><span>ERPLAB Toolbox</span></a> to make this easier.</p><p class=\"\">One thing that we discovered was that our old recommendation of 0.1\u201330 Hz does a good job of avoiding filter artifacts but is overly conservative for some components. For example, we can raise the low end to 0.5 Hz when measuring N2pc and MMN amplitudes, which gets rid of more noise without producing problematic waveform distortions. And we can go all the way up to 0.9 Hz for the N170 component. However, later/slower components like P3b and N400 require lower cutoffs (no higher than 0.2 Hz).</p><p class=\"\">You might be wondering how we defined the \u201coptimal\u201d filter settings. At one level, the answer is simple: The optimal filter is the one that maximizes the signal-to-noise ratio without producing too much waveform distortion. The complexities arise in quantifying the signal-to-noise ratio, quantifying the waveform distortion, and deciding how much waveform distortion is \u201ctoo much\u201d. We believe we have found reasonably straightforward and practical solutions to these problems, which you can read about in the published papers.</p>",
      "author": "Steve Luck",
      "published_date": "2024-02-04T23:46:20+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 568,
      "reading_time": 2,
      "created_at": "2025-11-02T05:41:56.732028+00:00",
      "updated_at": "2025-11-02T06:21:19.630580+00:00",
      "metadata": {
        "processed_at": "2025-11-02T06:21:19.630582+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f87e190d517408c7e19116ef6aed8544",
      "url": "https://brain.ieee.org/publications/neuroethics-framework/education/education-legal-issues/education-legal-issues/",
      "title": "Education: Legal Issues",
      "content": "The safety concerns and standards shared in other sections provide an initial foundation for legal protections. However, calls for stricter consumer protection laws must accompany the proliferation of neurotech devices. Special privacy laws must be promulgated to ensure \u201ccognitive privacy\u201d (Nita Farahany, 2012, 2023) [25]\u00a0 and educational autonomy. Raw brain data is uniquely sensitive, and an individual&#8217;s brain pattern may ...",
      "author": "Adriel Carridice",
      "published_date": "2025-02-05T15:33:45+00:00",
      "source": "Brain",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2025-11-02T05:41:54.274572+00:00",
      "updated_at": "2025-11-02T06:21:19.630584+00:00",
      "metadata": {
        "processed_at": "2025-11-02T06:21:19.630585+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}