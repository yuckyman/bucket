{
  "last_updated": "2025-11-13T06:23:54.384528+00:00",
  "pending_count": 920,
  "processed_count": 80,
  "pending_articles": [
    {
      "id": "3a3c933233b43615540f6cbd5e524f7c",
      "url": "https://arxiv.org/abs/2511.09309",
      "title": "TaskSense: Cognitive Chain Modeling and Difficulty Estimation for GUI Tasks",
      "content": "arXiv:2511.09309v1 Announce Type: new \nAbstract: Measuring GUI task difficulty is crucial for user behavior analysis and agent capability evaluation. Yet, existing benchmarks typically quantify difficulty based on motor actions (e.g., step counts), overlooking the cognitive demands underlying task completion. In this work, we propose Cognitive Chain, a novel framework that models task difficulty from a cognitive perspective. A cognitive chain decomposes the cognitive processes preceding a motor action into a sequence of cognitive steps (e.g., finding, deciding, computing), each with a difficulty index grounded in information theories. We develop an LLM-based method to automatically extract cognitive chains from task execution traces. Validation with linear regression shows that our estimated cognitive difficulty correlates well with user completion time (step-level R-square=0.46 after annotation). Assessment of state-of-the-art GUI agents shows reduced success on cognitively demanding tasks, revealing capability gaps and Human-AI consistency patterns. We conclude by discussing potential applications in agent training, capability assessment, and human-agent delegation optimization.",
      "author": "Yiwen Yin, Zhian Hu, Xiaoxi Xu, Chun Yu, Xintong Wu, Wenyu Fan, Yuanchun Shi",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415618+00:00",
      "updated_at": "2025-11-13T05:21:31.415620+00:00"
    },
    {
      "id": "6a97f3558b13ea1cce3dfee8ea6edf62",
      "url": "https://arxiv.org/abs/2511.09240",
      "title": "SimPath: Mitigating Motion Sickness in In - vehicle Infotainment Systems via Driving Condition Adaptation",
      "content": "arXiv:2511.09240v1 Announce Type: new \nAbstract: The problem of Motion Sickness (MS) among passengers significantly impacts the comfort and efficiency of In-Vehicle Infotainment Systems (IVIS) use. In this study, we innovatively designed SimPath, a visual design to effectively mitigate passengers' MS and boost their efficiency of using IVIS during driving. The study focuses on the problem of irregular motion conditions frequently encountered during actual driving. To validate the efficacy of this approach, two sets of real - vehicle experiments were carried out in real driving scenarios. The results demonstrate that this approach significantly reduces passenger's MS level to a certain extent. However, due to divided attention from visual content, it does not directly improve the IVIS efficiency. In conclusion, this study offers crucial insights for the design of a more intelligent and user friendly IVIS, based on the discussion of the principle, providing strong theoretical support and practical guidance for the development of future IVIS in autonomous vehicles.",
      "author": "Jinghao Huang, Siqi Yao, Yu Zhang",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 157,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415570+00:00",
      "updated_at": "2025-11-13T05:21:31.415571+00:00"
    },
    {
      "id": "6630f2746282a491867f2d346a3c4e41",
      "url": "https://arxiv.org/abs/2511.08971",
      "title": "Plug-and-Play Clarifier: A Zero-Shot Multimodal Framework for Egocentric Intent Disambiguation",
      "content": "arXiv:2511.08971v1 Announce Type: new \nAbstract: The performance of egocentric AI agents is fundamentally limited by multimodal intent ambiguity. This challenge arises from a combination of underspecified language, imperfect visual data, and deictic gestures, which frequently leads to task failure. Existing monolithic Vision-Language Models (VLMs) struggle to resolve these multimodal ambiguous inputs, often failing silently or hallucinating responses. To address these ambiguities, we introduce the Plug-and-Play Clarifier, a zero-shot and modular framework that decomposes the problem into discrete, solvable sub-tasks. Specifically, our framework consists of three synergistic modules: (1) a text clarifier that uses dialogue-driven reasoning to interactively disambiguate linguistic intent, (2) a vision clarifier that delivers real-time guidance feedback, instructing users to adjust their positioning for improved capture quality, and (3) a cross-modal clarifier with grounding mechanism that robustly interprets 3D pointing gestures and identifies the specific objects users are pointing to. Extensive experiments demonstrate that our framework improves the intent clarification performance of small language models (4--8B) by approximately 30%, making them competitive with significantly larger counterparts. We also observe consistent gains when applying our framework to these larger models. Furthermore, our vision clarifier increases corrective guidance accuracy by over 20%, and our cross-modal clarifier improves semantic answer accuracy for referential grounding by 5%. Overall, our method provides a plug-and-play framework that effectively resolves multimodal ambiguity and significantly enhances user experience in egocentric interaction.",
      "author": "Sicheng Yang, Yukai Huang, Weitong Cai, Shitong Sun, You He, Jiankang Deng, Hang Zhang, Jifei Song, Zhensong Zhang",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 225,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415541+00:00",
      "updated_at": "2025-11-13T05:21:31.415542+00:00"
    },
    {
      "id": "a479cfa86ba7f1308928bebbdfb53c0d",
      "url": "https://arxiv.org/abs/2511.08917",
      "title": "\"It's trained by non-disabled people\": Evaluating How Image Quality Affects Product Captioning with VLMs",
      "content": "arXiv:2511.08917v1 Announce Type: new \nAbstract: Vision-Language Models (VLMs) are increasingly used by blind and low-vision (BLV) people to identify and understand products in their everyday lives, such as food, personal products, and household goods. Despite their prevalence, we lack an empirical understanding of how common image quality issues, like blur and misframing of items, affect the accuracy of VLM-generated captions and whether resulting captions meet BLV people's information needs. Grounded in a survey with 86 BLV people, we systematically evaluate how image quality issues affect captions generated by VLMs. We show that the best model recognizes products in images with no quality issues with 98% accuracy, but drops to 75% accuracy overall when quality issues are present, worsening considerably as issues compound. We discuss the need for model evaluations that center on disabled people's experiences throughout the process and offer concrete recommendations for HCI and ML researchers to make VLMs more reliable for BLV people.",
      "author": "Kapil Garg, Xinru Tang, Jimin Heo, Dwayne R. Morgan, Darren Gergle, Erik B. Sudderth, Anne Marie Piper",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415507+00:00",
      "updated_at": "2025-11-13T05:21:31.415508+00:00"
    },
    {
      "id": "31da290567350f9eccfb454e72814e58",
      "url": "https://arxiv.org/abs/2511.08880",
      "title": "Simulating Psychological Risks in Human-AI Interactions: Real-Case Informed Modeling of AI-Induced Addiction, Anorexia, Depression, Homicide, Psychosis, and Suicide",
      "content": "arXiv:2511.08880v1 Announce Type: new \nAbstract: As AI systems become increasingly integrated into daily life, their potential to exacerbate or trigger severe psychological harms remains poorly understood and inadequately tested. This paper presents a proactive methodology for systematically exploring psychological risks in simulated human-AI interactions based on documented real-world cases involving AI-induced or AI-exacerbated addiction, anorexia, depression, homicide, psychosis, and suicide. We collected and analyzed 18 reported real-world cases where AI interactions contributed to severe psychological outcomes. From these cases, we developed a process to extract harmful interaction patterns and assess potential risks through 2,160 simulated scenarios using clinical staging models. We tested four major LLMs across multi-turn conversations to identify where psychological risks emerge: which harm domains, conversation stages, and contexts reveal system vulnerabilities. Through the analysis of 157,054 simulated conversation turns, we identify critical gaps in detecting psychological distress, responding appropriately to vulnerable users, and preventing harm escalation. Regression analysis reveals variability across persona types: LLMs tend to perform worse with elderly users but better with low- and middle-income groups compared to high-income groups. Clustering analysis of harmful responses reveals a taxonomy of fifteen distinct failure patterns organized into four categories of AI-enabled harm. This work contributes a novel methodology for identifying psychological risks, empirical evidence of common failure modes across systems, and a classification of harmful AI response patterns in high-stakes human-AI interactions.",
      "author": "Chayapatr Archiwaranguprok, Constanze Albrecht, Pattie Maes, Karrie Karahalios, Pat Pataranutaporn",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 225,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415476+00:00",
      "updated_at": "2025-11-13T05:21:31.415478+00:00"
    },
    {
      "id": "32eea95849d67baf2e5231d42766db20",
      "url": "https://arxiv.org/abs/2511.08763",
      "title": "Modeling multi-agent motion dynamics in immersive rooms",
      "content": "arXiv:2511.08763v1 Announce Type: new \nAbstract: Immersive rooms are increasingly popular augmented reality systems that support multi-agent interactions within a virtual world. However, despite extensive content creation and technological developments, insights about perceptually-driven social dynamics, such as the complex movement patterns during virtual world navigation, remain largely underexplored. Computational models of motion dynamics can help us understand the underlying mechanism of human interaction in immersive rooms and develop applications that better support spatially distributed interaction. In this work, we propose a new agent-based model of emergent human motion dynamics. The model represents human agents as simple spatial geometries in the room that relocate and reorient themselves based on the salient virtual spatial objects they approach. Agent motion is modeled as an interactive process combining external diffusion-driven influences from the environment with internal self-propelling interactions among agents. Further, we leverage simulation-based inference (SBI) to show that the governing parameters of motion patterns can be estimated from simple observables. Our results indicate that the model successfully captures action-related agent properties but exposes local non-identifiability linked to environmental awareness. We argue that our simulation-based approach paves the way for creating adaptive, responsive immersive rooms -- spaces that adjust their interfaces and interactions based on human collective movement patterns and spatial attention.",
      "author": "Mincong (Jerry),  Huang, Stefan T. Radev",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 207,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415435+00:00",
      "updated_at": "2025-11-13T05:21:31.415440+00:00"
    },
    {
      "id": "ff2a3486586f0a1b604755ed87b3e92c",
      "url": "https://arxiv.org/abs/2511.05221",
      "title": "ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy",
      "content": "arXiv:2511.05221v2 Announce Type: replace-cross \nAbstract: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major prodromal marker of $\\alpha$-synucleinopathies, often preceding the clinical onset of Parkinson's disease, dementia with Lewy bodies, or multiple system atrophy. While wrist-worn actimeters hold significant potential for detecting RBD in large-scale screening efforts by capturing abnormal nocturnal movements, they become inoperable without a reliable and efficient analysis pipeline. This study presents ActiTect, a fully automated, open-source machine learning tool to identify RBD from actigraphy recordings. To ensure generalizability across heterogeneous acquisition settings, our pipeline includes robust preprocessing and automated sleep-wake detection to harmonize multi-device data and extract physiologically interpretable motion features characterizing activity patterns. Model development was conducted on a cohort of 78 individuals, yielding strong discrimination under nested cross-validation (AUROC = 0.95). Generalization was confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To assess real-world robustness, leave-one-dataset-out cross-validation across the internal and external cohorts demonstrated consistent performance (AUROC range = 0.84-0.89). A complementary stability analysis showed that key predictive features remained reproducible across datasets, supporting the final pooled multi-center model as a robust pre-trained resource for broader deployment. By being open-source and easy to use, our tool promotes widespread adoption and facilitates independent validation and collaborative improvements, thereby advancing the field toward a unified and generalizable RBD detection model using wearable devices.",
      "author": "David Bertram, Anja Ophey, Sinah R\\\"ottgen, Konstantin Kufer, Gereon R. Fink, Elke Kalbe, Clint Hansen, Walter Maetzler, Maximilian Kapsecker, Lara M. Reimer, Stephan Jonas, Andreas T. Damgaard, Natasha B. Bertelsen, Casper Skjaerbaek, Per Borghammer, Karolien Groenewald, Pietro-Luca Ratti, Michele T. Hu, No\\'emie Moreau, Michael Sommerauer, Katarzyna Bozek",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 242,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:30.272053+00:00",
      "updated_at": "2025-11-13T05:21:30.272054+00:00"
    },
    {
      "id": "d0d363a51887b59f6c16199e82262e10",
      "url": "https://arxiv.org/abs/2507.01651",
      "title": "A Dynamical Cartography of the Epistemic Diffusion of Artificial Intelligence in Neuroscience",
      "content": "arXiv:2507.01651v2 Announce Type: replace-cross \nAbstract: Neuroscience and AI have an intertwined history, largely relayed in the literature of both fields. In recent years, due to the engineering orientations of AI research and the monopoly of industry for its large-scale applications, the mutual expansion of neuroscience and AI in fundamental research seems challenged. In this paper, we bring some empirical evidences that, on the contrary, AI and neuroscience are continuing to grow together, but with a pronounced interest in the fields of study related to neurodegenerative diseases since the 1990s. With a temporal knowledge cartography of neuroscience drawn with advanced document embedding techniques, we draw the dynamical shaping of the discipline since the 1970s and identified the conceptual articulation of AI with this particular subfield mentioned before. However, a further analysis of the underlying citation network of the studied corpus shows that the produced AI technologies remain confined in the different subfields and are not transferred from one subfield to another. This invites us to discuss the genericity capability of AI in the context of an intradisciplinary development, especially in the diffusion of its associated metrology.",
      "author": "Sylvain Fontaine",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 185,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:30.272018+00:00",
      "updated_at": "2025-11-13T05:21:30.272019+00:00"
    },
    {
      "id": "ec7f0f11cc7fc3c0888e2491a60fcc0e",
      "url": "https://arxiv.org/abs/2505.11477",
      "title": "Fractal geometry predicts dynamic differences in structural and functional connectomes",
      "content": "arXiv:2505.11477v2 Announce Type: replace-cross \nAbstract: Understanding the intricate architecture of brain networks and its connection to brain function is essential for deciphering the underlying principles of cognition and disease. While traditional graph-theoretical measures have been widely used to characterize these networks, they often fail to fully capture the emergent properties of large-scale neural dynamics. Here, we introduce an alternative approach to quantify brain networks that is rooted in complex dynamics, fractal geometry, and asymptotic analysis. We apply these concepts to brain connectomes and demonstrate how quadratic iterations and geometric properties of Mandelbrot-like sets can provide novel insights into structural and functional network dynamics. Our findings reveal fundamental distinctions between structural (positive) and functional (signed) connectomes, such as the shift of cusp orientation and the variability in equi-M set geometry. Notably, structural connectomes exhibit more robust, predictable features, while functional connectomes show increased variability for non-trivial tasks. We further demonstrate that traditional graph-theoretical measures, when applied separately to the positive and negative sub-networks of functional connectomes, fail to fully capture their dynamic complexity. Instead, size and shape-based invariants of the equi-M set effectively differentiate between rest and emotional task states, which highlights their potential as superior markers of emergent network dynamics. These results suggest that incorporating fractal-based methods into network neuroscience provides a powerful tool for understanding how information flows in natural systems beyond static connectivity measures, while maintaining their simplicity.",
      "author": "Anca Radulescu, Eva Kaslik, Alexandru Fikl, Johan Nakuci, Sarah Muldoon, Michael Anderson",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 230,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:30.271988+00:00",
      "updated_at": "2025-11-13T05:21:30.271989+00:00"
    },
    {
      "id": "8fc6fd3c65be9824116e2d9fb9f1d2dc",
      "url": "https://arxiv.org/abs/2509.02139",
      "title": "On sources to variabilities of simple cells in the primary visual cortex: A principled theory for the interaction between geometric image transformations and receptive field responses",
      "content": "arXiv:2509.02139v5 Announce Type: replace \nAbstract: This paper gives an overview of a theory for modelling the interaction between geometric image transformations and receptive field responses for a visual observer that views objects and spatio-temporal events in the environment. This treatment is developed over combinations of (i) uniform spatial scaling transformations, (ii) spatial affine transformations, (iii) Galilean transformations and (iv) temporal scaling transformations.\n  By postulating that the family of receptive fields should be covariant under these classes of geometric image transformations, it follows that the receptive field shapes should be expanded over the degrees of freedom of the corresponding image transformations, to enable a formal matching between the receptive field responses computed under different viewing conditions for the same scene or for a structurally similar spatio-temporal event.\n  We conclude the treatment by discussing and providing potential support for a working hypothesis that the receptive fields of simple cells in the primary visual cortex ought to be covariant under these classes of geometric image transformations, and thus have the shapes of their receptive fields expanded over the degrees of freedom of the corresponding geometric image transformations.",
      "author": "Tony Lindeberg",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 184,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:30.271953+00:00",
      "updated_at": "2025-11-13T05:21:30.271955+00:00"
    }
  ],
  "recently_processed": [
    {
      "id": "6730815f52be4bc9c4f8f362717fe86d",
      "url": "https://lwn.net/SubscriberLink/1042355/434ad706cc594276/",
      "title": "Mergiraf: Syntax-Aware Merging for Git",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45799664\">Comments</a>",
      "author": "",
      "published_date": "2025-11-03T14:54:54+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-13T05:43:26.795540+00:00",
      "updated_at": "2025-11-13T06:23:54.280166+00:00",
      "metadata": {
        "processed_at": "2025-11-13T06:23:54.280175+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3fce80778739ecdac94f0fb3d0579467",
      "url": "https://arxiv.org/abs/2511.09458",
      "title": "Exploring The Interaction-Outcome Paradox: Seemingly Richer and More Self-Aware Interactions with LLMs May Not Yet Lead to Better Learning",
      "content": "arXiv:2511.09458v1 Announce Type: new \nAbstract: While Large Language Models (LLMs) have transformed the user interface for learning, moving from keyword search to natural language dialogue, their impact on educational outcomes remains unclear. We present a controlled study (N=20) that directly compares the learning interaction and outcomes between LLM and search-based interfaces. We found that although LLMs elicit richer and nuanced interactions from a learner, they do not produce broadly better learning outcomes. In this paper, we explore this the ``Interaction-Outcome Paradox.'' To explain this, we discuss the concept of a cognitive shift: the locus of student effort moves from finding and synthesizing disparate sources (search) to a more self-aware identification and articulation of their knowledge gaps and strategies to bridge those gaps (LLMs). This insight provides a new lens for evaluating educational technologies, suggesting that the future of learning tools lies not in simply enriching interaction, but in designing systems that scaffold productive cognitive work by leveraging this student expressiveness.",
      "author": "Rahul R. Divekar, Sophia Guerra, Lisette Gonzalez, Natasha Boos",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 160,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415743+00:00",
      "updated_at": "2025-11-13T06:23:54.280179+00:00",
      "metadata": {
        "processed_at": "2025-11-13T06:23:54.280181+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "4bce2618116a74e3de51cc473b4aec42",
      "url": "https://arxiv.org/abs/2511.09454",
      "title": "Algorithmic Advice as a Strategic Signal on Competitive Markets",
      "content": "arXiv:2511.09454v1 Announce Type: new \nAbstract: As algorithms increasingly mediate competitive decision-making, their influence extends beyond individual outcomes to shaping strategic market dynamics. In two preregistered experiments, we examined how algorithmic advice affects human behavior in classic economic games with unique, non-collusive, and analytically traceable equilibria. In Experiment 1 (N = 107), participants played a Bertrand price competition with individualized or collective algorithmic recommendations. Initially, collusively upward-biased advice increased prices, particularly when individualized, but prices gradually converged toward equilibrium over the course of the experiment. However, participants avoided setting prices above the algorithm's recommendation throughout the experiment, suggesting that advice served as a soft upper bound for acceptable prices. In Experiment 2 (N = 129), participants played a Cournot quantity competition with equilibrium-aligned or strategically biased algorithmic recommendations. Here, individualized equilibrium advice supported stable convergence, whereas collusively downward-biased advice led to sustained underproduction and supracompetitive profits - hallmarks of tacit collusion. In both experiments, participants responded more strongly and consistently to individualized advice than collective advice, potentially due to greater perceived ownership of the former. These findings demonstrate that algorithmic advice can function as a strategic signal, shaping coordination even without explicit communication. The results echo real-world concerns about algorithmic collusion and underscore the need for careful design and oversight of algorithmic decision-support systems in competitive environments.",
      "author": "Tobias R. Rebholz, Maxwell Uphoff, Christian H. R. Bernges, Florian Scholten",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 216,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415714+00:00",
      "updated_at": "2025-11-13T06:23:54.280183+00:00",
      "metadata": {
        "processed_at": "2025-11-13T06:23:54.280185+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "0200829fccc1f03b9ea419a10b3c4c63",
      "url": "https://arxiv.org/abs/2511.09394",
      "title": "A multimodal AI agent for clinical decision support in ophthalmology",
      "content": "arXiv:2511.09394v1 Announce Type: new \nAbstract: Artificial intelligence has shown promise in medical imaging, yet most existing systems lack flexibility, interpretability, and adaptability - challenges especially pronounced in ophthalmology, where diverse imaging modalities are essential. We present EyeAgent, the first agentic AI framework for comprehensive and interpretable clinical decision support in ophthalmology. Using a large language model (DeepSeek-V3) as its central reasoning engine, EyeAgent interprets user queries and dynamically orchestrates 53 validated ophthalmic tools across 23 imaging modalities for diverse tasks including classification, segmentation, detection, image/report generation, and quantitative analysis. Stepwise ablation analysis demonstrated a progressive improvement in diagnostic accuracy, rising from a baseline of 69.71% (using only 5 general tools) to 80.79% when the full suite of 53 specialized tools was integrated. In an expert rating study on 200 real-world clinical cases, EyeAgent achieved 93.7% tool selection accuracy and received expert ratings of more than 88% across accuracy, completeness, safety, reasoning, and interpretability. In human-AI collaboration, EyeAgent matched or exceeded the performance of senior ophthalmologists and, when used as an assistant, improved overall diagnostic accuracy by 18.51% and report quality scores by 19%, with the greatest benefit observed among junior ophthalmologists. These findings establish EyeAgent as a scalable and trustworthy AI framework for ophthalmology and provide a blueprint for modular, multimodal, and clinically aligned next-generation AI systems.",
      "author": "Danli Shi, Xiaolan Chen, Bingjie Yan, Weiyi Zhang, Pusheng Xu, Jiancheng Yang, Ruoyu Chen, Siyu Huang, Bowen Liu, Xinyuan Wu, Meng Xie, Ziyu Gao, Yue Wu, Senlin Lin, Kai Jin, Xia Gong, Yih Chung Tham, Xiujuan Zhang, Li Dong, Yuzhou Zhang, Jason Yam, Guangming Jin, Xiaohu Ding, Haidong Zou, Yalin Zheng, Zongyuan Ge, Mingguang He",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415681+00:00",
      "updated_at": "2025-11-13T06:23:54.280187+00:00",
      "metadata": {
        "processed_at": "2025-11-13T06:23:54.280189+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "dda0b3085f939e5168fb664aafffea2a",
      "url": "https://arxiv.org/abs/2511.09337",
      "title": "TempoQL: A Readable, Precise, and Portable Query System for Electronic Health Record Data",
      "content": "arXiv:2511.09337v1 Announce Type: new \nAbstract: Electronic health record (EHR) data is an essential data source for machine learning for health, but researchers and clinicians face steep barriers in extracting and validating EHR data for modeling. Existing tools incur trade-offs between expressivity and usability and are typically specialized to a single data standard, making it difficult to write temporal queries that are ready for modern model-building pipelines and adaptable to new datasets. This paper introduces TempoQL, a Python-based toolkit designed to lower these barriers. TempoQL provides a simple, human-readable language for temporal queries; support for multiple EHR data standards, including OMOP, MEDS, and others; and an interactive notebook-based query interface with optional large language model (LLM) authoring assistance. Through a performance evaluation and two use cases on different datasets, we demonstrate that TempoQL simplifies the creation of cohorts for machine learning while maintaining precision, speed, and reproducibility.",
      "author": "Ziyong Ma, Richard D. Boyce, Adam Perer, Venkatesh Sivaraman",
      "published_date": "2025-11-13T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2025-11-13T05:21:31.415647+00:00",
      "updated_at": "2025-11-13T06:23:54.280191+00:00",
      "metadata": {
        "processed_at": "2025-11-13T06:23:54.280193+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "62fbc07721d3e40326fde5aaed0df13c",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306452225010395?dgcid=rss_sd_all",
      "title": "Seasonal changes in the balance of brain monoamines of hibernating long-tailed ground squirrels (<em>Urocitellus undulatus</em>)",
      "content": "<p>Publication date: 5 December 2025</p><p><b>Source:</b> Neuroscience, Volume 590</p><p>Author(s): Nadezhda M. Zakharova, Yury S. Tarahovsky</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 14,
      "reading_time": 1,
      "created_at": "2025-11-13T03:59:30.835939+00:00",
      "updated_at": "2025-11-13T04:21:25.303766+00:00",
      "metadata": {
        "processed_at": "2025-11-13T04:21:25.303776+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "ff9aefe17ebe2dd9814289080a9642ac",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306452225010498?dgcid=rss_sd_all",
      "title": "Impact of routine rehabilitation training on motor function and activities of daily living in oldest-old patients who have experienced a stroke",
      "content": "<p>Publication date: 5 December 2025</p><p><b>Source:</b> Neuroscience, Volume 590</p><p>Author(s): Yu-Juan Han, Hao-Ming Xu, Shan Han, Pei Dai, Xiao-Ping Kang</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 18,
      "reading_time": 1,
      "created_at": "2025-11-13T03:59:30.835921+00:00",
      "updated_at": "2025-11-13T04:21:25.303780+00:00",
      "metadata": {
        "processed_at": "2025-11-13T04:21:25.303782+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "34d15c19b23cf29f13ca20d5a14f2665",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306452225010590?dgcid=rss_sd_all",
      "title": "Suppression of AKAP150 palmitoylation alleviates seizures in kainic acid-induced epilepsy mice",
      "content": "<p>Publication date: 5 December 2025</p><p><b>Source:</b> Neuroscience, Volume 590</p><p>Author(s): Chen-Chao Chu, Ya-Hui Hu, Hai-Feng Zhang, Gui-Zhou Li, Shi-Yu Wu, Yan-Yu Zang, Jiang Chen, Hao-Yu Wang, Yang-Yang Xu, Hong-Li Guo, Yun Stone Shi, Feng Chen</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 33,
      "reading_time": 1,
      "created_at": "2025-11-13T03:59:30.835903+00:00",
      "updated_at": "2025-11-13T04:21:25.303784+00:00",
      "metadata": {
        "processed_at": "2025-11-13T04:21:25.303786+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f3ee81748b72f618d907f5bb08738130",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306452225010310?dgcid=rss_sd_all",
      "title": "Mental health and subjective well-being of trans and non-binary population in Colombia",
      "content": "<p>Publication date: 5 December 2025</p><p><b>Source:</b> Neuroscience, Volume 590</p><p>Author(s): Mar\u00eda Fernanda Reyes, Natalie Levy, Daniela Maldonado Salamanca, Minna Lyons, Juan-David Leong\u00f3mez</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 20,
      "reading_time": 1,
      "created_at": "2025-11-13T03:59:30.835828+00:00",
      "updated_at": "2025-11-13T04:21:25.303788+00:00",
      "metadata": {
        "processed_at": "2025-11-13T04:21:25.303790+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "5f2feef4ea2382f4ed7b096aedbe2db3",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005464?dgcid=rss_sd_all",
      "title": "The golden age of online readout: EEG-informed TMS from manual probing to closed-loop neuromodulation",
      "content": "<p>Publication date: 15 November 2025</p><p><b>Source:</b> NeuroImage, Volume 322</p><p>Author(s): Giuseppe Varone, Mana Biabani, Sara Tremblay, Joshua C. Brown, Elisa Kallioniemi, Nigel C. Rogasch</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 22,
      "reading_time": 1,
      "created_at": "2025-11-13T03:59:28.585173+00:00",
      "updated_at": "2025-11-13T04:21:25.303792+00:00",
      "metadata": {
        "processed_at": "2025-11-13T04:21:25.303793+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}