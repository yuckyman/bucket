{
  "last_updated": "2025-12-03T06:24:48.621789+00:00",
  "count": 20,
  "articles": [
    {
      "id": "fdc75d13e9c7e260b8387c574c6a2c5a",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306452225011248?dgcid=rss_sd_all",
      "title": "Modified constraint-induced movement therapy combined with intermittent theta-burst stimulation promotes functional recovery in MCAO rats by enhancing synaptic plasticity in the ipsilateral hippocampal CA1",
      "content": "<p>Publication date: 9 January 2026</p><p><b>Source:</b> Neuroscience, Volume 592</p><p>Author(s): Tiantian Jia, Jian Hu, Yuyuan Wang, Congqin Li, Yan Hua, Yulong Bai</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 20,
      "reading_time": 1,
      "created_at": "2025-12-03T05:44:44.360799+00:00",
      "updated_at": "2025-12-03T06:24:48.517024+00:00",
      "metadata": {
        "processed_at": "2025-12-03T06:24:48.517033+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "4c9bf85b6fee62899fef0d48c374559d",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925006019?dgcid=rss_sd_all",
      "title": "Resolution generalization of deep learning-based dipole inversion networks for QSM",
      "content": "<p>Publication date: 15 December 2025</p><p><b>Source:</b> NeuroImage, Volume 324</p><p>Author(s): Sooyeon Ji, Minjun Kim, Jongho Lee, Hyeong-Geol Shin</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 16,
      "reading_time": 1,
      "created_at": "2025-12-03T05:44:42.172347+00:00",
      "updated_at": "2025-12-03T06:24:48.517037+00:00",
      "metadata": {
        "processed_at": "2025-12-03T06:24:48.517039+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "10650446bd00d6310435d76e0596cd68",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925006184?dgcid=rss_sd_all",
      "title": "Expertise-related functional connectivity changes in Chinese calligraphy linked to flow experience",
      "content": "<p>Publication date: 15 December 2025</p><p><b>Source:</b> NeuroImage, Volume 324</p><p>Author(s): Qingyan Kong, Yue Wang, Min Li, Buxin Han, Rui Li</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 18,
      "reading_time": 1,
      "created_at": "2025-12-03T05:44:42.172328+00:00",
      "updated_at": "2025-12-03T06:24:48.517042+00:00",
      "metadata": {
        "processed_at": "2025-12-03T06:24:48.517043+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "9d414f3b56ac788715b1f6d8b57a6e78",
      "url": "https://fmhy.net/posts/Nov-2025",
      "title": "Monthly Updates [November]",
      "content": "<div class=\"info custom-block\"><p class=\"custom-block-title\">INFO</p>\n<p>These update threads only contains major updates. If you're interested\nin seeing all minor changes you can follow our\n<a href=\"https://github.com/fmhy/FMHYedit/commits/main\" rel=\"noreferrer\" target=\"_blank\">Commits Page</a> on GitHub or\n<a href=\"https://redd.it/17f8msf\" rel=\"noreferrer\" target=\"_blank\">Updates Channel</a> in Discord.</p>\n</div>\n<h1 id=\"wiki-updates\" tabindex=\"-1\">Wiki Updates <a class=\"header-anchor\" href=\"#wiki-updates\"></a></h1>\n<ul>\n<li>\n<p>Added the <strong><a href=\"https://i.ibb.co/hx97zL3W/978676.jpg\" rel=\"noreferrer\" target=\"_blank\">Amoled Theme</a></strong> / <a href=\"https://i.imgur.com/fMrnGmF.png\" rel=\"noreferrer\" target=\"_blank\">2</a> to our site as a <a href=\"https://i.ibb.co/pvkfg3hC/image.png\" rel=\"noreferrer\" target=\"_blank\">Toggle</a> / <a href=\"https://i.imgur.com/qF7exKw.png\" rel=\"noreferrer\" target=\"_blank\">2</a> that can be turned on or off. Thank you to @Land for doing this.</p>\n</li>\n<li>\n<p>Built a <strong><a href=\"https://fmhy-search.dev.zenir.tech/\" rel=\"noreferrer\" target=\"_blank\">External Search Engine</a></strong> that should work better in most cases than the built in VitePress search on our website.</p>\n</li>\n<li>\n<p>Added a <strong><a href=\"https://fmhyclone.pages.dev/\" rel=\"noreferrer\" target=\"_blank\">New Backup</a></strong> of FMHY with daily sync, hosted on GitLab. It also has a backup of the <a href=\"https://fmhyapi.wispy.qzz.io/single-page\" rel=\"noreferrer\" target=\"_blank\">raw markdown</a> page. We also added another <a href=\"https://a-fmhy.pages.dev/\" rel=\"noreferrer\" target=\"_blank\">backup</a> of our website that has the theme on from above on automatically.</p>\n</li>\n<li>\n<p>The Space section has become a bit hard to navigate (60+ lines,) so we've split it into its <a href=\"https://fmhy.net/educational#space\" rel=\"noreferrer\" target=\"_blank\">own head section</a>, with 2 new subsections: <a href=\"https://fmhy.net/educational#astronomy\" rel=\"noreferrer\" target=\"_blank\">Astronomy</a> and <a href=\"https://fmhy.net/educational#spacecraft\" rel=\"noreferrer\" target=\"_blank\">Spacecraft</a>. Astronomy will cover things related to celestial objects or phenomena in the cosmos. Spacecraft will cover things like rockets, launches and the ISS. The head section will be for more general space related things, like NASAs website, news, etc.</p>\n</li>\n<li>\n<p>Remakes / Ports in gaming had over 80 lines, and was pretty disorganized, so we've split it into 3 new sections to make it more comprehensible: <a href=\"https://fmhy.net/gaming#decomps-ports\" rel=\"noreferrer\" target=\"_blank\">Decomps / Ports</a>, <a href=\"https://fmhy.net/gaming#remakes-recreations\" rel=\"noreferrer\" target=\"_blank\">Remakes / Recreations</a> and <a href=\"https://fmhy.net/gaming#revival-projects\" rel=\"noreferrer\" target=\"_blank\">Revival Projects</a>. Also turned Special Interest into its own head category to help organize TOC better.</p>\n</li>\n<li>\n<p>Re-ordered <a href=\"https://fmhy.net/reading#manga\" rel=\"noreferrer\" target=\"_blank\">Manga Sites</a> based on poll results from our Discord. Weeb Central has been moved to #1 spot, and MangaFire + MangaNato have both been starred. Thank you to everyone who took part in voting and gave their thoughts. <a href=\"https://i.ibb.co/j9Sn4hRR/image.png\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/u8zFZTX.png\" rel=\"noreferrer\" target=\"_blank\">2</a>.</p>\n</li>\n<li>\n<p>Cleaned up multiple Audio Streaming sections, and added new ones to help better organize it including: <a href=\"https://fmhy.net/audio#specialty-streaming\" rel=\"noreferrer\" target=\"_blank\">Specialty</a>, <a href=\"https://fmhy.net/audio#genre-specific-streaming\" rel=\"noreferrer\" target=\"_blank\">Genre Specific</a>, <a href=\"https://fmhy.net/audio#radio-directories\" rel=\"noreferrer\" target=\"_blank\">Radio Directories</a>, and <a href=\"https://fmhy.net/audio#lofi-radio\" rel=\"noreferrer\" target=\"_blank\">Lofi Radio</a>. <a href=\"https://github.com/fmhy/edit/pull/4128#issuecomment-3476036920\" rel=\"noreferrer\" target=\"_blank\">Before vs. After</a>. Thank you to @AnarchyDR for doing this.</p>\n</li>\n<li>\n<p>Updated <a href=\"https://fmhy.net/audio\" rel=\"noreferrer\" target=\"_blank\">Audio Streaming</a> table of contents to make it less cluttered and easier to navigate. <a href=\"https://i.ibb.co/0yJbh03H/234243.jpg\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/fhgqKzb.png\" rel=\"noreferrer\" target=\"_blank\">2</a>.</p>\n</li>\n<li>\n<p>Cleaned up <a href=\"https://fmhy.net/gaming#browser-emulators\" rel=\"noreferrer\" target=\"_blank\">Browser Emulators</a>, fixed labels, removed dead sites, bumped sites with multiple, or unique emulators higher, and moved any only serving as EmulatorJS / NeptunJS frontends to storage. <a href=\"https://i.ibb.co/LXdhcDFD/Untitled.png\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/W9x8jY4.png\" rel=\"noreferrer\" target=\"_blank\">2</a>.</p>\n</li>\n<li>\n<p>We removed the &quot;No Torrenting&quot; label from ProtonVPN as you can set it up with a OpenVPN config that allows you to torrent for free. Note that they do expire and must be regenerated sometimes. Guide is listed next to <a href=\"https://fmhy.net/privacy#vpn\" rel=\"noreferrer\" target=\"_blank\">Proton</a>. TY to Wispy and others for figuring this out.</p>\n</li>\n<li>\n<p>Added <a href=\"https://fmhy.net/gaming#tetris\" rel=\"noreferrer\" target=\"_blank\">Tetris</a> Section to Gaming.</p>\n</li>\n<li>\n<p>Fixed ugly formatting in Game Optimization. <a href=\"https://i.ibb.co/Vc99kJhh/image.png\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/HRPUwL3.png\" rel=\"noreferrer\" target=\"_blank\">2</a>.</p>\n</li>\n</ul>\n<hr />\n<h1 id=\"stars-added-\u2b50\" tabindex=\"-1\">Stars Added \u2b50 <a class=\"header-anchor\" href=\"#stars-added-\u2b50\"></a></h1>\n<ul>\n<li>\n<p>Starred <a href=\"https://fmhy.net/file-tools#download-managers\" rel=\"noreferrer\" target=\"_blank\">AB Download Manager</a> in Download Managers. Open source, fast, cross platform, has resumable downloads, and a active dev team.</p>\n</li>\n<li>\n<p>Bumped <a href=\"https://fmhy.net/gaming#rom-sites\" rel=\"noreferrer\" target=\"_blank\">CrocDB</a> to new #1 over Myrient in ROM sites. Croc covers multiple sites (including myrient), has a better UI, and a new rompack feature.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video#torrent-apps\" rel=\"noreferrer\" target=\"_blank\">PlayTorrio</a> in Torrent Streaming Apps. New client that is very feature rich. It has similar addons to Stremio, such as Torrentio. It also has Jackett integration, Debrid support, and so many other features we're unable to list them all here.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/ai#specialized-chatbots\" rel=\"noreferrer\" target=\"_blank\">NotebookLM</a> in Specialized Chatbots. Document chatbots + note taking. Works well for quickly studying, has good audio/video overviews, can generate quizzes, flashcards, etc.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video#anime-streaming\" rel=\"noreferrer\" target=\"_blank\">Anidap</a> in Anime Streaming. Has a nice UI, uses hosts that are both unique and fast.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video#live-tv\" rel=\"noreferrer\" target=\"_blank\">PlayTorrio IPTV</a> in Live TV / Sports. Fast streams, huge library, good quality, one of the better Live TV sites we've been sent in awhile. Note that Darkness TV is the original, PlayTorrio just improved on their UI.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/linux-macos#linux-gaming\" rel=\"noreferrer\" target=\"_blank\">Heroic Games Launcher</a> in Linux Gaming. Linux game launcher for Epic, GOG, and prime games, better maintained than Lutris now.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/reading#curated-recommendations\" rel=\"noreferrer\" target=\"_blank\">ComicBookRoundup</a> in Reading Recommendations. Comic focused review / rating aggregator, similar to Metacritic or Rotten Tomatoes.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/gaming#rom-sites\" rel=\"noreferrer\" target=\"_blank\">Ziperto</a> in ROM Sites. Been around for a long time now, has a solid library, and uses fast hosts.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video#live-sports\" rel=\"noreferrer\" target=\"_blank\">Sportsbite</a> in live sports. Aggregator with lots of hosts, nice UI, covers most big events.</p>\n</li>\n</ul>\n<hr />\n<h1 id=\"things-removed\" tabindex=\"-1\">Things Removed <a class=\"header-anchor\" href=\"#things-removed\"></a></h1>\n<ul>\n<li>\n<p>Removed DramaGo as they seem to have shut down.</p>\n</li>\n<li>\n<p>Unstarred Character.AI as they had to <a href=\"https://deadline.com/2025/09/disney-cease-and-desist-letter-characterai-copyright-infringement-1236566831\" rel=\"noreferrer\" target=\"_blank\">remove a bunch of characters</a> recently due to copyright, and people were already unhappy with their limits / restrictions.</p>\n</li>\n</ul>",
      "author": "",
      "published_date": "2025-11-01T00:00:00+00:00",
      "source": "Fmhy",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 924,
      "reading_time": 4,
      "created_at": "2025-12-03T05:44:02.146986+00:00",
      "updated_at": "2025-12-03T06:24:48.517046+00:00",
      "metadata": {
        "processed_at": "2025-12-03T06:24:48.517047+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "bf00ce07992ad1768ae8f3571d82eaf9",
      "url": "https://arxiv.org/abs/2512.02848",
      "title": "Humans incorrectly reject confident accusatory AI judgments",
      "content": "arXiv:2512.02848v1 Announce Type: new \nAbstract: Automated verbal deception detection using methods from Artificial Intelligence (AI) has been shown to outperform humans in disentangling lies from truths. Research suggests that transparency and interpretability of computational methods tend to increase human acceptance of using AI to support decisions. However, the extent to which humans accept AI judgments for deception detection remains unclear. We experimentally examined how an AI model's accuracy (i.e., its overall performance in deception detection) and confidence (i.e., the model's uncertainty in single-statements predictions) influence human adoption of the model's judgments. Participants (n=373) were presented with veracity judgments of an AI model with high or low overall accuracy and various degrees of prediction confidence. The results showed that humans followed predictions from a highly accurate model more than from a less accurate one. Interestingly, the more confident the model, the more people deviated from it, especially if the model predicted deception. We also found that human interaction with algorithmic predictions either worsened the machine's performance or was ineffective. While this human aversion to accept highly confident algorithmic predictions was partly explained by participants' tendency to overestimate humans' deception detection abilities, we also discuss how truth-default theory and the social costs of accusing someone of lying help explain the findings.",
      "author": "Riccardo Loconte, Merylin Monaro, Pietro Pietrini, Bruno Verschuere, Bennett Kleinberg",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 209,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939849+00:00",
      "updated_at": "2025-12-03T06:24:48.517049+00:00",
      "metadata": {
        "processed_at": "2025-12-03T06:24:48.517051+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "094f3a35ae7c2b9c5e80b5253fc99489",
      "url": "https://arxiv.org/abs/2512.02785",
      "title": "Perception of AI-Generated Music - The Role of Composer Identity, Personality Traits, Music Preferences, and Perceived Humanness",
      "content": "arXiv:2512.02785v1 Announce Type: new \nAbstract: The rapid rise of AI-generated art has sparked debate about potential biases in how audiences perceive and evaluate such works. This study investigates how composer information and listener characteristics shape the perception of AI-generated music, adopting a mixed-method approach. Using a diverse set of stimuli across various genres from two AI music models, we examine effects of perceived authorship on liking and emotional responses, and explore how attitudes toward AI, personality traits, and music-related variables influence evaluations. We further assess the influence of perceived humanness and analyze open-ended responses to uncover listener criteria for judging AI-generated music. Attitudes toward AI proved to be the best predictor of both liking and emotional intensity of AI-generated music. This quantitative finding was complemented by qualitative themes from our thematic analysis, which identified ethical, cultural, and contextual considerations as important criteria in listeners' evaluations of AI-generated music. Our results offer a nuanced view of how people experience music created by AI tools and point to key factors and methodological considerations for future research on music perception in human-AI interaction.",
      "author": "David Stammer, Hannah Strauss, Peter Knees",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 180,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939816+00:00",
      "updated_at": "2025-12-03T05:21:26.939817+00:00"
    },
    {
      "id": "7c52ef81c525030841b3c2067a2e9851",
      "url": "https://arxiv.org/abs/2512.02651",
      "title": "Real-Time Multimodal Data Collection Using Smartwatches and Its Visualization in Education",
      "content": "arXiv:2512.02651v1 Announce Type: new \nAbstract: Wearable sensors, such as smartwatches, have become increasingly prevalent across domains like healthcare, sports, and education, enabling continuous monitoring of physiological and behavioral data. In the context of education, these technologies offer new opportunities to study cognitive and affective processes such as engagement, attention, and performance. However, the lack of scalable, synchronized, and high-resolution tools for multimodal data acquisition continues to be a significant barrier to the widespread adoption of Multimodal Learning Analytics in real-world educational settings. This paper presents two complementary tools developed to address these challenges: Watch-DMLT, a data acquisition application for Fitbit Sense 2 smartwatches that enables real-time, multi-user monitoring of physiological and motion signals; and ViSeDOPS, a dashboard-based visualization system for analyzing synchronized multimodal data collected during oral presentations. We report on a classroom deployment involving 65 students and up to 16 smartwatches, where data streams including heart rate, motion, gaze, video, and contextual annotations were captured and analyzed. Results demonstrate the feasibility and utility of the proposed system for supporting fine-grained, scalable, and interpretable Multimodal Learning Analytics in real learning environments.",
      "author": "Alvaro Becerra, Pablo Villegas, Ruth Cobos",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 181,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939785+00:00",
      "updated_at": "2025-12-03T05:21:26.939787+00:00"
    },
    {
      "id": "69153ed6c7efd43a8b83555f901bb3ac",
      "url": "https://arxiv.org/abs/2512.02608",
      "title": "Investigating the Integrated Digital Interventions Delivered by a Therapeutic Companion Agent for Young Adults with Symptoms of Depression: A Proof-of-Concept Study",
      "content": "arXiv:2512.02608v1 Announce Type: new \nAbstract: Background: Despite the clinical effectiveness of digital interventions for young adults with depression, low engagement and adherence remain persistent challenges. Building a strong digital therapeutic alliance has been proposed to address these barriers. This study highlights the need for a conversational therapeutic companion agent (TCA)-based intervention design. Objective: This study aimed to develop a Wizard-of-Oz TCA-centered prototype integrating social-support-based ecological momentary assessment (EMA), ecological momentary intervention (EMI), behavioral activation, and gamification. We evaluated the six-week proof-of-concept efficacy of this intervention among young adults with depressive symptoms. Methods: Korean young adults aged 20--39 years with mild-to-moderate depressive symptoms (PHQ-9) were recruited online. The intervention group ($n = 29$) received a six-week TCA-based digital intervention, while the control group ($n = 29$), recruited four weeks later, continued their usual routines. The TCA guided four daily behavioral-activation tasks, three mood assessments, meditation, daily summaries, and weekly mission feedback. Both groups were assessed at baseline and at weeks 2, 4, and 6 using the BDI-II, GAD-7, and Q-LES-Q-SF. Results: Of 58 participants, 57 completed the study (one dropout in the intervention group). At week 6, the intervention group showed significantly greater reductions in depressive symptoms and improvements in quality of life than controls. Adherence was 78\\% for EMA, 51\\% for EMI, and 65\\% for daily routines. Conclusions: The TCA-based digital intervention improved depressive symptoms and quality of life with adherence levels comparable to previous digital health interventions. Future studies should refine the TCA design and conduct larger-scale evaluations.",
      "author": "Youngjae Yoo, Minuk Kim, Soyoung Kim, Gayeon Lee, Jinwoo Kim",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 249,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939754+00:00",
      "updated_at": "2025-12-03T05:21:26.939756+00:00"
    },
    {
      "id": "97485487864ba36937423ef5d33f9b3d",
      "url": "https://arxiv.org/abs/2512.02569",
      "title": "Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models",
      "content": "arXiv:2512.02569v1 Announce Type: new \nAbstract: This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.",
      "author": "Yuchong Zhang, Yong Ma, Danica Kragic",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 204,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939718+00:00",
      "updated_at": "2025-12-03T05:21:26.939720+00:00"
    },
    {
      "id": "7a9f5e12c2c79e568213fc85e5770a63",
      "url": "https://arxiv.org/abs/2512.02442",
      "title": "A Visual Analytics System to Understand Behaviors of Multi Agents in Reinforcement Learning",
      "content": "arXiv:2512.02442v1 Announce Type: new \nAbstract: Multi-Agent Reinforcement Learning (MARL) is a branch of machine learning in which agents interact and learn optimal policies through trial and error, addressing complex scenarios where multiple agents interact and learn in the same environment at the same time. Analyzing and understanding these complex interactions is challenging, and existing analysis methods are limited in their ability to fully reflect and interpret this complexity. To address these challenges, we provide MARLViz, a visual analytics system for visualizing and analyzing the policies and interactions of agents in MARL environments. The system is designed to visually show the difference in behavior of agents under different environment settings and help users understand complex interaction patterns. In this study, we analyzed agents with similar behaviors and selected scenarios to understand the interactions of the agents, which made it easier to understand the strategies of agents in MARL.",
      "author": "Changhee Lee, Jeongmin Rhee, DongHwa Shin",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 147,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939686+00:00",
      "updated_at": "2025-12-03T05:21:26.939688+00:00"
    },
    {
      "id": "4c7a96734bec7aba10590231b09fb46e",
      "url": "https://arxiv.org/abs/2512.02288",
      "title": "Artographer: a Curatorial Interface for Art Space Exploration",
      "content": "arXiv:2512.02288v1 Announce Type: new \nAbstract: Relating a piece to previously established works is crucial in creating and engaging with art, but AI interfaces tend to obscure such relationships, rather than helping users explore them. Embedding models present new opportunities to support discovering and relating artwork through spatial interaction. We built Artographer, an art exploration system featuring a zoomable 2-D map, constructed from the similarity-clustered embeddings of 15,000+ historical artworks. Using Artographer as a probe to investigate spatial artwork exploration, we analyzed how 20 participants (including 9 art history scholars) traversed the map, during a goal-driven task and when freely exploring. We observe divergent and convergent exploration behaviors (Jumping, Wandering, Fixation, Revisiting) and identify values enacted by spatial art-finding (Visibility, Agency, Serendipity, Friction.) We situate spatial maps within a space of Curatorial Interfaces, systems that select and present artworks, and discuss centering pluralism and agency in the design of more responsible AI systems for art curation.",
      "author": "Shm Garanganao Almeda, John Joon Young Chung, Bjoern Hartmann, Sophia Liu, Brett Halperin, Yuwen Lu, Max Kreminski",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939658+00:00",
      "updated_at": "2025-12-03T05:21:26.939660+00:00"
    },
    {
      "id": "142a93fa1e9d9df0071c499bc9b703b2",
      "url": "https://arxiv.org/abs/2512.02275",
      "title": "Understanding Down Syndrome Stereotypes in LLM-Based Personas",
      "content": "arXiv:2512.02275v1 Announce Type: new \nAbstract: We present a case study of Persona-L, a system that leverages large language models (LLMs) and retrieval-augmented generation (RAG) to model personas of people with Down syndrome. Existing approaches to persona creation can often lead to oversimplified or stereotypical profiles of people with Down Syndrome. To that end, we built stereotype detection capabilities into Persona-L. Through interviews with caregivers and healthcare professionals (N=10), we examine how Down Syndrome stereotypes could manifest in both, content and delivery of LLMs, and interface design. Our findings show the challenges in stereotypes definition, and reveal the potential stereotype emergence from the training data, interface design, and the tone of LLM output. This highlights the need for participatory methods that capture the heterogeneity of lived experiences of people with Down Syndrome.",
      "author": "Chantelle Wu, Peinan Wang, Nafi Nibras, Meida Li, Dajun Yuan, Zhixiao Wang, Jiahuan He, Mona Ali, Mirjana Prpa",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 131,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939624+00:00",
      "updated_at": "2025-12-03T05:21:26.939626+00:00"
    },
    {
      "id": "a0fea8162a5e1a31c32c09bc157db76e",
      "url": "https://arxiv.org/abs/2512.02263",
      "title": "DepthScape: Authoring 2.5D Designs via Depth Estimation, Semantic Understanding, and Geometry Extraction",
      "content": "arXiv:2512.02263v1 Announce Type: new \nAbstract: 2.5D effects, such as occlusion and perspective foreshortening, enhance visual dynamics and realism by incorporating 3D depth cues into 2D designs. However, creating such effects remains challenging and labor-intensive due to the complexity of depth perception. We introduce DepthScape, a human-AI collaborative system that facilitates 2.5D effect creation by directly placing design elements into 3D reconstructions. Using monocular depth reconstruction, DepthScape transforms images into 3D reconstructions where visual contents are placed to automatically achieve realistic occlusion and perspective foreshortening. To further simplify 3D placement through a 2D viewport, DepthScape uses a vision-language model to analyze source images and extract key visual components as content anchors for direct manipulation editing. We evaluate DepthScape with nine participants of varying design backgrounds, confirming the effectiveness of our creation pipeline. We also test on 100 professional stock images to assess robustness, and conduct an expert evaluation that confirms the quality of DepthScape's results.",
      "author": "Xia Su, Cuong Nguyen, Matheus A. Gadelha, Jon E. Froehlich",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939596+00:00",
      "updated_at": "2025-12-03T05:21:26.939597+00:00"
    },
    {
      "id": "18037a3721e6dd03175033cdec225726",
      "url": "https://arxiv.org/abs/2512.02179",
      "title": "Young Children's Anthropomorphism of AI Chatbots and the Role of Parent Co-Presence",
      "content": "arXiv:2512.02179v1 Announce Type: new \nAbstract: Artificial Intelligence (AI) chatbots powered by a large language model (LLM) are entering young children's learning and play, yet little is known about how young children construe these agents or how such construals relate to engagement. We examined anthropomorphism of a social AI chatbot during collaborative storytelling and asked how children's attributions related to their behavior and prefrontal activation. Children at ages 5-6 (N = 23) completed three storytelling sessions: interacting with (1) an AI chatbot only, (2) a parent only, and (3) the AI and a parent together. After the sessions, children completed an interview assessing anthropomorphism toward both the AI chatbot and the parent. Behavioral engagement was indexed by the conversational turn count (CTC) ratio, and concurrent fNIRS measured oxygenated hemoglobin in bilateral vmPFC and dmPFC regions. Children reported higher anthropomorphism for parents than for the AI chatbot overall, although AI ratings were relatively high for perceptive abilities and epistemic states. Anthropomorphism was not associated with CTC. In the right dmPFC, higher perceptive scores were associated with greater activation during the AI-only condition and with lower activation during the AI+Parent condition. Exploratory analyses indicated that higher dmPFC activation during the AI-only condition correlated with higher end-of-session \"scared\" mood ratings. Findings suggest that stronger perceptive anthropomorphism can be associated with greater brain activation related to interpreting the AI's mental states, whereas parent co-presence may help some children interpret and regulate novel AI interactions. These results may have design implications for encouraging parent-AI co-use in early childhood.",
      "author": "Pilyoung Kim, Jenna H. Chin, Yun Xie, Nolan Brady, Tom Yeh, Sujin Yang",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 252,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:26.939558+00:00",
      "updated_at": "2025-12-03T05:21:26.939564+00:00"
    },
    {
      "id": "f1bb21dbce210f80422914a001e3d982",
      "url": "https://arxiv.org/abs/2511.20532",
      "title": "MIMIC-MJX: Neuromechanical Emulation of Animal Behavior",
      "content": "arXiv:2511.20532v2 Announce Type: replace \nAbstract: The primary output of the nervous system is movement and behavior. While recent advances have democratized pose tracking during complex behavior, kinematic trajectories alone provide only indirect access to the underlying control processes. Here we present MIMIC-MJX, a framework for learning biologically-plausible neural control policies from kinematics. MIMIC-MJX models the generative process of motor control by training neural controllers that learn to actuate biomechanically-realistic body models in physics simulation to reproduce real kinematic trajectories. We demonstrate that our implementation is accurate, fast, data-efficient, and generalizable to diverse animal body models. Policies trained with MIMIC-MJX can be utilized to both analyze neural control strategies and simulate behavioral experiments, illustrating its potential as an integrative modeling framework for neuroscience.",
      "author": "Charles Y. Zhang (Harvard University), Yuanjia Yang (Salk Institute for Biological Studies), Aidan Sirbu (Mila), Elliott T. T. Abe (University of Washington), Emil W\\\"arnberg (Harvard University), Eric J. Leonardis (Salk Institute for Biological Studies), Diego E. Aldarondo (Harvard University), Adam Lee (Harvard University), Aaditya Prasad (Massachusetts Institute of Technology), Jason Foat (Salk Institute for Biological Studies), Kaiwen Bian (Salk Institute for Biological Studies), Joshua Park (Salk Institute for Biological Studies), Rusham Bhatt (Salk Institute for Biological Studies), Hutton Saunders (Salk Institute for Biological Studies), Akira Nagamori (Salk Institute for Biological Studies), Ayesha R. Thanawalla (Salk Institute for Biological Studies), Kee Wui Huang (Salk Institute for Biological Studies), Fabian Plum (Imperial College London), Hendrik K. Beck (Imperial College London), Steven W. Flavell (Massachusetts Institute of Technology), David Labonte (Imperial College London), Blake A. Richards (Mila), Bingni W. Brunton (University of Washington), Eiman Azim (Salk Institute for Biological Studies), Bence P. \\\"Olveczky (Harvard University), Talmo D. Pereira (Salk Institute for Biological Studies)",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 122,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:25.811909+00:00",
      "updated_at": "2025-12-03T05:21:25.811910+00:00"
    },
    {
      "id": "a4050147083a7ff7ebd59562b2b5e7d0",
      "url": "https://arxiv.org/abs/2505.12653",
      "title": "High-dimensional structure underlying individual differences in naturalistic visual experience",
      "content": "arXiv:2505.12653v2 Announce Type: replace \nAbstract: How do different brains create unique visual experiences from identical sensory input? While neural representations vary across individuals, the fundamental architecture underlying these differences remains poorly understood. Here, we reveal that individual visual experience emerges from a high-dimensional neural geometry across the visual cortical hierarchy. Using spectral decomposition of fMRI responses during naturalistic movie viewing, we find that idiosyncratic neural patterns persist across multiple orders of magnitude of latent dimensions. Remarkably, each dimensional range encodes qualitatively distinct aspects of individual processing, and this multidimensional neural geometry predicts subsequent behavioral differences in memory recall. These fine-grained patterns of inter-individual variability cannot be reduced to those detected by conventional intersubject correlation measures. Our findings demonstrate that subjective visual experience arises from information integrated across an expansive multidimensional manifold. This geometric framework offers a powerful new lens for understanding how diverse brains construct unique perceptual worlds from shared experiences.",
      "author": "Chihye Han, Michael F. Bonner",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:25.811881+00:00",
      "updated_at": "2025-12-03T05:21:25.811883+00:00"
    },
    {
      "id": "f108d7982dd22df2e178aaee8108fa17",
      "url": "https://arxiv.org/abs/2403.15176",
      "title": "Brain-aligning of semantic vectors improves neural decoding of visual stimuli",
      "content": "arXiv:2403.15176v4 Announce Type: replace \nAbstract: The development of algorithms to accurately decode neural information has long been a research focus in the field of neuroscience. Brain decoding typically involves training machine learning models to map neural data onto a preestablished vector representation of stimulus features. These vectors are usually derived from image- and/or text-based feature spaces. Nonetheless, the intrinsic characteristics of these vectors might fundamentally differ from those that are encoded by the brain, limiting the ability of decoders to accurately learn this mapping. To address this issue, we propose a framework, called brain-aligning of semantic vectors, that fine-tunes pretrained feature vectors to better align with the structure of neural representations of visual stimuli in the brain. We trained this model with functional magnetic resonance imaging (fMRI) and then performed zero-shot brain decoding on fMRI, magnetoencephalography (MEG), and electrocorticography (ECoG) data. fMRI-based brain-aligned vectors improved decoding performance across all three neuroimaging datasets when accuracy was determined by calculating the correlation coefficients between true and predicted vectors. Additionally, when decoding accuracy was determined via stimulus identification, this accuracy increased in specific category types; improvements varied depending on the original vector space that was used for brain-alignment, and consistent improvements were observed across all neuroimaging modalities.",
      "author": "Shirin Vafaei, Ryohei Fukuma, Takufumi Yanagisawa, Huixiang Yang, Satoru Oshino, Naoki Tani, Hui Ming Khoo, Hidenori Sugano, Yasushi Iimura, Hiroharu Suzuki, Madoka Nakajima, Kentaro Tamura, Haruhiko Kishima",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 204,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:25.811847+00:00",
      "updated_at": "2025-12-03T05:21:25.811849+00:00"
    },
    {
      "id": "a34ab77b5e48cf0f93af896577d50941",
      "url": "https://arxiv.org/abs/2512.02719",
      "title": "Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs",
      "content": "arXiv:2512.02719v1 Announce Type: cross \nAbstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.",
      "author": "Julian Ma, Jun Wang, Zafeirios Fountas",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:25.811815+00:00",
      "updated_at": "2025-12-03T05:21:25.811817+00:00"
    },
    {
      "id": "f2ff03a2059b8bad3845bef96ce7a92b",
      "url": "https://arxiv.org/abs/2512.02028",
      "title": "Seizure-NGCLNet: Representation Learning of SEEG Spatial Pathological Patterns for Epileptic Seizure Detection via Node-Graph Dual Contrastive Learning",
      "content": "arXiv:2512.02028v1 Announce Type: cross \nAbstract: Complex spatial connectivity patterns, such as interictal suppression and ictal propagation, complicate accurate drug-resistant epilepsy (DRE) seizure detection using stereotactic electroencephalography (SEEG) and traditional machine learning methods. Two critical challenges remain:(1)a low signal-to-noise ratio in functional connectivity estimates, making it difficult to learn seizure-related interactions; and (2)expert labels for spatial pathological connectivity patterns are difficult to obtain, meanwhile lacking the patterns' representation to improve seizure detection. To address these issues, we propose a novel node-graph dual contrastive learning framework, Seizure-NGCLNet, to learn SEEG interictal suppression and ictal propagation patterns for detecting DRE seizures with high precision. First, an adaptive graph augmentation strategy guided by centrality metrics is developed to generate seizure-related brain networks. Second, a dual-contrastive learning approach is integrated, combining global graph-level contrast with local node-graph contrast, to encode both spatial structural and semantic epileptogenic features. Third, the pretrained embeddings are fine-tuned via a top-k localized graph attention network to perform the final classification. Extensive experiments on a large-scale public SEEG dataset from 33 DRE patients demonstrate that Seizure-NGCLNet achieves state-of-the-art performance, with an average accuracy of 95.93%, sensitivity of 96.25%, and specificity of 94.12%. Visualizations confirm that the learned embeddings clearly separate ictal from interictal states, reflecting suppression and propagation patterns that correspond to the clinical mechanisms. These results highlight Seizure-NGCLNet's ability to learn interpretable spatial pathological patterns, enhancing both seizure detection and seizure onset zone localization.",
      "author": "Yiping Wang, Peiren Wang, Zhenye Li, Fang Liu, Jinguo Huang",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:25.811780+00:00",
      "updated_at": "2025-12-03T05:21:25.811782+00:00"
    },
    {
      "id": "fb211b73df920e606239089d6584a946",
      "url": "https://arxiv.org/abs/2512.02978",
      "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding",
      "content": "arXiv:2512.02978v1 Announce Type: new \nAbstract: Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.",
      "author": "Paul Barbaste (Inclusive Brains, Wavestone, Human Technology Foundation), Olivier Oullier (Human-Computer Interaction Department, Mohamed bin Zayed University of Artificial Intelligence, Inclusive Brains, Institute for Artificial Intelligence, Biotech Dental Group), Xavier Vasques (IBM Technology, IBM France Lab)",
      "published_date": "2025-12-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 213,
      "reading_time": 1,
      "created_at": "2025-12-03T05:21:25.811746+00:00",
      "updated_at": "2025-12-03T05:21:25.811747+00:00"
    }
  ]
}