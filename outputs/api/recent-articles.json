{
  "last_updated": "2025-12-15T12:51:20.373534+00:00",
  "count": 20,
  "articles": [
    {
      "id": "af59f3868d5fd673c61d33129092270e",
      "url": "https://www.reddit.com/r/Python/comments/1pn51h1/my_first_c_extension/",
      "title": "My First C Extension",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>I've had decent success with pybind11, nanobind, and PyO3 in the past, and I've never really clicked with Cython for text-processing-heavy work. For my latest project, though, I decided to skip binding frameworks entirely and work directly with Python's C API.</p> <p>For a typical text parsing / templating workload, my reasoning went something like this:</p> <ol> <li>If we care about performance, we want to avoid copying or re-encoding potentially large input strings.</li> <li>If we're processing an opaque syntax tree (or other internal representation) with contextual data in the form of Python objects, we want to avoid data object wrappers or other indirect access to that data.</li> <li>If the result is a potentially large string, we want to avoid copying or re-encoding before handing it back to Python.</li> <li>If we exposing a large syntax tree to Python, we want to avoid indirect access for every node in the tree.</li> </ol> <p>The obvious downside is that we have to deal with manual memory management and Python reference counting. That is what I've been practicing with <a href=\"https://github.com/jg-rp/nano-template\">Nano Template</a>.</p> <h1>What My Project Does</h1> <p><a href=\"https://github.com/jg-rp/nano-template\">Nano Template</a> is a fast, non-evaluating template engine with syntax that should look familiar if you've used Jinja, Minijinja, or Django templates.</p> <p>Unlike those engines, Nano Template deliberately has a reduced feature set. The idea is to keep application logic out of template text. Instead of manipulating data inside the template, you're expected to prepare it in Python before rendering.</p> <p>Example usage:</p> <pre><code>import nano_template as nt template = nt.parse(&quot;&quot;&quot;\\ {% if page['heading override'] -%} # {{ page['heading override'] }} {% else -%} # Welcome to {{ page.title }}! {% endif %} Hello, {{ you or 'guest' }}. {% for tag in page.tags ~%} - {{ tag.name }} {% endfor -%} &quot;&quot;&quot;) data = { &quot;page&quot;: { &quot;title&quot;: &quot;Demo page&quot;, &quot;tags&quot;: [{&quot;name&quot;: &quot;programming&quot;, &quot;id&quot;: 42}, {&quot;name&quot;: &quot;python&quot;}], } } result = template.render(data) print(result) </code></pre> <h1>Target Audience</h1> <p>Nano Template is for Python developers who want improved performance from a template engine at the expense of features.</p> <h1>Comparison</h1> <p>A provisional benchmark shows Nano Template to be about 17 times faster than a pure Python implementation, and about 4 times faster than Minijinja, when measuring parsing and rendering together.</p> <p>For scenarios where you're parsing once and rendering many times, Jinja2 tends to beat Minijinja. Nano Template is still about 2.8 time faster than Jinja2 and bout 7.5 time faster than Minijinja in that scenario.</p> <p>Excluding parsing time and limiting our benchmark fixture to simple variable substitution, Nano Template renders about 10% slower than <code>str.format()</code> (we're using cPython's limited C API, which comes with a performance cost).</p> <pre><code>$ python scripts/benchmark.py (001) 5 rounds with 10000 iterations per round. parse c ext : best = 0.092587s | avg = 0.092743s parse pure py : best = 2.378554s | avg = 2.385293s just render c ext : best = 0.061812s | avg = 0.061850s just render pure py : best = 0.314468s | avg = 0.315076s just render jinja2 : best = 0.170373s | avg = 0.170706s just render minijinja : best = 0.454723s | avg = 0.457256s parse and render ext : best = 0.155797s | avg = 0.156455s parse and render pure py : best = 2.733121s | avg = 2.745028s parse and render jinja2 : &lt;with caching disabled, I got bored waiting&gt; parse and render minijinja : best = 0.705995s | avg = 0.707589s $ python scripts/benchmark_format.py (002) 5 rounds with 1000000 iterations per round. render template : best = 0.413830s | avg = 0.419547s format string : best = 0.375050s | avg = 0.375237s </code></pre> <h1>Conclusion</h1> <p>Jinja or Minijinja are still usually the right choice for a general-purpose template engine. They are well established and plenty fast enough for most use cases (especially if you're parsing once and rendering many times with Jinja).</p> <p>For me, this was mainly a stepping-stone project to get more comfortable with C, the Python C API, and the tooling needed to write and publish safe C extensions. My next project is to rewrite <a href=\"https://github.com/jg-rp/python-pest\">Python Pest</a> as a C extension using similar techniques.</p> <p>As always, feedback is most welcome.</p> <p>GitHub: <a href=\"https://github.com/jg-rp/nano-template\">https://github.com/jg-rp/nano-template</a><br /> PyPi: <a href=\"https://pypi.org/project/nano-template/\">https://pypi.org/project/nano-template/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hefty-Pianist-1958\"> /u/Hefty-Pianist-1958 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1pn51h1/my_first_c_extension/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1pn51h1/my_first_c_extension/\">[comments]</a></span>",
      "author": "/u/Hefty-Pianist-1958",
      "published_date": "2025-12-15T11:26:10+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 710,
      "reading_time": 3,
      "created_at": "2025-12-15T12:50:11.125092+00:00",
      "updated_at": "2025-12-15T12:50:11.125093+00:00"
    },
    {
      "id": "260462373a5b4056d5e2c97273c950bb",
      "url": "https://www.optery.com/careers/",
      "title": "Optery (YC W22) Hiring CISO, Release Manager, Tech Lead (Node), Full Stack Eng",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46273372\">Comments</a>",
      "author": "",
      "published_date": "2025-12-15T12:00:26+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-12-15T12:50:09.782956+00:00",
      "updated_at": "2025-12-15T12:50:09.782958+00:00"
    },
    {
      "id": "08730e5e81ac0a42b6e39476e7437cc4",
      "url": "https://marcusolang.substack.com/p/im-kenyan-i-dont-write-like-chatgpt",
      "title": "I'm Kenyan. I Don't Write Like ChatGPT. ChatGPT Writes Like Me",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46273466\">Comments</a>",
      "author": "",
      "published_date": "2025-12-15T12:12:24+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-12-15T12:50:09.782837+00:00",
      "updated_at": "2025-12-15T12:50:09.782839+00:00"
    },
    {
      "id": "ef2e181b1c63f9c14d4aa6b0f5cc08dc",
      "url": "https://www.bloodinthemachine.com/p/i-was-forced-to-use-ai-until-the",
      "title": "Copywriters reveal how AI has decimated their industry",
      "content": "<p>Article URL: <a href=\"https://www.bloodinthemachine.com/p/i-was-forced-to-use-ai-until-the\">https://www.bloodinthemachine.com/p/i-was-forced-to-use-ai-until-the</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46272921\">https://news.ycombinator.com/item?id=46272921</a></p>\n<p>Points: 18</p>\n<p># Comments: 2</p>",
      "author": "thm",
      "published_date": "2025-12-15T11:09:12+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-12-15T12:50:08.662485+00:00",
      "updated_at": "2025-12-15T12:50:08.662487+00:00"
    },
    {
      "id": "260462373a5b4056d5e2c97273c950bb",
      "url": "https://www.optery.com/careers/",
      "title": "Optery (YC W22) Hiring CISO, Release Manager, Tech Lead (Node), Full Stack Eng",
      "content": "<p>Article URL: <a href=\"https://www.optery.com/careers/\">https://www.optery.com/careers/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46273372\">https://news.ycombinator.com/item?id=46273372</a></p>\n<p>Points: 0</p>\n<p># Comments: 0</p>",
      "author": "beyondd",
      "published_date": "2025-12-15T12:00:26+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-12-15T12:50:08.662463+00:00",
      "updated_at": "2025-12-15T12:50:08.662465+00:00"
    },
    {
      "id": "08730e5e81ac0a42b6e39476e7437cc4",
      "url": "https://marcusolang.substack.com/p/im-kenyan-i-dont-write-like-chatgpt",
      "title": "I'm Kenyan. I Don't Write Like ChatGPT. ChatGPT Writes Like Me",
      "content": "<p>Article URL: <a href=\"https://marcusolang.substack.com/p/im-kenyan-i-dont-write-like-chatgpt\">https://marcusolang.substack.com/p/im-kenyan-i-dont-write-like-chatgpt</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46273466\">https://news.ycombinator.com/item?id=46273466</a></p>\n<p>Points: 30</p>\n<p># Comments: 1</p>",
      "author": "florian_s",
      "published_date": "2025-12-15T12:12:24+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-12-15T12:50:08.662432+00:00",
      "updated_at": "2025-12-15T12:50:08.662442+00:00"
    },
    {
      "id": "c2319578819743fdf0159bf723bcb1b5",
      "url": "https://erpinfo.org/blog/2024/3/5/changes-to-the-2024-erp-boot-camp",
      "title": "Important Changes to the 2024 ERP Boot Camp",
      "content": "<p class=\"\">We are disappointed to announce that we will not be holding a regular 10-day ERP Boot Camp this summer.</p><p class=\"\">We have held Boot Camps nearly every summer since 2007, supported by a series of generous grants from NIMH that allowed us to provide scholarships for all attendees. Unfortunately, although our recent renewal proposal received extremely positive reviews and scores, we were recently given the surprising and disappointing news that the renewal will not be funded this year. We believe that the ERP Boot Camp provides essential training to the field, and we will continue to pursue financial support to continue holding 10-day ERP Boot Camps in the future. </p><p class=\"\">In the meantime, we have partial funding that will allow us to hold a 5-day ERP Boot Camp this summer from July 8-12, 2024 in Davis, California. The workshop will include 5-days of lectures and activities on EEG and ERP measures, including practical and theoretical issues.</p><p class=\"\">Unfortunately, we will not be able to provide scholarships to pay for travel and lodging costs, and we must charge a registration fee. We are very sorry if this causes a hardship. </p><p class=\"\">We are no longer taking applications through our application portal. Instead of a competitive application process, we will simply accept the first 30 people who complete the registration process and pay the registration fee. This provides an opportunity to attend for individuals who might otherwise not make it through our ordinary application process, which is highly competitive. </p><p class=\"\">The registration fee will be $1000 (or $900 for people who register by April 15). The registration fee will cover 6 nights in a single occupancy hotel room (arriving July 7 and departing July 13), daily breakfast at the hotel, a catered lunch for each day of the workshop, and a group dinner. <strong>You must pay the registration fee with a credit card when you register.</strong> There are no exceptions to the registration fee policy.</p><p class=\"\"><strong>Registration is now open</strong> at <a href=\"https://na.eventscloud.com/793175\">https://na.eventscloud.com/793175</a>.</p><p class=\"\">Given that we will accept the first 30 registrants, we encourage you to register as soon as possible. <strong>Registration will close on May 20</strong>, but we anticipate that the workshop will be filled up long before then. </p><p class=\"\">You must pay for your own transportation to Davis. Davis is approximately 20 minutes away from the Sacramento Airport (SMF). You can take the <a href=\"https://www.davisairporter.com/\" target=\"_blank\">Davis Airporter</a> shuttle service or a rideshare service from SMF to Davis. If you are coming from outside North America, you may want to fly into the San Francisco airport (SFO), which is 135 km (84 miles) from Davis. We recommend taking the <a href=\"https://www.davisairporter.com/\" target=\"_blank\">Davis Airporter</a> from SFO to Davis.</p>",
      "author": "Steve Luck",
      "published_date": "2024-03-05T19:34:57+00:00",
      "source": "Erp Boot Camp",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 444,
      "reading_time": 2,
      "created_at": "2025-12-15T11:21:26.521023+00:00",
      "updated_at": "2025-12-15T12:36:50.144989+00:00",
      "metadata": {
        "processed_at": "2025-12-15T12:36:50.144997+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "30c62259d3f0fa60ef1bd8a2c9fc9762",
      "url": "https://embc.embs.org/2026/#new_tab",
      "title": "Join the Mailing List",
      "content": "<p>The post <a href=\"https://embc.embs.org/2026/#new_tab\">Join the Mailing List</a> appeared first on <a href=\"https://www.embs.org\">IEEE EMBS</a>.</p>",
      "author": "dziura",
      "published_date": "2025-12-10T16:41:05+00:00",
      "source": "Embs",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-12-15T11:21:16.327882+00:00",
      "updated_at": "2025-12-15T12:36:50.145001+00:00",
      "metadata": {
        "processed_at": "2025-12-15T12:36:50.145003+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "462d809fc097cbd2a6515bc023757c9f",
      "url": "https://app.smartsheet.com/b/form/0199e95f16727ef99ec5b9ea73e78ab6#new_tab",
      "title": "Apply for the 2026 One Mind Accelerator Emerging Innovator Observership",
      "content": "<p>The post <a href=\"https://app.smartsheet.com/b/form/0199e95f16727ef99ec5b9ea73e78ab6#new_tab\">Apply for the 2026 One Mind Accelerator Emerging Innovator Observership</a> appeared first on <a href=\"https://www.embs.org\">IEEE EMBS</a>.</p>",
      "author": "dziura",
      "published_date": "2025-12-10T16:49:24+00:00",
      "source": "Embs",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 19,
      "reading_time": 1,
      "created_at": "2025-12-15T11:21:16.327863+00:00",
      "updated_at": "2025-12-15T12:36:50.145006+00:00",
      "metadata": {
        "processed_at": "2025-12-15T12:36:50.145008+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "68153e6f4d1a3bbd6315ad5951b8ff4f",
      "url": "https://www.nature.com/articles/s41593-025-02136-5",
      "title": "The stroke risk gene Foxf2 maintains brain endothelial cell function via Tie2 signaling",
      "content": "",
      "author": "",
      "published_date": "2025-12-15T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-15T11:21:02.818239+00:00",
      "updated_at": "2025-12-15T12:36:50.145010+00:00",
      "metadata": {
        "processed_at": "2025-12-15T12:36:50.145012+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "011f23a424234e8462e5dee53b56471d",
      "url": "https://www.nature.com/articles/s41467-025-66932-0",
      "title": "Unequal mitochondrial segregation promotes asymmetric fates during neurogenesis",
      "content": "",
      "author": "",
      "published_date": "2025-12-15T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-15T11:21:02.818221+00:00",
      "updated_at": "2025-12-15T12:36:50.145014+00:00",
      "metadata": {
        "processed_at": "2025-12-15T12:36:50.145016+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "49e3bd4bd8e2136a3874a924618649ec",
      "url": "https://www.nature.com/articles/s41593-025-02123-w",
      "title": "A fully iPS-cell-derived 3D model of the human blood\u2013brain barrier for exploring neurovascular disease mechanisms and therapeutic interventions",
      "content": "",
      "author": "",
      "published_date": "2025-12-15T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-15T11:21:02.818183+00:00",
      "updated_at": "2025-12-15T11:21:02.818185+00:00"
    },
    {
      "id": "c78c9eee6e9d2b3783cb0d3103c96a8a",
      "url": "https://www.nature.com/articles/s41562-025-02357-5",
      "title": "Combined evidence from artificial neural networks and human brain-lesion models reveals that language modulates vision in human perception",
      "content": "",
      "author": "",
      "published_date": "2025-12-15T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-15T11:21:02.818143+00:00",
      "updated_at": "2025-12-15T11:21:02.818145+00:00"
    },
    {
      "id": "f3ea029d8f11e5ca20144f9ad88fcc85",
      "url": "https://www.nature.com/articles/s41467-025-67077-w",
      "title": "Single-neuron correlates of visual consciousness in human lateral occipital complex",
      "content": "",
      "author": "",
      "published_date": "2025-12-15T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-15T11:21:02.818118+00:00",
      "updated_at": "2025-12-15T11:21:02.818123+00:00"
    },
    {
      "id": "8cc127c70142a432a087ee3c14a0e433",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1408836",
      "title": "A hierarchical Bayesian inference model for volatile multivariate exponentially distributed signals",
      "content": "Brain activities often follow an exponential family of distributions. The exponential distribution is the maximum entropy distribution of continuous random variables in the presence of a mean. The memoryless and peakless properties of an exponential distribution impose difficulties for data analysis methods. To estimate the rate parameter of multivariate exponential distribution from a time series of sensory inputs (i.e., observations), we constructed a hierarchical Bayesian inference model based on a variant of general hierarchical Brownian filter (GHBF). To account for the complex interactions among multivariate exponential random variables, the model estimates the second-order interaction of the rate intensity parameter in logarithmic space. Using variational Bayesian scheme, a family of closed-form and analytical update equations are introduced. These update equations also constitute a complete predictive coding framework. The simulation study shows that our model has the ability to evaluate the time-varying rate parameters and the underlying correlation structure of volatile multivariate exponentially distributed signals. The proposed hierarchical Bayesian inference model is of practical utility in analyzing high-dimensional neural activities.",
      "author": "Bailu Si",
      "published_date": "2025-11-12T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 168,
      "reading_time": 1,
      "created_at": "2025-12-15T10:53:42.973298+00:00",
      "updated_at": "2025-12-15T10:53:42.973300+00:00"
    },
    {
      "id": "583f75b04e34ce2f7594a5123b501ba8",
      "url": "https://www.frontiersin.org/articles/10.3389/fnbot.2025.1689332",
      "title": "On collective behavior in C. elegans",
      "content": "C. elegans is a model organism in many biological domains, such as genetics, neurophysiology, and behavioral ecology. Despite our relatively deep knowledge of the neuronal, genetic and molecular mechanisms underlying C. elegans communication, we still lack a comprehensive understanding of emergent group-level dynamics. We review the literature on collective behavior of C. elegans by categorizing works in this relatively small research field along three main axes corresponding to primary collective responses: aggregation, swarming, and collective decision-making. Through an analysis of the methods and scientific contributions of these works, we develop a critical perspective that points to important gaps in our understanding of the mechanisms underlaying the emergence of collective responses. We discuss the consequences of the lack of evidence concerning the effect of population density on the emergence of specific group dynamics, and the relatively limited knowledge related to how self-generated pheromones regulate local interactions and contribute to the emergence of group responses. We elaborate on the methodological problems of developing experimental scenarios to disentangle causal relationships between population density, pheromone-based interactions and collective responses. We propose to overcome these limitations with an interdisciplinary approach based on the use of in vivo experiments, mathematical and computer-based models.",
      "author": "Elio Tuci",
      "published_date": "2025-11-17T00:00:00+00:00",
      "source": "Frontiers Neurorobotics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 197,
      "reading_time": 1,
      "created_at": "2025-12-15T10:53:40.205234+00:00",
      "updated_at": "2025-12-15T10:53:40.205236+00:00"
    },
    {
      "id": "b9fa56ef861a0da6638fdb600d7462cb",
      "url": "https://andyatkinson.com/avoid-uuid-version-4-primary-keys",
      "title": "Avoid UUIDv4 Primary Keys",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46272487\">Comments</a>",
      "author": "",
      "published_date": "2025-12-15T10:08:02+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-12-15T10:53:15.188119+00:00",
      "updated_at": "2025-12-15T10:53:15.188121+00:00"
    },
    {
      "id": "6ef0379a9edb4e2cc36abc094b6048ec",
      "url": "http://doi.org/10.1037/pmu0000304",
      "title": "The sound of manufactured music: Reviewing the role of artificial stimuli in music cognition research.",
      "content": "Having participants listen and react to musical stimuli is one of music cognition\u2019s foundational methods. Whereas most researchers have used stimuli adapted from existing musical traditions in such work, others have incorporated artificial stimuli (i.e., stimuli generated specifically for research that are not borrowed from any existing musical system) into their research designs. No review of this growing literature on artificial stimuli exists, leaving open the question of how useful they are in helping formulate and test research questions in music cognition. To this end, a systematic narrative review of empirical studies utilizing artificial musical stimuli (N = 52) was conducted. Comparing these studies to analogous works involving conventional stimuli in the areas of music preference, music learning, and musical emotion revealed the power of artificial stimuli in reducing concerns regarding stimulus familiarity and musical enculturation confounds, as well as in refining theory by reiterating the need for the development of composite predictive models in music cognition. This review also suggests that using artificial stimuli has the potential to help clarify the neurological and sociocognitive factors surrounding music listening behavior. Furthermore, because artificial stimuli are not tied to any existing sociocultural musical system, they may also lead to more generalizable findings. This enhanced generalizability may also coincide with expanded inclusivity, toward which the field of music cognition (and psychology more generally) are increasingly striving. Other potential uses, benefits, and limitations of artificial musical stimuli are discussed, as well as recommendations for researchers seeking to utilize such stimuli in future studies. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2024-01-22T00:00:00+00:00",
      "source": "Psychomusicology",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 259,
      "reading_time": 1,
      "created_at": "2025-12-15T10:27:50.137021+00:00",
      "updated_at": "2025-12-15T10:27:50.137023+00:00"
    },
    {
      "id": "529924c1ce4188238b69edd3f4ca1201",
      "url": "https://www.reddit.com/r/Python/comments/1pn2a3t/kreuzberg_v400rc8_is_available/",
      "title": "Kreuzberg v4.0.0-rc.8 is available",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>Hi Peeps,</p> <p>I'm excited to announce that <a href=\"https://github.com/kreuzberg-dev/kreuzberg\">Kreuzberg</a> v4.0.0 is coming very soon. We will release v4.0.0 at the beginning of next year - in just a couple of weeks time. For now, v4.0.0-rc.8 has been released to all channels.</p> <h2>What is Kreuzberg?</h2> <p>Kreuzberg is a document intelligence toolkit for extracting text, metadata, tables, images, and structured data from 56+ file formats. It was originally written in Python (v1-v3), where it demonstrated strong performance characteristics compared to alternatives in the ecosystem.</p> <h2>What's new in V4?</h2> <h3>A Complete Rust Rewrite with Polyglot Bindings</h3> <p>The new version of Kreuzberg represents a massive architectural evolution. <strong>Kreuzberg has been completely rewritten in Rust</strong> - leveraging Rust's memory safety, zero-cost abstractions, and native performance. The new architecture consists of a high-performance Rust core with native bindings to multiple languages. That's right - it's no longer just a Python library.</p> <p><strong>Kreuzberg v4 is now available for 7 languages across 8 runtime bindings:</strong></p> <ul> <li><strong>Rust</strong> (native library)</li> <li><strong>Python</strong> (PyO3 native bindings)</li> <li><strong>TypeScript</strong> - Node.js (NAPI-RS native bindings) + Deno/Browser/Edge (WASM)</li> <li><strong>Ruby</strong> (Magnus FFI)</li> <li><strong>Java 25+</strong> (Panama Foreign Function &amp; Memory API)</li> <li><strong>C#</strong> (P/Invoke)</li> <li><strong>Go</strong> (cgo bindings)</li> </ul> <p><strong>Post v4.0.0 roadmap includes:</strong></p> <ul> <li>PHP</li> <li>Elixir (via Rustler - with Erlang and Gleam interop)</li> </ul> <p>Additionally, it's available as a <strong>CLI</strong> (installable via <code>cargo</code> or <code>homebrew</code>), <strong>HTTP REST API server</strong>, <strong>Model Context Protocol (MCP) server</strong> for Claude Desktop/Continue.dev, and as <strong>public Docker images</strong>.</p> <h3>Why the Rust Rewrite? Performance and Architecture</h3> <p>The Rust rewrite wasn't just about performance - though that's a major benefit. It was an opportunity to fundamentally rethink the architecture:</p> <p><strong>Architectural improvements:</strong> - <strong>Zero-copy operations</strong> via Rust's ownership model - <strong>True async concurrency</strong> with Tokio runtime (no GIL limitations) - <strong>Streaming parsers</strong> for constant memory usage on multi-GB files - <strong>SIMD-accelerated text processing</strong> for token reduction and string operations - <strong>Memory-safe FFI boundaries</strong> for all language bindings - <strong>Plugin system</strong> with trait-based extensibility</p> <h3>v3 vs v4: What Changed?</h3> <table><thead> <tr> <th>Aspect</th> <th>v3 (Python)</th> <th>v4 (Rust Core)</th> </tr> </thead><tbody> <tr> <td><strong>Core Language</strong></td> <td>Pure Python</td> <td>Rust 2024 edition</td> </tr> <tr> <td><strong>File Formats</strong></td> <td>30-40+ (via Pandoc)</td> <td><strong>56+ (native parsers)</strong></td> </tr> <tr> <td><strong>Language Support</strong></td> <td>Python only</td> <td><strong>7 languages</strong> (Rust/Python/TS/Ruby/Java/Go/C#)</td> </tr> <tr> <td><strong>Dependencies</strong></td> <td>Requires Pandoc (system binary)</td> <td><strong>Zero system dependencies</strong> (all native)</td> </tr> <tr> <td><strong>Embeddings</strong></td> <td>Not supported</td> <td>\u2713 FastEmbed with ONNX (3 presets + custom)</td> </tr> <tr> <td><strong>Semantic Chunking</strong></td> <td>Via semantic-text-splitter library</td> <td>\u2713 Built-in (text + markdown-aware)</td> </tr> <tr> <td><strong>Token Reduction</strong></td> <td>Built-in (TF-IDF based)</td> <td>\u2713 Enhanced with 3 modes</td> </tr> <tr> <td><strong>Language Detection</strong></td> <td>Optional (fast-langdetect)</td> <td>\u2713 Built-in (68 languages)</td> </tr> <tr> <td><strong>Keyword Extraction</strong></td> <td>Optional (KeyBERT)</td> <td>\u2713 Built-in (YAKE + RAKE algorithms)</td> </tr> <tr> <td><strong>OCR Backends</strong></td> <td>Tesseract/EasyOCR/PaddleOCR</td> <td><strong>Same + better integration</strong></td> </tr> <tr> <td><strong>Plugin System</strong></td> <td>Limited extractor registry</td> <td><strong>Full trait-based</strong> (4 plugin types)</td> </tr> <tr> <td><strong>Page Tracking</strong></td> <td>Character-based indices</td> <td><strong>Byte-based with O(1) lookup</strong></td> </tr> <tr> <td><strong>Servers</strong></td> <td>REST API (Litestar)</td> <td><strong>HTTP (Axum) + MCP + MCP-SSE</strong></td> </tr> <tr> <td><strong>Installation Size</strong></td> <td>~100MB base</td> <td><strong>16-31 MB complete</strong></td> </tr> <tr> <td><strong>Memory Model</strong></td> <td>Python heap management</td> <td><strong>RAII with streaming</strong></td> </tr> <tr> <td><strong>Concurrency</strong></td> <td>asyncio (GIL-limited)</td> <td><strong>Tokio work-stealing</strong></td> </tr> </tbody></table> <h3>Replacement of Pandoc - Native Performance</h3> <p>Kreuzberg v3 relied on <strong>Pandoc</strong> - an amazing tool, but one that had to be invoked via subprocess because of its GPL license. This had significant impacts:</p> <p><strong>v3 Pandoc limitations:</strong> - System dependency (installation required) - Subprocess overhead on every document - No streaming support - Limited metadata extraction - ~500MB+ installation footprint</p> <p><strong>v4 native parsers:</strong> - <strong>Zero external dependencies</strong> - everything is native Rust - Direct parsing with full control over extraction - <strong>Substantially more metadata</strong> extracted (e.g., DOCX document properties, section structure, style information) - <strong>Streaming support</strong> for massive files (tested on multi-GB XML documents with stable memory) - Example: PPTX extractor is now a <strong>fully streaming parser</strong> capable of handling gigabyte-scale presentations with constant memory usage and high throughput</p> <h3>New File Format Support</h3> <p>v4 expanded format support from ~20 to <strong>56+ file formats</strong>, including:</p> <p><strong>Added legacy format support:</strong> - <code>.doc</code> (Word 97-2003) - <code>.ppt</code> (PowerPoint 97-2003) - <code>.xls</code> (Excel 97-2003) - <code>.eml</code> (Email messages) - <code>.msg</code> (Outlook messages)</p> <p><strong>Added academic/technical formats:</strong> - LaTeX (<code>.tex</code>) - BibTeX (<code>.bib</code>) - Typst (<code>.typ</code>) - JATS XML (scientific articles) - DocBook XML - FictionBook (<code>.fb2</code>) - OPML (<code>.opml</code>)</p> <p><strong>Better Office support:</strong> - XLSB, XLSM (Excel binary/macro formats) - Better structured metadata extraction from DOCX/PPTX/XLSX - Full table extraction from presentations - Image extraction with deduplication</p> <h3>New Features: Full Document Intelligence Solution</h3> <p>The v4 rewrite was also an opportunity to close gaps with commercial alternatives and add features specifically designed for <strong>RAG applications and LLM workflows</strong>:</p> <h4>1. <strong>Embeddings (NEW)</strong></h4> <ul> <li><strong>FastEmbed integration</strong> with full ONNX Runtime acceleration</li> <li>Three presets: <code>&quot;fast&quot;</code> (384d), <code>&quot;balanced&quot;</code> (512d), <code>&quot;quality&quot;</code> (768d/1024d)</li> <li>Custom model support (bring your own ONNX model)</li> <li>Local generation (no API calls, no rate limits)</li> <li>Automatic model downloading and caching</li> <li>Per-chunk embedding generation</li> </ul> <p>```python from kreuzberg import ExtractionConfig, EmbeddingConfig, EmbeddingModelType</p> <p>config = ExtractionConfig( embeddings=EmbeddingConfig( model=EmbeddingModelType.preset(&quot;balanced&quot;), normalize=True ) ) result = kreuzberg.extract_bytes(pdf_bytes, config=config)</p> <h1>result.embeddings contains vectors for each chunk</h1> <p>```</p> <h4>2. <strong>Semantic Text Chunking (NOW BUILT-IN)</strong></h4> <p>Now integrated directly into the core (v3 used external semantic-text-splitter library): - <strong>Structure-aware chunking</strong> that respects document semantics - Two strategies: - Generic text chunker (whitespace/punctuation-aware) - Markdown chunker (preserves headings, lists, code blocks, tables) - Configurable chunk size and overlap - Unicode-safe (handles CJK, emojis correctly) - Automatic chunk-to-page mapping - Per-chunk metadata with byte offsets</p> <h4>3. <strong>Byte-Accurate Page Tracking (BREAKING CHANGE)</strong></h4> <p>This is a critical improvement for LLM applications:</p> <ul> <li><strong>v3</strong>: Character-based indices (<code>char_start</code>/<code>char_end</code>) - incorrect for UTF-8 multi-byte characters</li> <li><strong>v4</strong>: Byte-based indices (<code>byte_start</code>/<code>byte_end</code>) - correct for all string operations</li> </ul> <p>Additional page features: - O(1) lookup: &quot;which page is byte offset X on?&quot; \u2192 instant answer - Per-page content extraction - Page markers in combined text (e.g., <code>--- Page 5 ---</code>) - Automatic chunk-to-page mapping for citations</p> <h4>4. <strong>Enhanced Token Reduction for LLM Context</strong></h4> <p>Enhanced from v3 with three configurable modes to save on LLM costs:</p> <ul> <li><strong>Light mode</strong>: ~15% reduction (preserve most detail)</li> <li><strong>Moderate mode</strong>: ~30% reduction (balanced)</li> <li><strong>Aggressive mode</strong>: ~50% reduction (key information only)</li> </ul> <p>Uses TF-IDF sentence scoring with position-aware weighting and language-specific stopword filtering. SIMD-accelerated for improved performance over v3.</p> <h4>5. <strong>Language Detection (NOW BUILT-IN)</strong></h4> <ul> <li>68 language support with confidence scoring</li> <li>Multi-language detection (documents with mixed languages)</li> <li>ISO 639-1 and ISO 639-3 code support</li> <li>Configurable confidence thresholds</li> </ul> <h4>6. <strong>Keyword Extraction (NOW BUILT-IN)</strong></h4> <p>Now built into core (previously optional KeyBERT in v3): - <strong>YAKE</strong> (Yet Another Keyword Extractor): Unsupervised, language-independent - <strong>RAKE</strong> (Rapid Automatic Keyword Extraction): Fast statistical method - Configurable n-grams (1-3 word phrases) - Relevance scoring with language-specific stopwords</p> <h4>7. <strong>Plugin System (NEW)</strong></h4> <p>Four extensible plugin types for customization:</p> <ul> <li><strong>DocumentExtractor</strong> - Custom file format handlers</li> <li><strong>OcrBackend</strong> - Custom OCR engines (integrate your own Python models)</li> <li><strong>PostProcessor</strong> - Data transformation and enrichment</li> <li><strong>Validator</strong> - Pre-extraction validation</li> </ul> <p>Plugins defined in Rust work across all language bindings. Python/TypeScript can define custom plugins with thread-safe callbacks into the Rust core.</p> <h4>8. <strong>Production-Ready Servers (NEW)</strong></h4> <ul> <li><strong>HTTP REST API</strong>: Production-grade Axum server with OpenAPI docs</li> <li><strong>MCP Server</strong>: Direct integration with Claude Desktop, Continue.dev, and other MCP clients</li> <li><strong>MCP-SSE Transport</strong> (RC.8): Server-Sent Events for cloud deployments without WebSocket support</li> <li>All three modes support the same feature set: extraction, batch processing, caching</li> </ul> <h2>Performance: Benchmarked Against the Competition</h2> <p>We maintain <strong>continuous benchmarks</strong> comparing Kreuzberg against the leading OSS alternatives:</p> <h3>Benchmark Setup</h3> <ul> <li><strong>Platform</strong>: Ubuntu 22.04 (GitHub Actions)</li> <li><strong>Test Suite</strong>: 30+ documents covering all formats</li> <li><strong>Metrics</strong>: Latency (p50, p95), throughput (MB/s), memory usage, success rate</li> <li><strong>Competitors</strong>: Apache Tika, Docling, Unstructured, MarkItDown</li> </ul> <h3>How Kreuzberg Compares</h3> <p><strong>Installation Size</strong> (critical for containers/serverless): - <strong>Kreuzberg</strong>: <strong>16-31 MB complete</strong> (CLI: 16 MB, Python wheel: 22 MB, Java JAR: 31 MB - all features included) - <strong>MarkItDown</strong>: ~251 MB installed (58.3 KB wheel, 25 dependencies) - <strong>Unstructured</strong>: ~146 MB minimal (open source base) - <strong>several GB with ML models</strong> - <strong>Docling</strong>: ~1 GB base, <strong>9.74GB Docker image</strong> (includes PyTorch CUDA) - <strong>Apache Tika</strong>: ~55 MB (tika-app JAR) + dependencies - <strong>GROBID</strong>: 500MB (CRF-only) to <strong>8GB</strong> (full deep learning)</p> <p><strong>Performance Characteristics:</strong></p> <table><thead> <tr> <th>Library</th> <th>Speed</th> <th>Accuracy</th> <th>Formats</th> <th>Installation</th> <th>Use Case</th> </tr> </thead><tbody> <tr> <td><strong>Kreuzberg</strong></td> <td>\u26a1 Fast (Rust-native)</td> <td>Excellent</td> <td>56+</td> <td><strong>16-31 MB</strong></td> <td><strong>General-purpose, production-ready</strong></td> </tr> <tr> <td><strong>Docling</strong></td> <td>\u26a1 Fast (3.1s/pg x86, 1.27s/pg ARM)</td> <td>Best</td> <td>7+</td> <td>1-9.74 GB</td> <td>Complex documents, when accuracy &gt; size</td> </tr> <tr> <td><strong>GROBID</strong></td> <td>\u26a1\u26a1 Very Fast (10.6 PDF/s)</td> <td>Best</td> <td>PDF only</td> <td>0.5-8 GB</td> <td><strong>Academic/scientific papers only</strong></td> </tr> <tr> <td><strong>Unstructured</strong></td> <td>\u26a1 Moderate</td> <td>Good</td> <td>25-65+</td> <td>146 MB-several GB</td> <td>Python-native LLM pipelines</td> </tr> <tr> <td><strong>MarkItDown</strong></td> <td>\u26a1 Fast (small files)</td> <td>Good</td> <td>11+</td> <td>~251 MB</td> <td><strong>Lightweight Markdown conversion</strong></td> </tr> <tr> <td><strong>Apache Tika</strong></td> <td>\u26a1 Moderate</td> <td>Excellent</td> <td><strong>1000+</strong></td> <td>~55 MB</td> <td>Enterprise, broadest format support</td> </tr> </tbody></table> <p><strong>Kreuzberg's sweet spot:</strong> - <strong>Smallest full-featured installation</strong>: 16-31 MB complete (vs 146 MB-9.74 GB for competitors) - <strong>5-15x smaller</strong> than Unstructured/MarkItDown, <strong>30-300x smaller</strong> than Docling/GROBID - <strong>Rust-native performance</strong> without ML model overhead - <strong>Broad format support</strong> (56+ formats) with native parsers - <strong>Multi-language support</strong> unique in the space (7 languages vs Python-only for most) - <strong>Production-ready</strong> with general-purpose design (vs specialized tools like GROBID)</p> <h2>Is Kreuzberg a SaaS Product?</h2> <p><strong>No.</strong> Kreuzberg is and will remain <strong>MIT-licensed open source</strong>.</p> <p>However, we are building <strong>Kreuzberg.cloud</strong> - a commercial SaaS and self-hosted document intelligence solution built <em>on top of</em> Kreuzberg. This follows the proven open-core model: the library stays free and open, while we offer a cloud service for teams that want managed infrastructure, APIs, and enterprise features.</p> <p><strong>Will Kreuzberg become commercially licensed?</strong> Absolutely not. There is no BSL (Business Source License) in Kreuzberg's future. The library was MIT-licensed and will remain MIT-licensed. We're building the commercial offering as a separate product around the core library, not by restricting the library itself.</p> <h2>Target Audience</h2> <p>Any developer or data scientist who needs: - Document text extraction (PDF, Office, images, email, archives, etc.) - OCR (Tesseract, EasyOCR, PaddleOCR) - Metadata extraction (authors, dates, properties, EXIF) - Table and image extraction - Document pre-processing for RAG pipelines - Text chunking with embeddings - Token reduction for LLM context windows - Multi-language document intelligence in production systems</p> <p><strong>Ideal for:</strong> - RAG application developers - Data engineers building document pipelines - ML engineers preprocessing training data - Enterprise developers handling document workflows - DevOps teams needing lightweight, performant extraction in containers/serverless</p> <h2>Comparison with Alternatives</h2> <h3>Open Source Python Libraries</h3> <p><strong>Unstructured.io</strong> - <strong>Strengths</strong>: Established, modular, broad format support (25+ open source, 65+ enterprise), LLM-focused, good Python ecosystem integration - <strong>Trade-offs</strong>: Python GIL performance constraints, 146 MB minimal installation (several GB with ML models) - <strong>License</strong>: Apache-2.0 - <strong>When to choose</strong>: Python-only projects where ecosystem fit &gt; performance</p> <p><strong>MarkItDown (Microsoft)</strong> - <strong>Strengths</strong>: Fast for small files, Markdown-optimized, simple API - <strong>Trade-offs</strong>: Limited format support (11 formats), less structured metadata, ~251 MB installed (despite small wheel), requires OpenAI API for images - <strong>License</strong>: MIT - <strong>When to choose</strong>: Markdown-only conversion, LLM consumption</p> <p><strong>Docling (IBM)</strong> - <strong>Strengths</strong>: Excellent accuracy on complex documents (97.9% cell-level accuracy on tested sustainability report tables), state-of-the-art AI models for technical documents - <strong>Trade-offs</strong>: Massive installation (1-9.74 GB), high memory usage, GPU-optimized (underutilized on CPU) - <strong>License</strong>: MIT - <strong>When to choose</strong>: Accuracy on complex documents &gt; deployment size/speed, have GPU infrastructure</p> <h3>Open Source Java/Academic Tools</h3> <p><strong>Apache Tika</strong> - <strong>Strengths</strong>: Mature, stable, broadest format support (1000+ types), proven at scale, Apache Foundation backing - <strong>Trade-offs</strong>: Java/JVM required, slower on large files, older architecture, complex dependency management - <strong>License</strong>: Apache-2.0 - <strong>When to choose</strong>: Enterprise environments with JVM infrastructure, need for maximum format coverage</p> <p><strong>GROBID</strong> - <strong>Strengths</strong>: Best-in-class for academic papers (F1 0.87-0.90), extremely fast (10.6 PDF/sec sustained), proven at scale (34M+ documents at CORE) - <strong>Trade-offs</strong>: Academic papers only, large installation (500MB-8GB), complex Java+Python setup - <strong>License</strong>: Apache-2.0 - <strong>When to choose</strong>: Scientific/academic document processing exclusively</p> <h3>Commercial APIs</h3> <p>There are numerous commercial options from startups (LlamaIndex, Unstructured.io paid tiers) to big cloud providers (AWS Textract, Azure Form Recognizer, Google Document AI). These are not OSS but offer managed infrastructure.</p> <p><strong>Kreuzberg's position</strong>: As an open-source library, Kreuzberg provides a self-hosted alternative with no per-document API costs, making it suitable for high-volume workloads where cost efficiency matters.</p> <h2>Community &amp; Resources</h2> <ul> <li><strong>GitHub</strong>: Star us at <a href=\"https://github.com/kreuzberg-dev/kreuzberg\">https://github.com/kreuzberg-dev/kreuzberg</a></li> <li><strong>Discord</strong>: Join our community server at <a href=\"https://discord.gg/pXxagNK2zN\">discord.gg/pXxagNK2zN</a></li> <li><strong>Subreddit</strong>: Join the discussion at <a href=\"https://www.reddit.com/r/kreuzberg_dev/\">r/kreuzberg_dev</a></li> <li><strong>Documentation</strong>: <a href=\"https://kreuzberg.dev\">kreuzberg.dev</a></li> </ul> <p>We'd love to hear your feedback, use cases, and contributions!</p> <hr /> <p><strong>TL;DR</strong>: Kreuzberg v4 is a complete Rust rewrite of a document intelligence library, offering native bindings for 7 languages (8 runtime targets), 56+ file formats, Rust-native performance, embeddings, semantic chunking, and production-ready servers - all in a 16-31 MB complete package (5-15x smaller than alternatives). Releasing January 2025. MIT licensed forever.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Goldziher\"> /u/Goldziher </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1pn2a3t/kreuzberg_v400rc8_is_available/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1pn2a3t/kreuzberg_v400rc8_is_available/\">[comments]</a></span>",
      "author": "/u/Goldziher",
      "published_date": "2025-12-15T08:21:10+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2105,
      "reading_time": 10,
      "created_at": "2025-12-15T10:27:42.018937+00:00",
      "updated_at": "2025-12-15T10:27:42.018944+00:00"
    },
    {
      "id": "7857c57cc337946dac855994dd3c0343",
      "url": "https://exclav.es/2025/08/03/dinacon-2025-passive-acoustic-listening/",
      "title": "$5 whale listening hydrophone making workshop",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46229030\">Comments</a>",
      "author": "",
      "published_date": "2025-12-11T08:42:22+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-12-15T09:33:15.690932+00:00",
      "updated_at": "2025-12-15T10:20:38.272487+00:00",
      "metadata": {
        "processed_at": "2025-12-15T10:20:38.272496+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}