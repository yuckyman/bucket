{
  "last_updated": "2026-01-22T22:18:20.085357+00:00",
  "count": 20,
  "articles": [
    {
      "id": "560b53b8a9f1b10c46ba5f7d619d4dec",
      "url": "http://ieeexplore.ieee.org/document/11164370",
      "title": "Drawing the Line: Wearable Linear Haptics Motivated by Guided Breathing",
      "content": "Haptic wearables provide an intuitive human-machine interface to convey information through the sense of touch, which may have promising applications in guided breathing. In this paper, we detail the design and evaluation of three wearable prototypes (Vibration, Skin Drag, and Tapping) capable of administering discrete (individual, separate pulses and stimuli). and continuous (overlapping or uninterrupted stimuli) forms of linear haptic cycles with inspiration from slow, deep guided breathing. Characterization was performed to quantify and validate the performance of six haptic stimuli (discrete/continuous vibration, skin drag, and tapping). Devices were quantified with key metrics that described the applied stimuli and the dynamics of the wearable. A human subjects study (N = 25), composed of two-cycle tracking tasks, was conducted to determine device performance and user aptitude. Results indicated consistent directional recognition across all six stimuli, but discrete stimuli performed better in spatial localization tasks. Although outperformed in tracking/localization tasks, continuous stimuli, especially skin drag, were described as the most apt and intuitive pairing to guided breathing. Findings highlight the potential of these linear haptic stimuli in a number of applications, including guided breathing, navigation, virtual immersion, and communication.",
      "author": "",
      "published_date": "2025-09-15T13:17:38+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 187,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:21.905960+00:00",
      "updated_at": "2026-01-22T22:18:19.978824+00:00",
      "metadata": {
        "processed_at": "2026-01-22T22:18:19.978834+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "273f5b74995aee1a22f54d9a1ad2625a",
      "url": "http://ieeexplore.ieee.org/document/11145272",
      "title": "Haptics of Pulse Palpation: Simulation and Validation Through Novel Sensor-Actuator System",
      "content": "Palpation of arteries holds significant physiological importance. Existing pulse actuator designs intended to replicate the haptic sensations of palpation primarily focus on normal force interactions, often overlooking the shear forces generated by oscillations of the arterial wall during blood flow. This study aims to evaluate the normal, longitudinal, and transverse forces exerted by arteries through both theoretical and experimental analyses during palpation. The experimental validation features a pulse actuator-sensor system. The actuator component is a hydroelectromagnetic actuator, while the haptic sensing is performed by the Subblescope. The Subblescope measures arterial force feedback from both soft and hard artery models, as well as from the radial pulse in 18 human subjects. Mathematical analysis establishes the operational range of the sensor-actuator system as 0.005 N to 2.5 N. The force feedback from the simulation has been used for designing the total force generation by the actuator. The reactive force along the Z-axis varies between 19.3 mN to 500 mN, while the transverse and longitudinal forces along the Y and X axes range from 6.9 mN to 88.01 mN and 5.46 mN to 87.85 mN, respectively. The pulse-force map of the hard artery reveals higher three-dimensional force interactions compared to the soft artery. The hydroelectromagnetic actuator effectively generates both normal and shear forces during pulsatile flow. Future work will focus on developing training modules that replicate pulse haptics associated with various physiological conditions such as diabetes.",
      "author": "",
      "published_date": "2025-08-29T13:18:28+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 233,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:21.905928+00:00",
      "updated_at": "2026-01-22T22:18:19.978838+00:00",
      "metadata": {
        "processed_at": "2026-01-22T22:18:19.978840+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "053a64c388be11291b58cb5dddb68ec6",
      "url": "http://ieeexplore.ieee.org/document/11303561",
      "title": "Scene-sensitive Medial Temporal Lobe Subregions Are Recruited for the Integration of Non-scene Stimuli",
      "content": "A hallmark feature of episodic memory is the ability to flexibly recombine information across episodes to form new associations and guide behavior. This process, termed associative inference, relies on the hippocampus and surrounding medial temporal lobe (MTL) subregions. We previously found that cross-episode binding was improved when episodes were linked by scenes rather than by faces or objects. Here, we tested whether differential recruitment of category-sensitive MTL subregions underlies these behavioral differences. Participants completed study-test phases of the Associative Inference in Memory task while undergoing fMRI scanning. During the study phase, they encoded overlapping AB and BC pairs. A and C items were always objects. The linking B item was either a face or a scene. At test, memory for the direct (AB, BC) and indirect associations (inferred AC) was tested. Category sensitivity in MTL subregions was tested using an independent functional localizer and the low integration (AB) trials from the study phase of the Associative Inference in Memory task. Within the MTL, no subregions exhibited face sensitivity. The anterior hippocampal head, anterolateral and posteromedial entorhinal cortices, and parahippocampal cortex were identified as scene sensitive. Although accuracy of the indirect inferences did not differ between pairs linked by faces and scenes, MTL subregion recruitment differed across categories. Scene-sensitive subregions in MTL cortex (anterolateral entorhinal cortex, posteromedial entorhinal cortex, and parahippocampal cortex), but not the hippocampus (anterior hippocampal head), were recruited to support associative inference for faces during encoding. These findings suggest that regions in MTL cortex identified as scene sensitive here may be involved in integrating disparate elements of episodes into coherent representations, and may be recruited for non-scene stimuli when integration demands during encoding are high (e.g., during associative inference).",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 281,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:17.182260+00:00",
      "updated_at": "2026-01-22T22:18:19.978842+00:00",
      "metadata": {
        "processed_at": "2026-01-22T22:18:19.978844+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "d07d9c88cf921ac53c3b05c1709a7d87",
      "url": "http://ieeexplore.ieee.org/document/11303567",
      "title": "Increasing Signal, Reducing Noise: Contrasting Neural Mechanisms of Attention in Visual Search",
      "content": "When invariant target\u2013distractor arrays are presented repeatedly during visual search, participants respond faster on repeated versus novel configuration trials. This effect reflects attentional guidance through long-term memory (LTM) templates\u2014a phenomenon termed contextual cueing. Subsequently, relocating the target within the same distractor layout abolishes any contextual cueing effects, and relearning the new target position is much harder than the initial learning\u2014likely due to consistent attentional misguidance toward the initial (learned) target position. Here, we studied how the different processes involved in contextual cueing and relearning affect the variability of neural responses in human participants as measured with EEG. Attention has previously been shown to reduce trial-by-trial variability in EEG [Arazi, A., Yeshurun, Y., & Dinstein, I. Neural variability is quenched by attention. Journal of Neuroscience, 39, 5975\u20135985, 2019], indicating that, in addition to increasing the neural response to an attended stimulus, attention may reduce the noise within the neural response itself. While repeated versus novel contexts did not modulate the trial-by-trial variability during initial learning, significant lateralized variability reductions were observed for repeated but not novel context trials in the relocations phase. This contrasts with how contextual cueing affected lateralized ERPs in past research. Zinchenko and colleagues [Zinchenko, A., Conci, M., T\u00f6llner, T., M\u00fcller, H. J., & Geyer, T. Automatic guidance (and misguidance) of visuospatial attention by acquired scene memory: Evidence from an N1pc polarity reversal. Psychological Science, 31, 1531\u20131543, 2020] found that lateralized ERPs signal correct and incorrect (i.e., misguided) attentional selection of target positions learned earlier. This phenomenon was observed during both the learning and relocation phases. Thus, variability and lateralized ERPs may represent different facets of attention, where variability becomes evident specifically under high attentional demand conditions, such as when participants must override the misguidance caused by LTM templates.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 291,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:17.182221+00:00",
      "updated_at": "2026-01-22T22:18:19.978846+00:00",
      "metadata": {
        "processed_at": "2026-01-22T22:18:19.978848+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f95a9f64dc97257327f12c06475d3643",
      "url": "http://ieeexplore.ieee.org/document/11303565",
      "title": "The Neural Bases of Graphical Perception: A Novel Instance of Cultural Recycling?",
      "content": "Graphical representations of quantitative data abound in our culture, and yet the brain mechanisms of graphicacy, by which viewers quickly extract statistical information from a data graphic, are unknown. Here, using scatterplots as stimuli, we tested two hypotheses about the brain areas underlying graphicacy. First, at the perceptual level, we hypothesized that the visual processing of scatterplots and their main trend recycles cortical regions devoted to the perception of the principal axis of objects. Second, at a higher level, we speculated that the math-responsive network active during arithmetic and mathematical truth judgments should also be involved in graphical perception. Using fMRI, we indeed found that the judgment of the trend in a scatterplot recruits a right lateral occipital area involved in detecting the orientation of objects, as well as a right anterior intraparietal region also recruited during mathematical tasks. Both behavior and brain activity were driven by the t value that indexes the statistical correlation in the data, and right intraparietal activation covaried with participants' graphicacy level. On the basis of this first approach to the neural bases of graphical perception, we suggest that, like literacy and numeracy, graphicacy relies on the recycling of brain areas previously attuned to a similar problem, here the perception of object orientation.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 208,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:17.182171+00:00",
      "updated_at": "2026-01-22T22:18:19.978850+00:00",
      "metadata": {
        "processed_at": "2026-01-22T22:18:19.978852+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "cfdf4cdfc85c3633062d4f70f1a372c0",
      "url": "http://ieeexplore.ieee.org/document/11303571",
      "title": "Neural Evidence for Tonal Prediction: Multivariate Decoding of Predicted Tone Categories Using Functional Magnetic Resonance Imaging Data",
      "content": "Predictive processing plays a central role in language comprehension, allowing listeners to generate predictions about upcoming linguistic input. Although considerable evidence supports segmental prediction, less is known about whether listeners can form predictions about suprasegmental features such as lexical tone. This study investigates whether listeners can generate and neurally represent predicted tonal information in the absence of auditory input. Using a Mandarin Chinese tone sandhi paradigm, we established tonal predictions based on sentence and visual context, recording brain activity with functional magnetic resonance imaging. Multivariate pattern analysis showed that predicted tonal categories could be decoded from brain activity even without tonal stimuli present. These representations were localized in auditory areas, articulatory motor regions, and the right cerebellum. We also found that predicted tone representations had distinct neural substrates compared to perceived tone representations. The study provides direct neural evidence that listeners can form representations of lexical tone in predictions of auditory input. The findings expand our understanding of suprasegmental prediction in speech and highlight the cerebellum's role in linguistic prediction.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 170,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:17.182138+00:00",
      "updated_at": "2026-01-22T21:48:17.182139+00:00"
    },
    {
      "id": "301d3b51ba3b7d4480873efc4d9dc87b",
      "url": "http://ieeexplore.ieee.org/document/11303645",
      "title": "Typical Perceptual Sensitivity to Changes in Interpersonal Distance in Developmental Prosopagnosia",
      "content": "Social perception research has traditionally sought to elucidate the visual processing engaged by the faces and bodies of individuals. Recently, however, there has been growing interest in how we perceive dyadic interactions between people. Early findings suggest that dyads arranged face-to-face may engage neurocognitive processing similar to that recruited by faces. Given these parallels, we sought to determine whether individuals with developmental prosopagnosa (DP), who exhibit lifelong face recognition difficulties, also exhibit impaired perception of facing dyads. The focus of our investigation was interpersonal distance\u2014a key visual feature of dyadic social interactions. Participants completed three distance change detection tasks. Two of the tasks depicted distance changes during dyadic social interactions (fighting and dancing). A third task depicted distance changes using nonsocial objects (a pair of grandfather clocks). If DP is associated with impoverished perception of dyadic interactions, we reasoned that individuals with DP should exhibit diminished sensitivity to distance changes on the dancers task and the boxers task, but not on the clocks task. Contrary to this prediction, however, individuals with DP and typical controls did not differ significantly in their ability to detect distance changes on any of the tasks. Although the visual processing of faces and facing dyads exhibit certain similarities, these findings suggest that the underlying perceptual mechanisms may dissociate.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 213,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:17.182104+00:00",
      "updated_at": "2026-01-22T21:48:17.182106+00:00"
    },
    {
      "id": "e3ec69f3c8697bf33383e62c0473d5c0",
      "url": "http://ieeexplore.ieee.org/document/11303570",
      "title": "The Interactive Effects of Negative Emotion and Reward Motivation on Visual Perception",
      "content": "Although there is a rapidly growing interest in reward\u2013emotion interactions, our current understanding of how negative emotion influences reward motivation and modulates reward-driven enhancements in visual perception remains limited. To address these gaps, we conducted an fMRI study using a novel variant of the monetary incentive delay task where the valence (negative or neutral) of an emotional scene image served as a cue to indicate a reward or no-reward prospect in the subsequent house\u2013building discrimination task. During the initial cue stage, we hypothesized competitive interactions between reward anticipation and negative emotion along the common value/valence dimension. However, we instead found independent neural signatures of reward (vs. no-reward) anticipation in the ventral striatum and negative (vs. neutral) emotion in the ventromedial pFC and amygdala, with a lack of evidence for their interaction. Notably, during the subsequent task stage, we detected an Emotion \u00d7 Reward interaction in the parahippocampal gyrus (PHG), wherein reward-driven enhancements in task-related processing were attenuated in the case of negative (vs. neutral) cue images. Furthermore, the Emotion \u00d7 Reward interaction scores in PHG and behavioral RTs were correlated across participants. Finally, a regression analysis revealed that negative valence-related activity in ventromedial pFC moderated the relationship between ventral striatum reward anticipation activity and PHG task-related processing. These findings demonstrate that negative emotion and reward motivation, which were largely segregated during the cue stage, interactively modulated subsequent visual perception, thus potentially influencing behavior.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 233,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:17.182065+00:00",
      "updated_at": "2026-01-22T21:48:17.182067+00:00"
    },
    {
      "id": "75ea4dfc0e3afdf03fcdbd572a25e70e",
      "url": "http://ieeexplore.ieee.org/document/11303560",
      "title": "Suppressive Interactions between Nearby Stimuli in Visual Cortex Reflect Crowding",
      "content": "Crowding is a phenomenon in which visual object identification is impaired by the close proximity of other stimuli. The neural processes leading to object recognition and its breakdown as seen in crowding are still debated. To assess how crowding affects the processing of stimuli in visual cortex, we recorded steady-state visual evoked potentials (SSVEPs) elicited by flickering target and flanker stimuli while manipulating the spacing of these stimuli (Experiment 1) as well as target similarity (Experiment 2). Participants who performed an orientation discrimination task while proportion correct, along with frequency-tagged SSVEPs elicited by target and flanker stimuli, were recorded. Decreasing target\u2013flanker distance reduced both behavioral performance and target-elicited SSVEP amplitudes. Estimates of the critical spacing, a measure of the spatial extent of crowding, from both behavioral data and SSVEP amplitudes were similar. In addition, manipulating target similarity affected both measures in the same way. These findings establish a clear connection between the suppression of stimulus processing by nearby flankers in visual cortex and crowding, and demonstrate the usefulness of SSVEPs in studying the cortical mechanisms of visual crowding.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 178,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:17.182014+00:00",
      "updated_at": "2026-01-22T21:48:17.182019+00:00"
    },
    {
      "id": "91f6c782e21b9da934fcd29f01d2dfa0",
      "url": "https://www.embs.org/featured-news/welcoming-congratulating-our-newly-elected-embs-excom-members/",
      "title": "Welcoming & Congratulating Our Newly Elected EMBS ExCom Members",
      "content": "<p>The post <a href=\"https://www.embs.org/featured-news/welcoming-congratulating-our-newly-elected-embs-excom-members/\">Welcoming &#038; Congratulating Our Newly Elected EMBS ExCom Members</a> appeared first on <a href=\"https://www.embs.org\">IEEE EMBS</a>.</p>",
      "author": "Nancy Zimmerman",
      "published_date": "2025-12-14T18:48:47+00:00",
      "source": "Embs",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 18,
      "reading_time": 1,
      "created_at": "2026-01-22T21:48:15.969684+00:00",
      "updated_at": "2026-01-22T21:48:15.969689+00:00"
    },
    {
      "id": "b69b45fa94105d00264fbaf74837235f",
      "url": "https://news.ycombinator.com/item?id=46721933",
      "title": "Launch HN: Constellation Space (YC W26) \u2013 AI for satellite mission assurance",
      "content": "<p>Hi HN! We're Kamran, Raaid, Laith, and Omeed from Constellation Space (<a href=\"https://constellation-io.com/\">https://constellation-io.com/</a>). We built an AI system that predicts satellite link failures before they happen. Here's a video walkthrough: <a href=\"https://www.youtube.com/watch?v=069V9fADAtM\" rel=\"nofollow\">https://www.youtube.com/watch?v=069V9fADAtM</a>.<p>Between us, we've spent years working on satellite operations at SpaceX, Blue Origin, and NASA. At SpaceX, we managed constellation health for Starlink. At Blue, we worked on next-gen test infra for New Glenn. At NASA, we dealt with deep space communications. The same problem kept coming up: by the time you notice a link is degrading, you've often already lost data.<p>The core issue is that satellite RF links are affected by dozens of interacting variables. A satellite passes overhead, and you need to predict whether the link will hold for the next few minutes. That depends on: the orbital geometry (elevation angle changes constantly), tropospheric attenuation (humidity affects signal loss via ITU-R P.676), rain fade (calculated via ITU-R P.618 - rain rates in mm/hr translate directly to dB of loss at Ka-band and above), ionospheric scintillation (we track the KP index from magnetometer networks), and network congestion on top of all that.<p>The traditional approach is reactive. Operators watch dashboards, and when SNR drops below a threshold, they manually reroute traffic or switch to a backup link. With 10,000 satellites in orbit today and 70,000+ projected by 2030, this doesn't scale.\nOur system ingests telemetry at around 100,000 messages per second from satellites, ground stations, weather radar, IoT humidity sensors, and space weather monitors. We run physics-based models in real-time - the full link budget equations, ITU atmospheric standards, orbital propagation - to compute what should be happening. Then we layer ML models on top, trained on billions of data points from actual multi-orbit operations.<p>The ML piece is where it gets interesting. We use federated learning because constellation operators (understandably) don't want to share raw telemetry. Each constellation trains local models on their own data, and we aggregate only the high-level patterns. This gives us transfer learning across different orbit types and frequency bands - learnings from LEO Ka-band links help optimize MEO or GEO operations.\nWe can predict most link failures 3-5 minutes out with >90% accuracy, which gives enough time to reroute traffic before data loss. The system is fully containerized (Docker/Kubernetes) and deploys on-premise for air-gapped environments, on GovCloud (AWS GovCloud, Azure Government), or standard commercial clouds.<p>Right now we're testing with defense and commercial partners. The dashboard shows real-time link health, forecasts at 60/180/300 seconds out, and root cause analysis (is this rain fade? satellite setting below horizon? congestion?). We expose everything via API - telemetry ingestion, predictions, topology snapshots, even an LLM chat endpoint for natural language troubleshooting.<p>The hard parts we're still working on: prediction accuracy degrades for longer time horizons (beyond 5 minutes gets dicey), we need more labeled failure data for rare edge cases, and the federated learning setup requires careful orchestration across different operators' security boundaries.\nWe'd love feedback from anyone who's worked on satellite ops, RF link modeling, or time-series prediction at scale. What are we missing? What would make this actually useful in a production NOC environment?<p>Happy to answer any technical questions!</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46721933\">https://news.ycombinator.com/item?id=46721933</a></p>\n<p>Points: 26</p>\n<p># Comments: 5</p>",
      "author": "kmajid",
      "published_date": "2026-01-22T17:03:21+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 534,
      "reading_time": 2,
      "created_at": "2026-01-22T21:47:21.243955+00:00",
      "updated_at": "2026-01-22T21:47:21.243957+00:00"
    },
    {
      "id": "065cbccd797ff5492f1f62416896e099",
      "url": "https://github.com/21st-dev/1code",
      "title": "Show HN: First Claude Code client for Ollama local models",
      "content": "<p>Just to clarify the background a bit. This project wasn\u2019t planned as a big standalone release at first. On January 16, Ollama added support for an Anthropic-compatible API, and I was curious how far this could be pushed in practice. I decided to try plugging local Ollama models directly into a Claude Code-style workflow and see if it would actually work end to end.<p>Here is the release note from Ollama that made this possible: <a href=\"https://ollama.com/blog/claude\">https://ollama.com/blog/claude</a><p>Technically, what I do is pretty straightforward:<p>- Detect which local models are available in Ollama.<p>- When internet access is unavailable, the client automatically switches to Ollama-backed local models instead of remote ones.<p>- From the user\u2019s perspective, it is the same Claude Code flow, just backed by local inference.<p>In practice, the best-performing model so far has been qwen3-coder:30b. I also tested glm-4.7-flash, which was released very recently, but it struggles with reliably following tool-calling instructions, so it is not usable for this workflow yet.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46722285\">https://news.ycombinator.com/item?id=46722285</a></p>\n<p>Points: 14</p>\n<p># Comments: 5</p>",
      "author": "SerafimKorablev",
      "published_date": "2026-01-22T17:26:12+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 169,
      "reading_time": 1,
      "created_at": "2026-01-22T21:47:21.243885+00:00",
      "updated_at": "2026-01-22T21:47:21.243887+00:00"
    },
    {
      "id": "bcb6b72cd62b8e161b8fb3f40caa20b0",
      "url": "http://ieeexplore.ieee.org/document/11125831",
      "title": "HapticGiant: A Novel Very Large Kinesthetic Haptic Interface With Hierarchical Force Control",
      "content": "Research in virtual reality and haptic technologies has consistently aimed to enhance immersion. While advanced head-mounted displays are now commercially available, kinesthetic haptic interfaces still face challenges such as limited workspaces, insufficient degrees of freedom, and kinematics not matching the human arm. In this paper, we present HapticGiant, a novel large-scale kinesthetic haptic interface designed to match the properties of the human arm as closely as possible and to facilitate natural user locomotion while providing full haptic feedback. The interface incorporates a novel admittance-type force control scheme, leveraging hierarchical optimization to render both arbitrary serial kinematic chains and Cartesian admittances. Notably, the proposed control scheme natively accounts for system limitations, including joint and Cartesian constraints, as well as singularities. Experimental results demonstrate the effectiveness of HapticGiant and its control scheme, paving the way for highly immersive virtual reality applications.",
      "author": "",
      "published_date": "2025-08-14T13:17:43+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 139,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:44.718644+00:00",
      "updated_at": "2026-01-22T21:27:44.718645+00:00"
    },
    {
      "id": "76c23c5300399082bb749ca190929d23",
      "url": "http://ieeexplore.ieee.org/document/11130394",
      "title": "\u201cPersuasive Vibrations\u201d: Studying the Influence of Vibration Parameters on Speech Persuasion",
      "content": "This paper investigates the notion of \u201cPersuasive Vibrations\u201d, which showed that augmenting a person's speech with vibrotactile feedback could artificially increase persuasion. However, while the initial paper has shown the effect, the underlying reasons why vibrations enhance persuasion remain unknown. Through two different user studies, this paper aims to study how the underlying parameters of the vibratory feedback (e.g., frequency, amplitude, or audio-vibration synchronization) influence persuasion. The first study aimed to identify the parameters of vibrotactile feedback that can positively influence persuasion. The second study evaluated vibrotactile feedback that might impair the persuasive effect. In a nutshell, the first experiment suggests that the isolation of different properties of the vibratory signal could tend to provide higher persuasion compared to no vibratory feedback. A lower frequency at 100 Hz seems the most efficient way to generate a persuasive effect. In contrast, the second experiment suggests that some alteration of the vibratory signal (e.g., latency) does not decrease the levels of persuasion compared to the no-vibration condition. All in all, the results suggest that using lower frequencies could have a better effect on persuasion. These results could serve as a basis for haptic design in applications like videoconferencing, virtual meetings, and training systems where supporting user speech is essential.",
      "author": "",
      "published_date": "2025-08-19T13:17:09+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 207,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:44.718614+00:00",
      "updated_at": "2026-01-22T21:27:44.718616+00:00"
    },
    {
      "id": "23dd8d4f1e90a8b410a6b21713fcf022",
      "url": "http://ieeexplore.ieee.org/document/11121155",
      "title": "A Survey on Tactile Change Blindness",
      "content": "While vibrotactile displays continue to gain popularity, it remains that the phenomenon of tactile change blindness negatively impacts the human ability to detect changes between and within tactile signals. This paper surveys the research literature on tactile change detection and blindness under various parameters, including the number of tactors used, the intensity and length of the stimulus, and whether distractors between stimuli (i.e., transients) were used during experimentation, among others. The goal of this survey is to summarize what has been done in an attempt to better understand the parameters that exacerbate tactile change blindness and identify potential areas of future research. When such an understanding is reached, the design of haptic and multimodal displays may ideally be improved.",
      "author": "",
      "published_date": "2025-08-08T13:16:47+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 119,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:44.718572+00:00",
      "updated_at": "2026-01-22T21:27:44.718574+00:00"
    },
    {
      "id": "87344862fd4253ecc468dd068b2c0436",
      "url": "http://ieeexplore.ieee.org/document/11284873",
      "title": "From Restoration to Augmentation: New Approaches to Haptic Feedback for Artificial Limbs",
      "content": "Haptic feedback is essential for precise motor control, making its integration into artificial limbs a critical design challenge. Current approaches to restoring lost tactile input in patients have focused on interfacing with somatosensory pathways at the brain, nerve, or skin level, achieving partial restoration. However, augmentative artificial limbs, devices that provide novel movement capabilities beyond the biological body, pose unique challenges. These limbs lack dedicated sensory pathways, raising fundamental questions about how to deliver tactile feedback for these devices without disrupting existing somatosensory function. A promising direction lies in exploiting intrinsic tactile feedback, which emerges naturally at the interface between wearable devices and the body. When an artificial limb moves or interacts with objects, the skin detects rich tactile cues transmitted through this interface. Amplifying and refining this intrinsic feedback, via materials optimized for transmission of tactile signals and wearable designs, could enable more intuitive and interpretable haptic feedback for augmentative limbs. This approach offers a pathway toward enhancing embodiment and motor control of augmentative artificial limbs.",
      "author": "",
      "published_date": "2025-12-08T13:16:55+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:44.718543+00:00",
      "updated_at": "2026-01-22T21:27:44.718545+00:00"
    },
    {
      "id": "0e26a7e93d9b4a75df2d60781a3dcc65",
      "url": "http://ieeexplore.ieee.org/document/11264420",
      "title": "Recent Achievements of Electrotactile Displays in IEEE Transactions on Haptics",
      "content": "Electrotactile displays are a promising technology that combines the simplicity of implementation using only electronic circuits with the flexibility to deliver tactile sensations across many body sites. In recent years, their applications and research have rapidly advanced. This article provides an overview of studies published in IEEE Transactions on Haptics, covering diverse aspects such as application domains of electrotactile stimulation, techniques for stabilizing percepts, methods for efficient information rendering, and the use of electrotactile displays as tools for investigating human tactile perception. Through this review, the engineering and scientific potential of electrotactile interfaces is highlighted, along with prospects for realizing future tactile displays that are low-cost, high-density, and large-area.",
      "author": "",
      "published_date": "2025-11-21T13:16:44+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 109,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:44.718507+00:00",
      "updated_at": "2026-01-22T21:27:44.718509+00:00"
    },
    {
      "id": "c578348c6b9165044969f256985bad4c",
      "url": "http://ieeexplore.ieee.org/document/11339382",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2026-01-08T13:16:13+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:44.718474+00:00",
      "updated_at": "2026-01-22T21:27:44.718476+00:00"
    },
    {
      "id": "6174f202539bb198b67816a4e3eb5995",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.21.700689v1?rss=1",
      "title": "Multi-modal and multi-region distance model for neuroimaging: Application to ABCD study",
      "content": "Large-scale neuroimaging studies often collect multiple modalities, such as task and resting-state functional MRI, diffusion MRI, and structural MRI. Joint inference across these modalities uses shared variation to improve statistical efficiency, increase replicability, and provide a more integrated view of brain-phenotype associations. In practice, however, such analyses are limited because complex cross-modality covariance cannot be flexibly modeled, which makes the resulting joint effects difficult to interpret. A recent distance-based ANOVA extension allows multimodal analysis and increases power for detecting group differences, but it cannot easily distinguish location from scale effects in distance space, offers only an omnibus pseudo-F test without interpretable parameters, and requires computationally intensive permutation inference. We propose a novel semiparametric, U-statistics-based Generalized Estimating Equation (UGEE) framework that unifies univariate and multivariate distance models. By regressing pairwise dissimilarities on covariates, this method yields interpretable regression coefficients that disentangle location and scale effects and quantify inter-modality differences, while flexibly accounting for correlations among modality distances. The estimator is based on efficient influence functions, ensuring asymptotic efficiency, robustness to misspecification, and computational scalability for large-scale data analysis. We evaluate the proposed method through extensive simulations and analyses of the Adolescent Brain Cognitive Development dataset. Results show that UGEE accurately estimates modality, group, and interaction effects and achieves a 100-fold speed-up compared with permutation-based approaches. This framework provides a general and computationally efficient tool for semiparametric inference on multimodal data, particularly suited for large neuroimaging applications.",
      "author": "Zhang, X., Vandekar, S., Chen, A. A., Kang, K., Seidlitz, J., Alexander-Bloch, A., Liu, J.",
      "published_date": "2026-01-22T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 235,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:27.600323+00:00",
      "updated_at": "2026-01-22T21:27:27.600325+00:00"
    },
    {
      "id": "d19080d1470cb2b44b481248991c4658",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.21.700878v1?rss=1",
      "title": "PRISME: A MATLAB Toolbox For Large Data-Driven Multimodal Power Benchmarking",
      "content": "Low statistical power in neuroimaging often undermines research in the field, leading to missed effects, wasted resources, and reduced reproducibility. Performing power analyses during the study design phase is extremely important, but often prohibitively difficult due to a lack of analytical solutions and high computational costs. We present PRISME (Power Resampling Infrastructure for Statistical Method Evaluation), a MATLAB toolbox for neuroimaging power benchmarking. PRISME provides a computational framework for empirical power analysis independent of inference methods, enabling large scale power benchmarking and method comparison.The toolbox supports diverse neuroimaging data types, including both voxel-based activation and functional connectivity analyses, with a non-parametric, flexible algorithm and unified data representations. Furthermore, unlike previous empirical power approaches, PRISME supports multiple test types, such as association and difference tests with behavioral and clinical measures. Finally, PRISME's $25times$ speedup from algorithmic optimizations enables larger-scale power benchmarking, including the first power analysis for the ABCD dataset. Overall, PRISME is the first method- and data-type-agnostic power benchmarking tool for neuroimaging, providing a single solution for power analysis across diverse study designs.",
      "author": "Cravo, F., Fischbach, A., Shearer, H., Rosenblatt, M., Scheinost, D., Noble, S.",
      "published_date": "2026-01-22T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 173,
      "reading_time": 1,
      "created_at": "2026-01-22T21:27:27.600283+00:00",
      "updated_at": "2026-01-22T21:27:27.600285+00:00"
    }
  ]
}