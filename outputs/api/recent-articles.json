{
  "last_updated": "2025-10-06T04:23:43.343815+00:00",
  "count": 20,
  "articles": [
    {
      "id": "78a543035cc17e2ac6f73ecd95cce1ae",
      "url": "http://ieeexplore.ieee.org/document/10750441",
      "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities and Future Directions",
      "content": "Foundation model, trained on a diverse range of data and adaptable to a myriad of tasks, is advancing healthcare. It fosters the development of healthcare artificial intelligence (AI) models tailored to the intricacies of the medical field, bridging the gap between limited AI models and the varied nature of healthcare practices. The advancement of a healthcare foundation model (HFM) brings forth tremendous potential to augment intelligent healthcare services across a broad spectrum of scenarios. However, despite the imminent widespread deployment of HFMs, there is currently a lack of clear understanding regarding their operation in the healthcare field, their existing challenges, and their future trajectory. To answer these critical inquiries, we present a comprehensive and in-depth examination that delves into the landscape of HFMs. It begins with a comprehensive overview of HFMs, encompassing their methods, data, and applications, to provide a quick understanding of the current progress. Subsequently, it delves into a thorough exploration of the challenges associated with data, algorithms, and computing infrastructures in constructing and widely applying foundation models in healthcare. Furthermore, this survey identifies promising directions for future development in this field. We believe that this survey will enhance the community's understanding of the current progress of HFMs and serve as a valuable source of guidance for future advancements in this domain.",
      "author": "",
      "published_date": "2024-11-12T13:16:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197505+00:00",
      "updated_at": "2025-10-06T04:23:28.197507+00:00"
    },
    {
      "id": "f3ebee0a159c29e785b8640ab568613e",
      "url": "http://ieeexplore.ieee.org/document/10729663",
      "title": "Data- and Physics-Driven Deep Learning Based Reconstruction for Fast MRI: Fundamentals and Methodologies",
      "content": "Magnetic Resonance Imaging (MRI) is a pivotal clinical diagnostic tool, yet its extended scanning times often compromise patient comfort and image quality, especially in volumetric, temporal and quantitative scans. This review elucidates recent advances in MRI acceleration via data and physics-driven models, leveraging techniques from algorithm unrolling models, enhancement-based methods, and plug-and-play models to the emerging full spectrum of generative model-based methods. We also explore the synergistic integration of data models with physics-based insights, encompassing the advancements in multi-coil hardware accelerations like parallel imaging and simultaneous multi-slice imaging, and the optimization of sampling patterns. We then focus on domain-specific challenges and opportunities, including image redundancy exploitation, image integrity, evaluation metrics, data heterogeneity, and model generalization. This work also discusses potential solutions and future research directions, with an emphasis on the role of data harmonization and federated learning for further improving the general applicability and performance of these methods in MRI reconstruction.",
      "author": "",
      "published_date": "2024-10-22T13:18:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197471+00:00",
      "updated_at": "2025-10-06T04:23:28.197472+00:00"
    },
    {
      "id": "b71ad97ebddb2087936b4010c1aaf456",
      "url": "http://ieeexplore.ieee.org/document/10746601",
      "title": "Artificial General Intelligence for Medical Imaging Analysis",
      "content": "Large-scale Artificial General Intelligence (AGI) models, including Large Language Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented success in a variety of general domain tasks. Yet, when applied directly to specialized domains like medical imaging, which require in-depth expertise, these models face notable challenges arising from the medical field's inherent complexities and unique characteristics. In this review, we delve into the potential applications of AGI models in medical imaging and healthcare, with a primary focus on LLMs, Large Vision Models, and Large Multimodal Models. We provide a thorough overview of the key features and enabling techniques of LLMs and AGI, and further examine the roadmaps guiding the evolution and implementation of AGI models in the medical sector, summarizing their present applications, potentialities, and associated challenges. In addition, we highlight potential future research directions, offering a holistic view on upcoming ventures. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare, and beyond.",
      "author": "",
      "published_date": "2024-11-07T13:17:37+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197441+00:00",
      "updated_at": "2025-10-06T04:23:28.197442+00:00"
    },
    {
      "id": "3970a7e47edc49703b34feadbd5d1dab",
      "url": "http://ieeexplore.ieee.org/document/10720187",
      "title": "Exhaled Breath Analysis: From Laboratory Test to Wearable Sensing",
      "content": "Breath analysis and monitoring have emerged as pivotal components in both clinical research and daily health management, particularly in addressing the global health challenges posed by respiratory and metabolic disorders. The advancement of breath analysis strategies necessitates a multidisciplinary approach, seamlessly integrating expertise from medicine, biology, engineering, and materials science. Recent innovations in laboratory methodologies and wearable sensing technologies have ushered in an era of precise, real-time, and in situ breath analysis and monitoring. This comprehensive review elucidates the physical and chemical aspects of breath analysis, encompassing respiratory parameters and both volatile and non-volatile constituents. It emphasizes their physiological and clinical significance, while also exploring cutting-edge laboratory testing techniques and state-of-the-art wearable devices. Furthermore, the review delves into the application of sophisticated data processing technologies in the burgeoning field of breathomics and examines the potential of breath control in human-machine interaction paradigms. Additionally, it provides insights into the challenges of translating innovative laboratory and wearable concepts into mainstream clinical and daily practice. Continued innovation and interdisciplinary collaboration will drive progress in breath analysis, potentially revolutionizing personalized medicine through entirely non-invasive breath methodology.",
      "author": "",
      "published_date": "2024-10-16T13:15:55+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 182,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197410+00:00",
      "updated_at": "2025-10-06T04:23:28.197411+00:00"
    },
    {
      "id": "483769689d304d6940ab358e0b085a8c",
      "url": "http://ieeexplore.ieee.org/document/10771694",
      "title": "Earable Multimodal Sensing and Stimulation: A Prospective Toward Unobtrusive Closed-Loop Biofeedback",
      "content": "The human ear has emerged as a bidirectional gateway to the brain's and body's signals. Recent advances in around-the-ear and in-ear sensors have enabled the assessment of biomarkers and physiomarkers derived from brain and cardiac activity using ear-electroencephalography (ear-EEG), photoplethysmography (ear-PPG), and chemical sensing of analytes from the ear, with ear-EEG having been taken beyond-the-lab to outer space. Parallel advances in non-invasive and minimally invasive brain stimulation techniques have leveraged the ear's access to two cranial nerves to modulate brain and body activity. The vestibulocochlear nerve stimulates the auditory cortex and limbic system with sound, while the auricular branch of the vagus nerve indirectly but significantly couples to the autonomic nervous system and cardiac output. Acoustic and current mode stimuli delivered using discreet and unobtrusive earables are an active area of research, aiming to make biofeedback and bioelectronic medicine deliverable outside of the clinic, with remote and continuous monitoring of therapeutic responsivity and long-term adaptation. Leveraging recent advances in ear-EEG, transcutaneous auricular vagus nerve stimulation (taVNS), and unobtrusive acoustic stimulation, we review accumulating evidence that combines their potential into an integrated earable platform for closed-loop multimodal sensing and neuromodulation, towards personalized and holistic therapies that are near, in- and around-the-ear.",
      "author": "",
      "published_date": "2024-11-29T13:16:54+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197376+00:00",
      "updated_at": "2025-10-06T04:23:28.197378+00:00"
    },
    {
      "id": "1d8d5e8cf0c2514bbeb45a8e0b9c28f5",
      "url": "http://ieeexplore.ieee.org/document/10856220",
      "title": "Editorial: Harnessing Reviews to Advance Biomedical Engineering's New Horizons",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197333+00:00",
      "updated_at": "2025-10-06T04:23:28.197334+00:00"
    },
    {
      "id": "6071ce99ab68021ed48d4600bdeec843",
      "url": "http://ieeexplore.ieee.org/document/10856214",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197315+00:00",
      "updated_at": "2025-10-06T04:23:28.197316+00:00"
    },
    {
      "id": "f18dbf7099a24df1b7e9875d0258e8eb",
      "url": "http://ieeexplore.ieee.org/document/10856213",
      "title": "IEEE Engineering in Medicine and Biology Society",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:20+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197295+00:00",
      "updated_at": "2025-10-06T04:23:28.197297+00:00"
    },
    {
      "id": "9b7968741403d6b479424052728c8879",
      "url": "http://ieeexplore.ieee.org/document/10856260",
      "title": "Front Cover",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:28.197270+00:00",
      "updated_at": "2025-10-06T04:23:28.197274+00:00"
    },
    {
      "id": "3940e00c078b0713f1d0bd1b558c104a",
      "url": "https://arxiv.org/abs/2510.02563",
      "title": "Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds",
      "content": "arXiv:2510.02563v1 Announce Type: cross \nAbstract: Ear canal scanning/sensing (ECS) has emerged as a novel biometric authentication method for mobile devices paired with wireless earbuds. Existing studies have demonstrated the uniqueness of ear canals by training and testing machine learning classifiers on ECS data. However, implementing practical ECS-based authentication requires preventing raw biometric data leakage and designing computationally efficient protocols suitable for resource-constrained earbuds. To address these challenges, we propose an ear canal key extraction protocol, \\textbf{EarID}. Without relying on classifiers, EarID extracts unique binary keys directly on the earbuds during authentication. These keys further allow the use of privacy-preserving fuzzy commitment scheme that verifies the wearer's key on mobile devices. Our evaluation results demonstrate that EarID achieves a 98.7\\% authentication accuracy, comparable to machine learning classifiers. The mobile enrollment time (160~ms) and earbuds processing time (226~ms) are negligible in terms of wearer's experience. Moreover, our approach is robust and attack-resistant, maintaining a false acceptance rate below 1\\% across all adversarial scenarios. We believe the proposed EarID offers a practical and secure solution for next-generation wireless earbuds.",
      "author": "Chenpei Huang, Lingfeng Yao, Hui Zhong, Kyu In Lee, Lan Zhang, Xiaoyong Yuan, Tomoaki Ohtsuki, Miao Pan",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 176,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948694+00:00",
      "updated_at": "2025-10-06T04:23:23.948695+00:00"
    },
    {
      "id": "8c5cf86e9019c5ab78d3292e05dba65f",
      "url": "https://arxiv.org/abs/2510.02464",
      "title": "ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality",
      "content": "arXiv:2510.02464v1 Announce Type: cross \nAbstract: We propose the Extended Reality Universal Planning Toolkit (ERUPT), an extended reality (XR) system for interactive motion planning. Our system allows users to create and dy- namically reconfigure environments while they plan robot paths. In immersive three-dimensional XR environments, users gain a greater spatial understanding. XR also unlocks a broader range of natural interaction capabilities, allowing users to grab and adjust objects in the environment similarly to the real world, rather than using a mouse and keyboard with the scene projected onto a two-dimensional computer screen. Our system integrates with MoveIt, a manipulation planning framework, allowing users to send motion planning requests and visualize the resulting robot paths in virtual or augmented reality. We provide a broad range of interaction modalities, allowing users to modify objects in the environment and interact with a virtual robot. Our system allows operators to visualize robot motions, ensuring desired behavior as it moves throughout the environment, without risk of collisions within a virtual space, and to then deploy planned paths on physical robots in the real world.",
      "author": "Isaac Ngui, Courtney McBeth, Andr\\'e Santos, Grace He, Katherine J. Mimnaugh, James D. Motes, Luciano Soares, Marco Morales, Nancy M. Amato",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 178,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948663+00:00",
      "updated_at": "2025-10-06T04:23:23.948664+00:00"
    },
    {
      "id": "010189621ff5cedb52dca092fc6b5438",
      "url": "https://arxiv.org/abs/2510.02836",
      "title": "VR as a \"Drop-In\" Well-being Tool for Knowledge Workers",
      "content": "arXiv:2510.02836v1 Announce Type: new \nAbstract: Virtual Reality (VR) is increasingly being used to support workplace well-being, but many interventions focus narrowly on a single activity or goal. Our work explores how VR can meet the diverse physical and mental needs of knowledge workers. We developed Tranquil Loom, a VR app offering stretching, guided meditation, and open exploration across four environments. The app includes an AI assistant that suggests activities based on users' emotional states. We conducted a two-phase mixed-methods study: (1) interviews with 10 knowledge workers to guide the app's design, and (2) deployment with 35 participants gathering usage data, well-being measures, and interviews. Results showed increases in mindfulness and reductions in anxiety. Participants enjoyed both structured and open-ended activities, often using the app playfully. While AI suggestions were used infrequently, they prompted ideas for future personalization. Overall, participants viewed VR as a flexible, ``drop-in'' tool, highlighting its value for situational rather than prescriptive well-being support.",
      "author": "Sophia Ppali, Haris Psallidopoulos, Marios Constantinides, Fotis Liarokapis",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 156,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948628+00:00",
      "updated_at": "2025-10-06T04:23:23.948629+00:00"
    },
    {
      "id": "acf102ec2fa754d9bb70d9380f3ab008",
      "url": "https://arxiv.org/abs/2510.02814",
      "title": "PromptMap: Supporting Exploratory Text-to-Image Generation",
      "content": "arXiv:2510.02814v1 Announce Type: new \nAbstract: Text-to-image generative models can be tremendously valuable in supporting creative tasks by providing inspirations and enabling quick exploration of different design ideas. However, one common challenge is that users may still not be able to find anything useful after many hours and hundreds of images. Without effective help, users can easily get lost in the vast design space, forgetting what has been tried and what has not. In this work, we first propose the Design-Exploration model to formalize the exploration process. Based on this model, we create an interactive visualization system, PromptMap, to support exploratory text-to-image generation. Our system provides a new visual representation that better matches the non-linear nature of such processes, making them easier to understand and follow. It utilizes novel visual representations and intuitive interactions to help users structure the many possibilities that they can explore. We evaluated the system through in-depth interviews with users.",
      "author": "Yuhan Guo, Xingyou Liu, Xiaoru Yuan, Kai Xu",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948598+00:00",
      "updated_at": "2025-10-06T04:23:23.948600+00:00"
    },
    {
      "id": "34a1828b5608484f12062a8c1c815c73",
      "url": "https://arxiv.org/abs/2510.02766",
      "title": "Fostering Collective Discourse: A Distributed Role-Based Approach to Online News Commenting",
      "content": "arXiv:2510.02766v1 Announce Type: new \nAbstract: Current news commenting systems are designed based on implicitly individualistic assumptions, where discussion is the result of a series of disconnected opinions. This often results in fragmented and polarized conversations that fail to represent the spectrum of public discourse. In this work, we develop a news commenting system where users take on distributed roles to collaboratively structure the comments to encourage a connected, balanced discussion space. Through a within-subject, mixed-methods evaluation (N=38), we find that the system supported three stages of participation: understanding issues, collaboratively structuring comments, and building a discussion. With our system, users' comments displayed more balanced perspectives and a more emotionally neutral argumentation. Simultaneously, we observed reduced argument strength compared to a traditional commenting system, indicating a trade-off between inclusivity and depth. We conclude with design considerations and trade-offs for introducing distributed roles in news commenting system design.",
      "author": "Yoojin Hong, Yersultan Doszhan, Joseph Seering",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948569+00:00",
      "updated_at": "2025-10-06T04:23:23.948571+00:00"
    },
    {
      "id": "ad96af9a711a2edcbd2397fdea1737e0",
      "url": "https://arxiv.org/abs/2510.02759",
      "title": "Prototyping Digital Social Spaces through Metaphor-Driven Design: Translating Spatial Concepts into an Interactive Social Simulation",
      "content": "arXiv:2510.02759v1 Announce Type: new \nAbstract: Social media platforms are central to communication, yet their designs remain narrowly focused on engagement and scale. While researchers have proposed alternative visions for online spaces, these ideas are difficult to prototype within platform constraints. In this paper, we introduce a metaphor-driven system to help users imagine and explore new social media environments. The system translates users' metaphors into structured sets of platform features and generates interactive simulations populated with LLM-driven agents. To evaluate this approach, we conducted a study where participants created and interacted with simulated social media spaces. Our findings show that metaphors allow users to express distinct social expectations, and that perceived authenticity of the simulation depended on how well it captured dynamics like intimacy, participation, and temporal engagement. We conclude by discussing how metaphor-driven simulation can be a powerful design tool for prototyping alternative social architectures and expanding the design space for future social platforms.",
      "author": "Yoojin Hong, Martina Di Paola, Braahmi Padmakumar, Hwi Joon Lee, Mahnoor Shafiq, Joseph Seering",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948540+00:00",
      "updated_at": "2025-10-06T04:23:23.948542+00:00"
    },
    {
      "id": "f84607bf8b76b29d091749a731a62be4",
      "url": "https://arxiv.org/abs/2510.02680",
      "title": "\"It Felt Real\" Victim Perspectives on Platform Design and Longer-Running Scams",
      "content": "arXiv:2510.02680v1 Announce Type: new \nAbstract: Longer-running scams, such as romance fraud and \"pig-butchering\" scams, exploit not only victims' emotions but also the design of digital platforms. Scammers commonly leverage features such as professional-looking profile verification, algorithmic recommendations that reinforce contact, integrated payment systems, and private chat affordances to gradually establish trust and dependency with victims. Prior work in HCI and criminology has examined online scams through the lenses of detection mechanisms, threat modeling, and user-level vulnerabilities. However, less attention has been paid to how platform design itself enables longer-running scams. To address this gap, we conducted in-depth interviews with 25 longer-running scam victims in China. Our findings show how scammers strategically use platform affordances to stage credibility, orchestrate intimacy, and sustain coercion with victims. By analyzing scams as socio-technical projects, we highlight how platform design can be exploited in longer-running scams, and point to redesigning future platforms to better protect users.",
      "author": "Jingjia Xiao, Qing Xiao, Hong Shen",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948511+00:00",
      "updated_at": "2025-10-06T04:23:23.948513+00:00"
    },
    {
      "id": "ab03e19cdcb857bad4db54ac412f67b3",
      "url": "https://arxiv.org/abs/2510.02660",
      "title": "When Researchers Say Mental Model/Theory of Mind of AI, What Are They Really Talking About?",
      "content": "arXiv:2510.02660v1 Announce Type: new \nAbstract: When researchers claim AI systems possess ToM or mental models, they are fundamentally dis- cussing behavioral predictions and bias corrections rather than genuine mental states. This position paper argues that the current discourse conflates sophisticated pattern matching with authentic cog- nition, missing a crucial distinction between simulation and experience. While recent studies show LLMs achieving human-level performance on ToM laboratory tasks, these results are based only on behavioral mimicry. More importantly, the entire testing paradigm may be flawed in applying individual human cognitive tests to AI systems, but assessing human cognition directly in the moment of human-AI interaction. I suggest shifting focus toward mutual ToM frameworks that acknowledge the simultaneous contributions of human cognition and AI algorithms, emphasizing the interaction dynamics, instead of testing AI in isolation.",
      "author": "Xiaoyun Yin, Elmira Zahmat Doost, Shiwen Zhou, Garima Arya Yadav, Jamie C. Gorman",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 132,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948482+00:00",
      "updated_at": "2025-10-06T04:23:23.948483+00:00"
    },
    {
      "id": "bf19e5d339baec7dea39a9eb531afe37",
      "url": "https://arxiv.org/abs/2510.02546",
      "title": "Open WebUI: An Open, Extensible, and Usable Interface for AI Interaction",
      "content": "arXiv:2510.02546v1 Announce Type: new \nAbstract: While LLMs enable a range of AI applications, interacting with multiple models and customizing workflows can be challenging, and existing LLM interfaces offer limited support for collaborative extension or real-world evaluation. In this work, we present an interface toolkit for LLMs designed to be open (open-source and local), extensible (plugin support and users can interact with multiple models), and usable. The extensibility is enabled through a two-pronged plugin architecture and a community platform for sharing, importing, and adapting extensions. To evaluate the system, we analyzed organic engagement through social platforms, conducted a user survey, and provided notable examples of the toolkit in the wild. Through studying how users engage with and extend the toolkit, we show how extensible, open LLM interfaces provide both functional and social value, and highlight opportunities for future HCI work on designing LLM toolkit platforms and shaping local LLM-user interaction.",
      "author": "Jaeryang Baek, Ayana Hussain, Danny Liu, Nicholas Vincent, Lawrence H. Kim",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 149,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948452+00:00",
      "updated_at": "2025-10-06T04:23:23.948454+00:00"
    },
    {
      "id": "236588ed7e01ce7a0fb9e6b0577612d9",
      "url": "https://arxiv.org/abs/2510.02511",
      "title": "Vector Autoregression (VAR) of Longitudinal Sleep and Self-report Mood Data",
      "content": "arXiv:2510.02511v1 Announce Type: new \nAbstract: Self-tracking is one of many behaviors involved in the long-term self-management of chronic illnesses. As consumer-grade wearable sensors have made the collection of health-related behaviors commonplace, the quality, volume, and availability of such data has dramatically improved. This exploratory longitudinal N-of-1 study quantitatively assesses four years of sleep data captured via the Oura Ring, a consumer-grade sleep tracking device, along with self-reported mood data logged using eMood Tracker for iOS. After assessing the data for stationarity and computing the appropriate lag-length selection, a vector autoregressive (VAR) model was fit along with Granger causality tests to assess causal mechanisms within this multivariate time series. Oura's nightly sleep quality score was shown to Granger-cause the presence of depressed and anxious moods using a VAR(2) model.",
      "author": "Jeff Brozena",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 128,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:23.948415+00:00",
      "updated_at": "2025-10-06T04:23:23.948419+00:00"
    },
    {
      "id": "610910461ac2259b80584bc145831d39",
      "url": "https://arxiv.org/abs/2510.02120",
      "title": "VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI",
      "content": "arXiv:2510.02120v2 Announce Type: replace-cross \nAbstract: Accounting for inter-individual variability in brain function is key to precision medicine. Here, by considering functional inter-individual variability as meaningful data rather than noise, we introduce VarCoNet, an enhanced self-supervised framework for robust functional connectome (FC) extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs self-supervised contrastive learning to exploit inherent functional inter-individual variability, serving as a brain function encoder that generates FC embeddings readily applicable to downstream tasks even in the absence of labeled data. Contrastive learning is facilitated by a novel augmentation strategy based on segmenting rs-fMRI signals. At its core, VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series processing, enhanced with a robust Bayesian hyperparameter optimization. Our VarCoNet framework is evaluated on two downstream tasks: (i) subject fingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii) autism spectrum disorder (ASD) classification, using rs-fMRI data from the ABIDE I and ABIDE II datasets. Using different brain parcellations, our extensive testing against state-of-the-art methods, including 13 deep learning methods, demonstrates VarCoNet's superiority, robustness, interpretability, and generalizability. Overall, VarCoNet provides a versatile and robust framework for FC analysis in rs-fMRI.",
      "author": "Charalampos Lamprou, Aamna Alshehhi, Leontios J. Hadjileontiadis, Mohamed L. Seghier",
      "published_date": "2025-10-06T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 186,
      "reading_time": 1,
      "created_at": "2025-10-06T04:23:22.890714+00:00",
      "updated_at": "2025-10-06T04:23:22.890716+00:00"
    }
  ]
}