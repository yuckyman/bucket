{
  "last_updated": "2025-10-17T04:44:03.629198+00:00",
  "count": 20,
  "articles": [
    {
      "id": "78a543035cc17e2ac6f73ecd95cce1ae",
      "url": "http://ieeexplore.ieee.org/document/10750441",
      "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities and Future Directions",
      "content": "Foundation model, trained on a diverse range of data and adaptable to a myriad of tasks, is advancing healthcare. It fosters the development of healthcare artificial intelligence (AI) models tailored to the intricacies of the medical field, bridging the gap between limited AI models and the varied nature of healthcare practices. The advancement of a healthcare foundation model (HFM) brings forth tremendous potential to augment intelligent healthcare services across a broad spectrum of scenarios. However, despite the imminent widespread deployment of HFMs, there is currently a lack of clear understanding regarding their operation in the healthcare field, their existing challenges, and their future trajectory. To answer these critical inquiries, we present a comprehensive and in-depth examination that delves into the landscape of HFMs. It begins with a comprehensive overview of HFMs, encompassing their methods, data, and applications, to provide a quick understanding of the current progress. Subsequently, it delves into a thorough exploration of the challenges associated with data, algorithms, and computing infrastructures in constructing and widely applying foundation models in healthcare. Furthermore, this survey identifies promising directions for future development in this field. We believe that this survey will enhance the community's understanding of the current progress of HFMs and serve as a valuable source of guidance for future advancements in this domain.",
      "author": "",
      "published_date": "2024-11-12T13:16:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819345+00:00",
      "updated_at": "2025-10-17T04:43:49.819347+00:00"
    },
    {
      "id": "f3ebee0a159c29e785b8640ab568613e",
      "url": "http://ieeexplore.ieee.org/document/10729663",
      "title": "Data- and Physics-Driven Deep Learning Based Reconstruction for Fast MRI: Fundamentals and Methodologies",
      "content": "Magnetic Resonance Imaging (MRI) is a pivotal clinical diagnostic tool, yet its extended scanning times often compromise patient comfort and image quality, especially in volumetric, temporal and quantitative scans. This review elucidates recent advances in MRI acceleration via data and physics-driven models, leveraging techniques from algorithm unrolling models, enhancement-based methods, and plug-and-play models to the emerging full spectrum of generative model-based methods. We also explore the synergistic integration of data models with physics-based insights, encompassing the advancements in multi-coil hardware accelerations like parallel imaging and simultaneous multi-slice imaging, and the optimization of sampling patterns. We then focus on domain-specific challenges and opportunities, including image redundancy exploitation, image integrity, evaluation metrics, data heterogeneity, and model generalization. This work also discusses potential solutions and future research directions, with an emphasis on the role of data harmonization and federated learning for further improving the general applicability and performance of these methods in MRI reconstruction.",
      "author": "",
      "published_date": "2024-10-22T13:18:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819312+00:00",
      "updated_at": "2025-10-17T04:43:49.819313+00:00"
    },
    {
      "id": "b71ad97ebddb2087936b4010c1aaf456",
      "url": "http://ieeexplore.ieee.org/document/10746601",
      "title": "Artificial General Intelligence for Medical Imaging Analysis",
      "content": "Large-scale Artificial General Intelligence (AGI) models, including Large Language Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented success in a variety of general domain tasks. Yet, when applied directly to specialized domains like medical imaging, which require in-depth expertise, these models face notable challenges arising from the medical field's inherent complexities and unique characteristics. In this review, we delve into the potential applications of AGI models in medical imaging and healthcare, with a primary focus on LLMs, Large Vision Models, and Large Multimodal Models. We provide a thorough overview of the key features and enabling techniques of LLMs and AGI, and further examine the roadmaps guiding the evolution and implementation of AGI models in the medical sector, summarizing their present applications, potentialities, and associated challenges. In addition, we highlight potential future research directions, offering a holistic view on upcoming ventures. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare, and beyond.",
      "author": "",
      "published_date": "2024-11-07T13:17:37+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819282+00:00",
      "updated_at": "2025-10-17T04:43:49.819283+00:00"
    },
    {
      "id": "483769689d304d6940ab358e0b085a8c",
      "url": "http://ieeexplore.ieee.org/document/10771694",
      "title": "Earable Multimodal Sensing and Stimulation: A Prospective Toward Unobtrusive Closed-Loop Biofeedback",
      "content": "The human ear has emerged as a bidirectional gateway to the brain's and body's signals. Recent advances in around-the-ear and in-ear sensors have enabled the assessment of biomarkers and physiomarkers derived from brain and cardiac activity using ear-electroencephalography (ear-EEG), photoplethysmography (ear-PPG), and chemical sensing of analytes from the ear, with ear-EEG having been taken beyond-the-lab to outer space. Parallel advances in non-invasive and minimally invasive brain stimulation techniques have leveraged the ear's access to two cranial nerves to modulate brain and body activity. The vestibulocochlear nerve stimulates the auditory cortex and limbic system with sound, while the auricular branch of the vagus nerve indirectly but significantly couples to the autonomic nervous system and cardiac output. Acoustic and current mode stimuli delivered using discreet and unobtrusive earables are an active area of research, aiming to make biofeedback and bioelectronic medicine deliverable outside of the clinic, with remote and continuous monitoring of therapeutic responsivity and long-term adaptation. Leveraging recent advances in ear-EEG, transcutaneous auricular vagus nerve stimulation (taVNS), and unobtrusive acoustic stimulation, we review accumulating evidence that combines their potential into an integrated earable platform for closed-loop multimodal sensing and neuromodulation, towards personalized and holistic therapies that are near, in- and around-the-ear.",
      "author": "",
      "published_date": "2024-11-29T13:16:54+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819250+00:00",
      "updated_at": "2025-10-17T04:43:49.819252+00:00"
    },
    {
      "id": "1d8d5e8cf0c2514bbeb45a8e0b9c28f5",
      "url": "http://ieeexplore.ieee.org/document/10856220",
      "title": "Editorial: Harnessing Reviews to Advance Biomedical Engineering's New Horizons",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819207+00:00",
      "updated_at": "2025-10-17T04:43:49.819209+00:00"
    },
    {
      "id": "6071ce99ab68021ed48d4600bdeec843",
      "url": "http://ieeexplore.ieee.org/document/10856214",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819188+00:00",
      "updated_at": "2025-10-17T04:43:49.819190+00:00"
    },
    {
      "id": "f18dbf7099a24df1b7e9875d0258e8eb",
      "url": "http://ieeexplore.ieee.org/document/10856213",
      "title": "IEEE Engineering in Medicine and Biology Society",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:20+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819168+00:00",
      "updated_at": "2025-10-17T04:43:49.819170+00:00"
    },
    {
      "id": "9b7968741403d6b479424052728c8879",
      "url": "http://ieeexplore.ieee.org/document/10856260",
      "title": "Front Cover",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:11.746080+00:00",
      "updated_at": "2025-10-17T04:24:11.746086+00:00"
    },
    {
      "id": "d2f24a3d09c41e9d05c4b4acdcd73e85",
      "url": "https://arxiv.org/abs/2510.14308",
      "title": "ReUseIt: Synthesizing Reusable AI Agent Workflows for Web Automation",
      "content": "arXiv:2510.14308v1 Announce Type: new \nAbstract: AI-powered web agents have the potential to automate repetitive tasks, such as form filling, information retrieval, and scheduling, but they struggle to reliably execute these tasks without human intervention, requiring users to provide detailed guidance during every run. We address this limitation by automatically synthesizing reusable workflows from an agent's successful and failed attempts. These workflows incorporate execution guards that help agents detect and fix errors while keeping users informed of progress and issues. Our approach enables agents to successfully complete repetitive tasks of the same type with minimal intervention, increasing the success rates from 24.2% to 70.1% across fifteen tasks. To evaluate this approach, we invited nine users and found that our agent helped them complete web tasks with a higher success rate and less guidance compared to two baseline methods, as well as allowed users to easily monitor agent behavior and understand failures.",
      "author": "Yimeng Liu, Misha Sra, Jeevana Priya Inala, Chenglong Wang",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415850+00:00",
      "updated_at": "2025-10-17T04:24:06.415852+00:00"
    },
    {
      "id": "132a662524e3f66c5f49e360eddb4c4b",
      "url": "https://arxiv.org/abs/2510.14277",
      "title": "GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds and Characters",
      "content": "arXiv:2510.14277v1 Announce Type: new \nAbstract: We introduce GenLARP, a virtual reality (VR) system that transforms personalized stories into immersive live action role-playing (LARP) experiences. GenLARP enables users to act as both creators and players, allowing them to design characters based on their descriptions and live in the story world. Generative AI and agents powered by Large Language Models (LLMs) enrich these experiences.",
      "author": "Yichen Yu, Yifan Jiang, Mandy Lui, Qiao Jin",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 62,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415821+00:00",
      "updated_at": "2025-10-17T04:24:06.415822+00:00"
    },
    {
      "id": "eaab92b0431b3ef22abe65172c04d246",
      "url": "https://arxiv.org/abs/2510.14267",
      "title": "TapNav: Adaptive Spatiotactile Screen Readers for Tactually Guided Touchscreen Interactions for Blind and Low Vision People",
      "content": "arXiv:2510.14267v1 Announce Type: new \nAbstract: Screen readers are audio-based software that Blind and Low Vision (BLV) people use to interact with computing devices, such as tablets and smartphones. Although this technology has significantly improved the accessibility of touchscreen devices, the sequential nature of audio limits the bandwidth of information users can receive and process. We introduce TapNav, an adaptive spatiotactile screen reader prototype developed to interact with touchscreen interfaces spatially. TapNav's screen reader provides adaptive auditory feedback that, in combination with a tactile overlay, conveys spatial information and location of interface elements on-screen. We evaluated TapNav with 12 BLV users who interacted with TapNav to explore a data visualization and interact with a bank transactions application. Our qualitative findings show that touch points and spatially constrained navigation helped users anticipate outcomes for faster exploration, and offload cognitive load to touch. We provide design guidelines for creating tactile overlays for adaptive spatiotactile screen readers and discuss their generalizability beyond our exploratory data analysis and everyday application navigation scenarios.",
      "author": "Ricardo Gonzalez, Fannie Liu, Blair MacIntyre, David Saffo",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415798+00:00",
      "updated_at": "2025-10-17T04:24:06.415800+00:00"
    },
    {
      "id": "1f46a7f742e44396ae60bdcb0995d517",
      "url": "https://arxiv.org/abs/2510.14247",
      "title": "VisAider: AI-Assisted Context-Aware Visualization Support for Data Presentations",
      "content": "arXiv:2510.14247v1 Announce Type: new \nAbstract: Effective real-time data presentation is essential in small-group interactive contexts, where discussions evolve dynamically and presenters must adapt visualizations to shifting audience interests. However, most existing interactive visualization systems rely on fixed mappings between user actions and visualization commands, limiting their ability to support richer operations such as changing visualization types, adjusting data transformations, or incorporating additional datasets on the fly during live presentations. This work-in-progress paper presents VisAider, an AI-assisted interactive data presentation prototype that continuously analyzes the live presentation context, including the available dataset, active visualization, ongoing conversation, and audience profile, to generate ranked suggestions for relevant visualization aids. Grounded in a formative study with experienced data analysts, we identified key challenges in adapting visual content in real time and distilled design considerations to guide system development. A prototype implementation demonstrates the feasibility of this approach in simulated scenarios, and preliminary testing highlights challenges in inferring appropriate data transformations, resolving ambiguous visualization tasks, and achieving low-latency responsiveness. Ongoing work focuses on addressing these limitations, integrating the system into presentation environments, and preparing a summative user study to evaluate usability and communicative impact.",
      "author": "Kentaro Takahira, Yuki Ueno",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415768+00:00",
      "updated_at": "2025-10-17T04:24:06.415769+00:00"
    },
    {
      "id": "bfe1505ed4f6fa5eaa70a289330f3e09",
      "url": "https://arxiv.org/abs/2510.14141",
      "title": "Understanding Data Usage when Making High-Stakes Frontline Decisions in Homelessness Services",
      "content": "arXiv:2510.14141v1 Announce Type: new \nAbstract: Frontline staff of emergency shelters face challenges such as vicarious trauma, compassion fatigue, and burnout. The technology they use is often not designed for their unique needs, and can feel burdensome on top of their already cognitively and emotionally taxing work. While existing literature focuses on data-driven technologies that automate or streamline frontline decision-making about vulnerable individuals, we discuss scenarios in which staff may resist such automation. We then suggest how data-driven technologies can better align with their human-centred decision-making processes. This paper presents findings from a qualitative fieldwork study conducted from 2022 to 2024 at a large emergency shelter in Canada. The goal of this fieldwork was to co-design, develop, and deploy an interactive data-navigation interface that supports frontline staff when making collaborative, high-stakes decisions about individuals experiencing homelessness. By reflecting on this fieldwork, we contribute insight into the role that administrative shelter data play during decision-making, and unpack staff members' apparent reluctance to outsource decisions about vulnerable individuals to data systems. Our findings suggest a data-outsourcing continuum, which we discuss in terms of how designers may create technologies to support compassionate, data-driven decision-making in nonprofit domains.",
      "author": "Teale W. Masrani, Geoffrey Messier, Amy Voida, Gina Dimitropoulos, Helen Ai He",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 193,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415734+00:00",
      "updated_at": "2025-10-17T04:24:06.415736+00:00"
    },
    {
      "id": "af2494df3bdf66e61db702dfccefc05e",
      "url": "https://arxiv.org/abs/2510.13814",
      "title": "Reversing the Lens: Using Explainable AI to Understand Human Expertise",
      "content": "arXiv:2510.13814v1 Announce Type: new \nAbstract: Both humans and machine learning models learn from experience, particularly in safety- and reliability-critical domains. While psychology seeks to understand human cognition, the field of Explainable AI (XAI) develops methods to interpret machine learning models. This study bridges these domains by applying computational tools from XAI to analyze human learning. We modeled human behavior during a complex real-world task -- tuning a particle accelerator -- by constructing graphs of operator subtasks. Applying techniques such as community detection and hierarchical clustering to archival operator data, we reveal how operators decompose the problem into simpler components and how these problem-solving structures evolve with expertise. Our findings illuminate how humans develop efficient strategies in the absence of globally optimal solutions, and demonstrate the utility of XAI-based methods for quantitatively studying human cognition.",
      "author": "Roussel Rahman, Aashwin Ananda Mishra, Wan-Lin Hu",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 134,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415701+00:00",
      "updated_at": "2025-10-17T04:24:06.415702+00:00"
    },
    {
      "id": "456a3aca9258f2274f99c42dd52630d3",
      "url": "https://arxiv.org/abs/2510.13813",
      "title": "Puzzlegram: a Serious Game Designed for the Elderly in Group Settings",
      "content": "arXiv:2510.13813v1 Announce Type: new \nAbstract: An original serious game prototype named 'Puzzlegram' is created for the elderly demographic in group settings as the target players. Puzzlegram is precisely designed to accentuate memory, auditory interaction as well as haptic response to visual signals with the use of music. Music is introduced as a key component for establishing the game design that provides a source of meaningful contextualization (familiar music from the past) for setting the game mechanics, which facilitated the construction of the serious game design process. The discussion topics raised include the need to design serious games for fostering meaningful interactions, as well as developing a thorough framework for constructing purposeful design for serious games. A potential integral of artificial intelligence to Puzzlegram may involve assigning a novel dimension to its existing problem solving task by adapting to varying states of cognitive function for monitoring purposes based on an individual's interaction with the game.",
      "author": "Sunny Choi",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415670+00:00",
      "updated_at": "2025-10-17T04:24:06.415671+00:00"
    },
    {
      "id": "6f1f761779cdee839ff875e70bcb3794",
      "url": "https://arxiv.org/abs/2510.13812",
      "title": "MindBenchAI: An Actionable Platform to Evaluate the Profile and Performance of Large Language Models in a Mental Healthcare Context",
      "content": "arXiv:2510.13812v1 Announce Type: new \nAbstract: Individuals are increasingly utilizing large language model (LLM)based tools for mental health guidance and crisis support in place of human experts. While AI technology has great potential to improve health outcomes, insufficient empirical evidence exists to suggest that AI technology can be deployed as a clinical replacement; thus, there is an urgent need to assess and regulate such tools. Regulatory efforts have been made and multiple evaluation frameworks have been proposed, however,field-wide assessment metrics have yet to be formally integrated. In this paper, we introduce a comprehensive online platform that aggregates evaluation approaches and serves as a dynamic online resource to simplify LLM and LLM-based tool assessment: MindBenchAI. At its core, MindBenchAI is designed to provide easily accessible/interpretable information for diverse stakeholders (patients, clinicians, developers, regulators, etc.). To create MindBenchAI, we built off our work developing MINDapps.org to support informed decision-making around smartphone app use for mental health, and expanded the technical MINDapps.org framework to encompass novel large language model (LLM) functionalities through benchmarking approaches. The MindBenchAI platform is designed as a partnership with the National Alliance on Mental Illness (NAMI) to provide assessment tools that systematically evaluate LLMs and LLM-based tools with objective and transparent criteria from a healthcare standpoint, assessing both profile (i.e. technical features, privacy protections, and conversational style) and performance characteristics (i.e. clinical reasoning skills).",
      "author": "Bridget Dwyer, Matthew Flathers, Akane Sano, Allison Dempsey, Andrea Cipriani, Asim H. Gazi, Carla Gorban, Carolyn I. Rodriguez, Charles Stromeyer IV, Darlene King, Eden Rozenblit, Gillian Strudwick, Jake Linardon, Jiaee Cheong, Joseph Firth, Julian Herpertz, Julian Schwarz, Margaret Emerson, Martin P. Paulus, Michelle Patriquin, Yining Hua, Soumya Choudhary, Steven Siddals, Laura Ospina Pinillos, Jason Bantjes, Steven Scheuller, Xuhai Xu, Ken Duckworth, Daniel H. Gillison, Michael Wood, John Torous",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 224,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415639+00:00",
      "updated_at": "2025-10-17T04:24:06.415641+00:00"
    },
    {
      "id": "a8460891218f7cbc6340d7b681f43143",
      "url": "https://arxiv.org/abs/2510.13811",
      "title": "Generative AI in Heritage Practice: Improving the Accessibility of Heritage Guidance",
      "content": "arXiv:2510.13811v1 Announce Type: new \nAbstract: This paper discusses the potential for integrating Generative Artificial Intelligence (GenAI) into professional heritage practice with the aim of enhancing the accessibility of public-facing guidance documents. We developed HAZEL, a GenAI chatbot fine-tuned to assist with revising written guidance relating to heritage conservation and interpretation. Using quantitative assessments, we compare HAZEL's performance to that of ChatGPT (GPT-4) in a series of tasks related to the guidance writing process. The results of this comparison indicate a slightly better performance of HAZEL over ChatGPT, suggesting that the GenAI chatbot is more effective once the underlying large language model (LLM) has been fine-tuned. However, we also note significant limitations, particularly in areas requiring cultural sensitivity and more advanced technical expertise. These findings suggest that, while GenAI cannot replace human heritage professionals in technical authoring tasks, its potential to automate and expedite certain aspects of guidance writing could offer valuable benefits to heritage organisations, especially in resource-constrained contexts.",
      "author": "Jessica Witte, Edmund Lee, Lisa Brausem, Verity Shillabeer, Chiara Bonacchi",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415599+00:00",
      "updated_at": "2025-10-17T04:24:06.415600+00:00"
    },
    {
      "id": "1cf440b4b06aab4a0605863ccd30cd09",
      "url": "https://arxiv.org/abs/2510.13810",
      "title": "Choreographing Trash Cans: On Speculative Futures of Weak Robots in Public Spaces",
      "content": "arXiv:2510.13810v1 Announce Type: new \nAbstract: Delivering groceries or cleaning airports, mobile robots exist in public spaces. While these examples showcase robots that execute tasks, this paper explores mobile robots that encourage posthuman collaboration rather than managing environments independently. With feigned fragility, cuteness and incomplete functionalities, the so-called \"weak robots\" invite passersby to engage not only on a utilitarian level, but also through imaginative and emotional responses. After examining the workings of \"weak robots\" by queering notions of function and ability, we introduce two speculative design fiction vignettes that describe choreographies of such robots in future urban spaces -- one exploring a utopian weak robot and the other a dystopian weak robot. We introduce these speculations in order to discuss how different values may drive design decisions, and how such decisions may shape and drive different socio-technical futures in which robots and humans share public spaces that incentivise collaboration.",
      "author": "Minja Axelsson, Lea Luka Sikau",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 148,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415550+00:00",
      "updated_at": "2025-10-17T04:24:06.415556+00:00"
    },
    {
      "id": "e5ee3247f77859dca98e1b45a2701a79",
      "url": "https://arxiv.org/abs/2510.14486",
      "title": "Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks",
      "content": "arXiv:2510.14486v1 Announce Type: new \nAbstract: Brains learn to represent information from a large set of stimuli, typically by weak supervision. Unsupervised learning is therefore a natural approach for exploring the design of biological neural networks and their computations. Accordingly, redundancy reduction has been suggested as a prominent design principle of neural encoding, but its ``mechanistic'' biological implementation is unclear. Analogously, unsupervised training of artificial neural networks yields internal representations that allow for accurate stimulus classification or decoding, but typically rely on biologically-implausible implementations. We suggest that interactions between parallel subnetworks in the brain may underlie such learning: we present a model of representation learning by ensembles of neural networks, where each network learns to encode stimuli into an abstract representation space by cross-supervising interactions with other networks, for inputs they receive simultaneously or in close temporal proximity. Aiming for biological plausibility, each network has a small ``receptive field'', thus receiving a fixed part of the external input, and the networks do not share weights. We find that for different types of network architectures, and for both visual or neuronal stimuli, these cross-supervising networks learn semantic representations that are easily decodable and that decoding accuracy is comparable to supervised networks -- both at the level of single networks and the ensemble. We further show that performance is optimal for small receptive fields, and that sparse connectivity between networks is nearly as accurate as all-to-all interactions, with far fewer computations. We thus suggest a sparsely interacting collective of cross-supervising networks as an algorithmic framework for representational learning and collective computation in the brain.",
      "author": "Roy Urbach, Elad Schneidman",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 261,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:05.302346+00:00",
      "updated_at": "2025-10-17T04:24:05.302348+00:00"
    },
    {
      "id": "481ae57a96b36ec0c1933f184466871c",
      "url": "https://arxiv.org/abs/2510.14382",
      "title": "Joint encoding of \"what\" and \"when\" predictions through error-modulated plasticity in reservoir spiking networks",
      "content": "arXiv:2510.14382v1 Announce Type: new \nAbstract: The brain understands the external world through an internal model that generates predictions and refines them based on prediction errors. A complete prediction specifies what will happen, when it will happen, and with what probability, which we refer to as a \"prediction object\". Existing models typically capture only what and when, omit probabilities, and rely on biologically-implausible algorithms. Here we show that a single population of spiking neurons can jointly encode the prediction object through a biologically grounded learning mechanism. We implement a heterogeneous Izhikevich spiking reservoir with readouts trained by an error-modulated, attention-gated three-factor Hebbian rule and test it on a novel paradigm that controls both the timing and probability of upcoming stimuli. By integrating real-time learning of \"when\" with offline consolidation of \"what\", the model encodes the complete prediction object, firing at the correct times with magnitudes proportional to the probabilities. Critically, it rapidly adapts to changes in both stimulus timing and probability, an ability that global least-squares methods such as FORCE lack without explicit resets. During learning, the model self-organizes its readout weights into near-orthogonal subspaces for \"what\" and \"when,\" showing that multiplexed encoding arises naturally from generic recurrent dynamics under local, error-gated modulation. These results challenge the view that \"what\" and \"when\" predictions require separate modules, suggesting instead that mixed selectivity within shared populations supports flexible predictive cognition. The model also predicts phase-specific neuromodulation and overlapping neural subspaces, offering a parsimonious alternative to hierarchical predictive-coding accounts.",
      "author": "Yohei Yamada, Zenas C. Chao",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 245,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:05.302309+00:00",
      "updated_at": "2025-10-17T04:24:05.302310+00:00"
    }
  ]
}