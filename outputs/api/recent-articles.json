{
  "last_updated": "2026-01-25T02:01:25.452935+00:00",
  "count": 20,
  "articles": [
    {
      "id": "6c9b73cc9c1c11ac63f41b3b11cb3eae",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.22.701022v1?rss=1",
      "title": "Gamma oscillations across recording scales show a preference for saturated long-wavelength (reddish) hues in the primate visual cortex",
      "content": "Stimulus-induced narrowband gamma oscillations (30-70 Hz) arise due to excitatory-inhibitory interactions and have been proposed to carry feedforward prediction errors during visual processing. However, the dependence of gamma on stimulus color is not well characterized, with some studies showing a preference for saturated long-wavelength (reddish) hues in invasive recordings from the primary visual cortex (V1), while others showing no preference for red in luminance and cone-contrast balanced colors spanning the Derrington-Krauskopf-Lennie (DKL) isoluminant plane in non-invasive magnetoencephalography (MEG) recordings. To address these discrepancies, we simultaneously recorded local field potentials (LFPs) from V1 (n=2 monkeys) along with scalp electroencephalography (EEG), while presenting luminance-matched hues spanning the entire permissible range of colors. We found that saturated reddish hues produced the strongest gamma in both LFP and EEG in both monkeys. However, gamma was reduced when the colors were desaturated, so that selectivity for reddish hues was diminished or absent on the DKL space, as shown previously. Interestingly, selectivity was further reduced in EEG compared to LFP. Simultaneous recordings from an intermediate visual area (V4, n=1) revealed weak color-induced gamma, partially explaining the weakening of hue selectivity in macro-signals. Gamma power increased with increasing activation along the red-green (L-M) cardinal axis but was virtually absent along the blue-yellow (S-(L+M)) axis, suggesting that gamma could be a reflection of the L-M cone-contrast mechanism. These results comprehensively resolve the earlier discrepancy and shed light about the neural mechanisms underlying gamma generation in primate V1.",
      "author": "Biswas, A., Ray, S.",
      "published_date": "2026-01-24T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 239,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:52.284783+00:00",
      "updated_at": "2026-01-25T02:00:52.284785+00:00"
    },
    {
      "id": "04fbaf5d0943a790448c02933bfe8e91",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.22.701034v1?rss=1",
      "title": "The ApoC2 mimetic peptide D6PV enhances remyelination by stimulating oxidative phosphorylation in oligodendrocytes",
      "content": "Failure of remyelination drives neurodegeneration in demyelinating disorders such as multiple sclerosis (MS), with disrupted lipid handling and metabolic stress in oligodendrocyte precursor cells (OPCs) posing major barriers to repair. Here, we identify the dual ApoC-II mimetic- ApoC-III antagonist peptide D6PV as a metabolic modulator that directly enhances OPC differentiation and myelin repair. Across ex vivo and in vivo models of chemically induced demyelination, D6PV promotes oligodendrocyte maturation and restores myelin integrity independently of lipoprotein hydrolysis or modulation of lipid droplet-containing phagocytes. Guided by transcriptomics analyses, we find that D6PV stimulates mitochondrial oxidative phosphorylation and fatty acid {beta}-oxidation, while suppressing inflammatory transcriptional programs, thereby driving OPCs toward a myelinating phenotype. Notably, D6PV does not alter peripheral immune composition or autoimmune-driven pathology in the experimental autoimmune encephalomyelitis model, indicating a central nervous system (CNS) cell-autonomous effect. These findings reveal a metabolism-linked pathway for remyelination and position D6PV as a promising therapeutic strategy to enhance CNS repair in demyelinating diseases.",
      "author": "Moonen, B., Bolkaerts, L., Poelmans, K., Wouters, F., Kuipers, K., Guns, J., Verberk, S. G. S., Wolfs, E., Doering, Y., Jansen, Y., Vanmierlo, T., Dierckx, T., Gibson, E., Wolska, A., Reimund, M., Remaley, A. T., Loix, M., Hendriks, J. J. A., Bogie, J. F. J., Vanherle, S.",
      "published_date": "2026-01-24T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:52.284730+00:00",
      "updated_at": "2026-01-25T02:00:52.284735+00:00"
    },
    {
      "id": "116b3dd37c401a6913a364dc05dea34f",
      "url": "https://www.reddit.com/r/Python/comments/1qm3ags/i_built_a_tool_that_learns_your_codebase_patterns/",
      "title": "I built a tool that \"learns\" your codebase patterns. Plot twist: zero AI involved. Just AST parsing",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>I spent the last six months teaching myself to orchestrate engineering codebases using AI agents. What I found is that the biggest bottleneck isn\u2019t intelligence it\u2019s the context window. Why have we not given agents the proper tooling to defeat this limitation? Agents constantly forget how I handle error structures or which specific components I use for the frontend. This forces mass auditing and refactoring, causing me to spend about 75% of my token budget on auditing versus writing.</p> <p>What my project does:</p> <p>That is why I built Drift. Drift is a first-in-class codebase intelligence tool that leverages semantic learning through AST parsing with Regex fallbacks. It scans your codebase and extracts 15 different categories with over 150 patterns. Everything is persisted and recallable via CLI or MCP in your IDE of choice.</p> <p>What makes drift different?</p> <p>It\u2019s learning based not rule based. AI is capable of writing high quality code but the context limitation makes fitting conventions through a large code base extremely tedious and time consuming often leading to things silently failing or just straight up not working.</p> <p>Drift_context is the real magic</p> <p>Instead of an agent calling 10 tools and sytheneszing results it:</p> <p>Takes intent</p> <p>Takes focus area</p> <p>Returned a curated package</p> <p>This eliminates the audit loop, hallucination risk and gives the agent everything needed in one call.</p> <p>Call graph analysis across 6 different languages</p> <p>Not just \u201cWhat functions exists\u201d but..</p> <p>Drift_reachability_forward &gt; What data can this code access? (Massive for helping with security)</p> <p>Drift_reachability_inverse &gt; Who can access this field?</p> <p>Drift_impact_analysis &gt; what breaks if I change this with scoring.</p> <p>Security-audit-grade analysis available to you or your agent through MCP or CLI</p> <p>The MCP has been built out with frontier capabilities ensuring context is preserved and is a true tool for your agents</p> <p>Currently support TS, PY, Java, C#, PHP, GO :</p> <p>with\u2026</p> <p>Tree sitter parsing</p> <p>Regex fallback</p> <p>Framework aware detection</p> <p>All data persist into a local file (/.drift) and you have the ability to approve, deny and ignore certain components, functions and features you don\u2019t want the agent to be trained on.</p> <p>Comparison would be like the big brother to sonarqube</p> <p>Target audience is anyone who\u2019s building with ai looking to save tokens and have more consistent codebases! Also don\u2019t have to work with ai at all it has a fully operational cli \ud83e\udef6</p> <p>check it out here:</p> <p>IF you run into any edge cases or I don\u2019t support the framework your code base is currently running on open a git issue feature request and ive been banging them out quick</p> <p>Thank you for all the upvotes and stars on the project it means so much!</p> <p>check it out here: <a href=\"https://github.com/dadbodgeoff/drift\">https://github.com/dadbodgeoff/drift</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fluffy_Citron3547\"> /u/Fluffy_Citron3547 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qm3ags/i_built_a_tool_that_learns_your_codebase_patterns/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qm3ags/i_built_a_tool_that_learns_your_codebase_patterns/\">[comments]</a></span>",
      "author": "/u/Fluffy_Citron3547",
      "published_date": "2026-01-25T00:01:08+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 468,
      "reading_time": 2,
      "created_at": "2026-01-25T02:00:07.931232+00:00",
      "updated_at": "2026-01-25T02:00:07.931234+00:00"
    },
    {
      "id": "3da69ae2c84d9e15e1873416cad163fc",
      "url": "https://blocksandfiles.com/2026/01/19/a-window-into-hbf-progress/",
      "title": "High-bandwidth flash progress and future",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46700384\">Comments</a>",
      "author": "",
      "published_date": "2026-01-21T02:17:36+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:06.590473+00:00",
      "updated_at": "2026-01-25T02:00:06.590474+00:00"
    },
    {
      "id": "4d694482f94681ff0cd77d43fbbcdf2a",
      "url": "https://essenceia.github.io/projects/two_weeks_until_tapeout/",
      "title": "Two Weeks Until Tapeout",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46749671\">Comments</a>",
      "author": "",
      "published_date": "2026-01-25T01:25:37+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:06.590434+00:00",
      "updated_at": "2026-01-25T02:00:06.590436+00:00"
    },
    {
      "id": "df403e38edb5949dd28c8d9ceed6591d",
      "url": "https://keck.usc.edu/news/adoption-of-electric-vehicles-tied-to-real-world-reductions-in-air-pollution-study-finds/",
      "title": "Adoption of EVs tied to real-world reductions in air pollution: study",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46749198\">Comments</a>",
      "author": "",
      "published_date": "2026-01-25T00:14:40+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:06.590374+00:00",
      "updated_at": "2026-01-25T02:00:06.590375+00:00"
    },
    {
      "id": "5c6f275d8ceac0206809d41b4b70467f",
      "url": "https://eclypsium.com/blog/xray-counterfeit-usb-cable/",
      "title": "We X-Rayed a Suspicious FTDI USB Cable",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46749053\">Comments</a>",
      "author": "",
      "published_date": "2026-01-24T23:55:10+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:06.590350+00:00",
      "updated_at": "2026-01-25T02:00:06.590352+00:00"
    },
    {
      "id": "888c4db7f8ada8647c1fb05496f1f86e",
      "url": "https://theconversation.com/europe-wants-to-end-its-dangerous-reliance-on-us-internet-technology-274042",
      "title": "Europe wants to end its dangerous reliance on US internet technology",
      "content": "<p>Article URL: <a href=\"https://theconversation.com/europe-wants-to-end-its-dangerous-reliance-on-us-internet-technology-274042\">https://theconversation.com/europe-wants-to-end-its-dangerous-reliance-on-us-internet-technology-274042</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46748771\">https://news.ycombinator.com/item?id=46748771</a></p>\n<p>Points: 133</p>\n<p># Comments: 99</p>",
      "author": "DyslexicAtheist",
      "published_date": "2026-01-24T23:21:37+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:05.475025+00:00",
      "updated_at": "2026-01-25T02:00:05.475027+00:00"
    },
    {
      "id": "5c6f275d8ceac0206809d41b4b70467f",
      "url": "https://eclypsium.com/blog/xray-counterfeit-usb-cable/",
      "title": "We X-Rayed a Suspicious FTDI USB Cable",
      "content": "<p>Article URL: <a href=\"https://eclypsium.com/blog/xray-counterfeit-usb-cable/\">https://eclypsium.com/blog/xray-counterfeit-usb-cable/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46749053\">https://news.ycombinator.com/item?id=46749053</a></p>\n<p>Points: 44</p>\n<p># Comments: 9</p>",
      "author": "aa_is_op",
      "published_date": "2026-01-24T23:55:10+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:05.475005+00:00",
      "updated_at": "2026-01-25T02:00:05.475007+00:00"
    },
    {
      "id": "df403e38edb5949dd28c8d9ceed6591d",
      "url": "https://keck.usc.edu/news/adoption-of-electric-vehicles-tied-to-real-world-reductions-in-air-pollution-study-finds/",
      "title": "Adoption of EVs tied to real-world reductions in air pollution: study",
      "content": "<p>Article URL: <a href=\"https://keck.usc.edu/news/adoption-of-electric-vehicles-tied-to-real-world-reductions-in-air-pollution-study-finds/\">https://keck.usc.edu/news/adoption-of-electric-vehicles-tied-to-real-world-reductions-in-air-pollution-study-finds/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46749198\">https://news.ycombinator.com/item?id=46749198</a></p>\n<p>Points: 14</p>\n<p># Comments: 0</p>",
      "author": "hhs",
      "published_date": "2026-01-25T00:14:40+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:05.474985+00:00",
      "updated_at": "2026-01-25T02:00:05.474987+00:00"
    },
    {
      "id": "f0c96393ec216ff95d6d63b43eb8a7a5",
      "url": "https://lightbrd.com/ZacksJerryRig/status/2015119993428705575#m",
      "title": "Tesla unsupervised Robotaxis are nowhere to be found",
      "content": "<p>Article URL: <a href=\"https://lightbrd.com/ZacksJerryRig/status/2015119993428705575#m\">https://lightbrd.com/ZacksJerryRig/status/2015119993428705575#m</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46749319\">https://news.ycombinator.com/item?id=46749319</a></p>\n<p>Points: 12</p>\n<p># Comments: 2</p>",
      "author": "TheAlchemist",
      "published_date": "2026-01-25T00:33:46+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:05.474963+00:00",
      "updated_at": "2026-01-25T02:00:05.474965+00:00"
    },
    {
      "id": "3b83c06a72da4c02bc5b0ef853b24ef0",
      "url": "https://www.youtube.com/watch?v=7H3UTmFsE6g",
      "title": "If the Cops Are Unlawfully Shooting at Me, Can I Shoot Back? [video]",
      "content": "<p>Article URL: <a href=\"https://www.youtube.com/watch?v=7H3UTmFsE6g\">https://www.youtube.com/watch?v=7H3UTmFsE6g</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46749684\">https://news.ycombinator.com/item?id=46749684</a></p>\n<p>Points: 11</p>\n<p># Comments: 0</p>",
      "author": "MPSimmons",
      "published_date": "2026-01-25T01:27:44+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-25T02:00:05.474933+00:00",
      "updated_at": "2026-01-25T02:00:05.474941+00:00"
    },
    {
      "id": "77db3477c8d22ebbd43d1cb7c510512e",
      "url": "https://fmhy.net/posts/jan-2026",
      "title": "Monthly Updates [January]",
      "content": "<div class=\"info custom-block\"><p class=\"custom-block-title\">INFO</p>\n<p>These update threads only contains major updates. If you're interested\nin seeing all minor changes you can follow our\n<a href=\"https://github.com/fmhy/FMHYedit/commits/main\" rel=\"noreferrer\" target=\"_blank\">Commits Page</a> on GitHub or\n<a href=\"https://redd.it/17f8msf\" rel=\"noreferrer\" target=\"_blank\">Updates Channel</a> in Discord.</p>\n</div>\n<h1 id=\"wiki-updates\" tabindex=\"-1\">Wiki Updates <a class=\"header-anchor\" href=\"#wiki-updates\"></a></h1>\n<ul>\n<li>\n<p>Added <strong><a href=\"https://ffmhy.pages.dev/\" rel=\"noreferrer\" target=\"_blank\">Alternative Frontend</a></strong> of FMHY with totally different design. This pulls from the official source, so it will stay synced with new edits. It also has a <a href=\"https://i.ibb.co/fVkHqhRP/image.png\" rel=\"noreferrer\" target=\"_blank\">random site</a> / <a href=\"https://i.imgur.com/p4Mxs8y.png\" rel=\"noreferrer\" target=\"_blank\">2</a> button that works per page. Note the front page for this has been removed, and it now directs to the wiki itself. Thank you to nw for making this.</p>\n</li>\n<li>\n<p>Added <strong><a href=\"https://fmhy.net/other/backups\" rel=\"noreferrer\" target=\"_blank\">3 New Instances</a></strong> to our Backups Page. (Samidy, JBugel, ArtistGrid.)</p>\n</li>\n<li>\n<p>Added <strong><a href=\"https://i.ibb.co/Ps8vDJL0/image.png\" rel=\"noreferrer\" target=\"_blank\">Catppuccin</a></strong> / <a href=\"https://i.imgur.com/558l4gH.png\" rel=\"noreferrer\" target=\"_blank\">2</a> as a option in our color picker. Thank you to Samidy for doing this.</p>\n</li>\n<li>\n<p>Added new section for <a href=\"https://fmhy.net/reading#textbooks\" rel=\"noreferrer\" target=\"_blank\">Textbooks</a>.</p>\n</li>\n<li>\n<p>Added new section for <a href=\"https://fmhy.net/file-tools#file-info-metadata\" rel=\"noreferrer\" target=\"_blank\">File Info / Metadata</a>.</p>\n</li>\n<li>\n<p>Merged Linux + Mac Torrent Clients into <a href=\"https://fmhy.net/torrenting#torrent-clients\" rel=\"noreferrer\" target=\"_blank\">Main Section</a>, and added labels + platform icons. <a href=\"https://i.ibb.co/fV1zdt44/Untitled.jpg\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/THLiMqL.jpeg\" rel=\"noreferrer\" target=\"_blank\">2</a></p>\n</li>\n<li>\n<p>Re-ordered <a href=\"https://fmhy.net/reading#light-novels\" rel=\"noreferrer\" target=\"_blank\">Light Novel Sites</a> based on library size and UI, added labels, removed small libraries, and starred two that stood out (NovelCool + WuxiaClick). <a href=\"https://i.ibb.co/DSRfsrM/Untitled.png\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/IMRUQoo.png\" rel=\"noreferrer\" target=\"_blank\">2</a></p>\n</li>\n<li>\n<p>Debloated <a href=\"https://fmhy.net/audio#curated-recommendations\" rel=\"noreferrer\" target=\"_blank\">Audio Recommendation</a> section + added new section for <a href=\"https://fmhy.net/audio#song-artist-discovery\" rel=\"noreferrer\" target=\"_blank\">Song / Artist Discovery</a>. <a href=\"https://i.ibb.co/zTDNZgr9/Untitled.png\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/V6h0mQx.png\" rel=\"noreferrer\" target=\"_blank\">2</a></p>\n</li>\n<li>\n<p>Added Labels to both <a href=\"https://fmhy.net/linux-macos#cli-cheat-sheets\" rel=\"noreferrer\" target=\"_blank\">Linux Cheat Sheets</a> + <a href=\"https://fmhy.net/linux-macos#linux-distros\" rel=\"noreferrer\" target=\"_blank\">Linux Distros</a>, thank you to <a href=\"https://github.com/fmhy/edit/commit/9a9cd6dd47027ac63370b453ec86a943cdc0b9d6\" rel=\"noreferrer\" target=\"_blank\">helxop</a>. <a href=\"https://ibb.co/gMD8mKbw\" rel=\"noreferrer\" target=\"_blank\">Before vs After</a> / <a href=\"https://i.imgur.com/hqqLsCB.png\" rel=\"noreferrer\" target=\"_blank\">2</a></p>\n</li>\n<li>\n<p>Revamped our <a href=\"https://i.ibb.co/JjpzRXL7/image.png\" rel=\"noreferrer\" target=\"_blank\">Feedback Window</a> / <a href=\"https://i.imgur.com/nODLPVM.png\" rel=\"noreferrer\" target=\"_blank\">2</a> to try to improve its look. Thank you to Samidy for doing this.</p>\n</li>\n<li>\n<p>Added a backup / frontend for our <a href=\"https://fmhy-grading.pages.dev/\" rel=\"noreferrer\" target=\"_blank\">Grading Page</a>, and moved both our <a href=\"https://fmhy.net/other/backups\" rel=\"noreferrer\" target=\"_blank\">Backup</a> + <a href=\"https://fmhy.net/other/FAQ\" rel=\"noreferrer\" target=\"_blank\">FAQ</a> pages to our website instead of reddit/github. Thank you to Roi Goat + Samidy for doing this.</p>\n</li>\n<li>\n<p>Improved our <a href=\"https://github.com/fmhy/edit/pull/4386\" rel=\"noreferrer\" target=\"_blank\">theme handler</a>, which will help us more easily switch or add themes in the future (like halloween, christmas, etc.) Thank you to Land for doing this.</p>\n</li>\n<li>\n<p>Added 360 Total Security to unsafe sites. If you install any of their apps, it will keep giving the user popups to install their &quot;toolbox,&quot; or will install it without consent if you use clean, repair, or optimize options. Once installed, the toolbox will then <a href=\"https://en.wikipedia.org/wiki/Criticism_of_Qihoo_360#Malicious_promotion\" rel=\"noreferrer\" target=\"_blank\">modify your default apps</a> (such as your browser) and switch them all to 360 options.</p>\n</li>\n</ul>\n<hr />\n<h1 id=\"stars-added-\u2b50\" tabindex=\"-1\">Stars Added \u2b50 <a class=\"header-anchor\" href=\"#stars-added-\u2b50\"></a></h1>\n<ul>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video#download-sites\" rel=\"noreferrer\" target=\"_blank\">DDLBase</a> in Video Download, has both 4K + 1080p content, uses only fast hosts, has huge library.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video#anime-streaming\" rel=\"noreferrer\" target=\"_blank\">Anime Realms</a> in Anime Streaming. Good sources, nice UI, auto-next, custom player, multi-language.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/audio#telegram-bots\" rel=\"noreferrer\" target=\"_blank\">Music Hunters</a> in Telegram Audio Bots. Supports lots of sites, has good quality, easy to use.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/gaming#download-games\" rel=\"noreferrer\" target=\"_blank\">AstralGames </a> in Game Downloading, big library, fast hosts, pre-installs, allows requests.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/reading#manga\" rel=\"noreferrer\" target=\"_blank\">Comix</a> in Manga Sites, big library, allows uploads, nice UI, will be adding more languages soon.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/audio#streaming-sites\" rel=\"noreferrer\" target=\"_blank\">Monochrome</a> in Audio Streaming, no-signup needed, multi API integrations, last.fm integration, customizable, big library.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video\" rel=\"noreferrer\" target=\"_blank\">Rive, Aether, Willow, and FlyX</a> in Streaming. These have nice custom players, lots of features, fast streams, and impressive UIs.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/video#live-sports\" rel=\"noreferrer\" target=\"_blank\">StreamSports99</a> in Live Sports, has fast servers and solid UI.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/social-media-tools#twitter-x-customization\" rel=\"noreferrer\" target=\"_blank\">Control Panel for Twitter</a> Twitter/X Customization, adds a lot of extra features, lets you hide sections you don't use, improves UI, and much more.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/file-tools#file-hosts\" rel=\"noreferrer\" target=\"_blank\">Rootz</a> File Hosts, good speed, 25GB per file, lasts 15 days after last download.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/ai#image-generation\" rel=\"noreferrer\" target=\"_blank\">PigenAI</a> in Image Gen, has unlimited Imagen 4, Qwen, and Nano Banana.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/mobile#ios-streaming\" rel=\"noreferrer\" target=\"_blank\">Nuvio</a> in iOS Streaming, this uses Stremio Plugins, and has less limits than the &quot;Lite&quot; version currently available on iOS.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/mobile#android-torrenting\" rel=\"noreferrer\" target=\"_blank\">Flud</a> in Android Torrenting, allows binding, well maintained, ads can easily be blocked via DNS.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/misc#physical-health\" rel=\"noreferrer\" target=\"_blank\">Sleeping Guide</a> in Physical Health, sleep hygiene / science guide, seems to be well received so far, curated by Dan.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/gaming#remakes-recreations\" rel=\"noreferrer\" target=\"_blank\">OpenMW</a> in Remakes / Recreations, complete ground up reimplementation of Morrowind, <a href=\"https://wiki.openmw.org/index.php?title=Bethesda_Emails\" rel=\"noreferrer\" target=\"_blank\">approved by Bethesda</a> themselves.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/gaming#remakes-recreations\" rel=\"noreferrer\" target=\"_blank\">Clone Hero</a> in Remakes / Recreations, feature-rich Guitar Hero clone, thousands of songs + playlists, no lag, works with all guitar controllers, and supports midi drum kits.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/gaming#party-multiplayer\" rel=\"noreferrer\" target=\"_blank\">Gidd</a> in Multiplayer Browser Games, has quality versions of things like monopoly, geoguessr, card games, etc.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/system-tools#virtual-machines\" rel=\"noreferrer\" target=\"_blank\">QEMU</a> in Virtual Machines, performs well on non-windows systems, best used with Virt-Manager or Vagrantup.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/file-tools#file-backup\" rel=\"noreferrer\" target=\"_blank\">Restic</a> in File Backup, easy to use, multi-platform, performed well in benchmarks.</p>\n</li>\n<li>\n<p>Starred <a href=\"https://fmhy.net/ai#coding-ais\" rel=\"noreferrer\" target=\"_blank\">Gemini CLI</a> in Coding AIs, this seems to have some of the best limits as of now.</p>\n</li>\n</ul>\n<hr />\n<h1 id=\"things-removed\" tabindex=\"-1\">Things Removed <a class=\"header-anchor\" href=\"#things-removed\"></a></h1>\n<ul>\n<li>\n<p>Removed CrocDB as they've been <a href=\"https://i.ibb.co/RpDvJFgf/image.png\" rel=\"noreferrer\" target=\"_blank\">hit by DMCA</a> / <a href=\"https://i.imgur.com/iSV0MHq.png\" rel=\"noreferrer\" target=\"_blank\">2</a>.</p>\n</li>\n<li>\n<p>Removed IcebergCharts website as they've been <a href=\"https://i.ibb.co/XxMMwqDk/image.png\" rel=\"noreferrer\" target=\"_blank\">hacked a few times now</a> / <a href=\"https://i.imgur.com/MkvElvr.png\" rel=\"noreferrer\" target=\"_blank\">2</a>, and can't figure out why its happening. We may re-add in future if they figure it out. Note that we'll keep the subreddit in its place for now.</p>\n</li>\n<li>\n<p>Removed CanvasBlocker as its Firefox only, and FF has this built in now, it can also apparently make fingerprinting worse in some cases, and isn't maintained well anymore.</p>\n</li>\n<li>\n<p>Removed FTV and WAC from Live Sports as both are now gone.</p>\n</li>\n<li>\n<p>Removed Foxified as its suddenly <a href=\"https://i.ibb.co/4RzPsQ8Z/image.png\" rel=\"noreferrer\" target=\"_blank\">filling results with ads</a> / <a href=\"https://i.imgur.com/AjBMdfX.png\" rel=\"noreferrer\" target=\"_blank\">2</a>.</p>\n</li>\n<li>\n<p>Unstarred OnTheSpot as it will be losing Spotify Support soon.</p>\n</li>\n<li>\n<p>Unstarred Windsurf in Coding AIs as its limits and free models aren't as good anymore.</p>\n</li>\n</ul>",
      "author": "",
      "published_date": "2026-01-01T00:00:00+00:00",
      "source": "Fmhy",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1122,
      "reading_time": 5,
      "created_at": "2026-01-24T23:41:48.927473+00:00",
      "updated_at": "2026-01-25T01:25:33.565240+00:00",
      "metadata": {
        "processed_at": "2026-01-25T01:25:33.565250+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "26da94320919aca79c6f893cdc353a63",
      "url": "https://www.reddit.com/r/Python/comments/1qm1swm/score_exacte_fifa_virtuelle_24_4x4/",
      "title": "Score exacte FIFA virtuelle 24 4x4",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>Je veux que mon application soit installable et une application de pronostic FIFA virtuelle 24 4x4 dans le jeux. Sur les c\u00f4tes donne par le bookmaker 1XBET selons les histoires des scores exacte du dernier \u00e9v\u00e9nement et leurs c\u00f4tes, et une fois que je lui donne les c\u00f4tes de v1 et V2 qu'il puisse me g\u00e9n\u00e9r\u00e9 automatiquement les scores exacte et une probabilit\u00e9 rassurant par AI</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No_Marsupial9443\"> /u/No_Marsupial9443 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qm1swm/score_exacte_fifa_virtuelle_24_4x4/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qm1swm/score_exacte_fifa_virtuelle_24_4x4/\">[comments]</a></span>",
      "author": "/u/No_Marsupial9443",
      "published_date": "2026-01-24T22:59:52+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 87,
      "reading_time": 1,
      "created_at": "2026-01-24T23:41:47.554969+00:00",
      "updated_at": "2026-01-25T01:25:33.565254+00:00",
      "metadata": {
        "processed_at": "2026-01-25T01:25:33.565256+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "548f3cd93de572165bbbf83d05d2a9f4",
      "url": "https://www.reddit.com/r/Python/comments/1qm1uz3/web_scraping_change_detection_scrapes_the/",
      "title": "Web scraping - change detection (scrapes the underlying APIs not just raw selectors)",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>I was recently building a RAG pipeline where I needed to extract web data at scale. I found that many of the LLM scrapers that generate markdown are way too noisy for vector DBs and are extremely expensive.</p> <p><strong>What My Project Does</strong><br /> I ended up releasing what I built for myself: it's an easy way to run large scale web scraping jobs and only get changes to content you've already scraped. It can fully automate API calls or just extract raw HTML.</p> <p>Scraping lots of data is hard to orchestrate, requires antibot handling, proxies, etc. I built all of this into the platform so you can just point it to a URL, extract what data you want in JSON, and then track the changes to the content.</p> <p><strong>Target Audience</strong></p> <p>Anyone running scraping jobs in production - whether that's mass data extraction or monitoring job boards, price changes, etc. </p> <p><strong>Comparison</strong></p> <p>Tools like firecrawl and others use full browsers - this is slow and why these services are so expensive. This tool finds the underlying APIs or extracts the raw HTML with only requests - it's much faster and allows us to deterministically monitor for changes because we are only pulling out relevant data. </p> <p>The entire app runs through our python SDK! </p> <p>sdk: <a href=\"https://github.com/reverse/meter-sdk\">https://github.com/reverse/meter-sdk</a> </p> <p>homepage: <a href=\"https://meter.sh\">https://meter.sh</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ready-Interest-1024\"> /u/Ready-Interest-1024 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qm1uz3/web_scraping_change_detection_scrapes_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qm1uz3/web_scraping_change_detection_scrapes_the/\">[comments]</a></span>",
      "author": "/u/Ready-Interest-1024",
      "published_date": "2026-01-24T23:01:57+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 243,
      "reading_time": 1,
      "created_at": "2026-01-24T23:21:03.345126+00:00",
      "updated_at": "2026-01-25T01:25:33.565259+00:00",
      "metadata": {
        "processed_at": "2026-01-25T01:25:33.565261+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "6d0f6b6f41334f236a55497487dd4c32",
      "url": "https://www.reddit.com/r/Python/comments/1qm1kxu/how_i_went_down_a_massive_rabbit_hole_and_ended/",
      "title": "How I went down a massive rabbit hole and ended up building 4 libraries",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>A few months ago, I was in between jobs and hacking on a personal project just for fun. I built one of those automated video generators using an LLM. You know the type: the LLM writes a script, TTS narrates it, stock footage is grabbed, and it's all stitched together. Nothing revolutionary, just a fun experiment.</p> <p>I hit a wall when I wanted to add subtitles. I didn't want boring static text; I wanted styled, animated captions (like the ones you see on social media). I started researching Python libraries to do this easily, but I couldn't find anything &quot;plug-and-play.&quot; Everything seemed to require a lot of manual logic for positioning and styling.</p> <p>During my research, I stumbled upon a YouTube video called <em>&quot;Shortrocity EP6: Styling Captions Better with MoviePy&quot;</em>. At around the 44:00 mark, the creator said something that stuck with me: <em>&quot;I really wish I could do this like in CSS, that would be the best.&quot;</em></p> <p>That was the spark. I thought, <em>why not?</em> Why not render the subtitles using HTML/CSS (where styling is easy) and then burn them into the video?</p> <p>I implemented this idea using Playwright (using a headless browser) to render the HTML+CSS and then get the images. It worked, and I packaged it into a tool called <strong>pycaps</strong>. However, as I started testing it, it just felt wrong. I was spinning up an entire, heavy web browser instance just to render a few words on a transparent background. It felt incredibly wasteful and inefficient.</p> <p>I spent a good amount of time trying to optimize this setup. I implemented aggressive caching for Playwright and even wrote a custom rendering solution using OpenCV inside <code>pycaps</code> to avoid MoviePy and speed things up. It worked, but I still couldn't shake the feeling that I was using a sledgehammer to crack a nut.</p> <p>So, I did what any reasonable developer trying to avoid &quot;real work&quot; would do: I decided to solve these problems by building my own dedicated tools.</p> <p>First, weeks after releasing <code>pycaps</code>, I couldn't stop thinking about generating text images without the overhead of a browser. That led to <strong>pictex</strong>. Initially, it was just a library to render text using Skia (PICture + TEXt). Honestly, that first version was enough for what <code>pycaps</code> needed. But I fell into another rabbit hole. I started thinking, <em>&quot;What about having two texts with different styles? What about positioning text relative to other elements?&quot;</em> I went way beyond the original scope and integrated Taffy to support a full Flexbox-like architecture, turning it into a generic rendering engine.</p> <p>Then, to connect my original CSS templates from <code>pycaps</code> with this new engine, I wrote <strong>html2pic</strong>, which acts as a bridge, translating HTML/CSS directly into <code>pictex</code> render calls.</p> <p>Finally, I went back to my original AI video generator project. I remembered the custom OpenCV solution I had hacked together inside <code>pycaps</code> earlier. I decided to extract that logic into a standalone library called <strong>movielite</strong>. Just like with <code>pictex</code>, I couldn't help myself. I didn't simply extract the code. Instead, I ended up over-engineering it completely. I added Numba for JIT compilation and polished the API to make it a generic, high-performance video editor, far exceeding the simple needs of my original script.</p> <p><strong>Long story short:</strong> I tried to add subtitles to a video, and I ended up maintaining four different open-source libraries. The original &quot;AI Video Generator&quot; project is barely finished, and honestly, now that I have a full-time job and these four repos to maintain, it will probably never be finished. But hey, at least the subtitles render fast now.</p> <p>If anyone is interested in the tech stack that came out of this madness, or has dealt with similar performance headaches, here are the repos:</p> <ul> <li> <strong>pictex</strong> (The graphics engine): <a href=\"https://github.com/francozanardi/pictex\">https://github.com/francozanardi/pictex</a></li> <li> <strong>movielite</strong> (The video editor): <a href=\"https://github.com/francozanardi/movielite\">https://github.com/francozanardi/movielite</a></li> <li> <strong>html2pic</strong> (The HTML/CSS to image tool): <a href=\"https://github.com/francozanardi/html2pic\">https://github.com/francozanardi/html2pic</a></li> <li> <strong>pycaps</strong> (The subtitle tool that started it all): <a href=\"https://github.com/francozanardi/pycaps\">https://github.com/francozanardi/pycaps</a></li> </ul> <hr /> <p><strong>What My Project Does</strong></p> <p>This is a suite of four interconnected libraries designed for high-performance video and image generation in Python: * <strong>pictex:</strong> Generates images programmatically using Skia and Taffy (Flexbox), allowing for complex layouts without a browser. * <strong>pycaps:</strong> Automatically generates animated subtitles for videos using Whisper for transcription and CSS for styling. * <strong>movielite:</strong> A lightweight video editing library optimized with Numba/OpenCV for fast frame-by-frame processing. * <strong>html2pic:</strong> Converts HTML/CSS to images by translating markup into <code>pictex</code> render calls.</p> <p><strong>Target Audience</strong></p> <p>Developers working on video automation, content creation pipelines, or anyone needing to render text/HTML to images efficiently without the overhead of Selenium or Playwright. While they started as hobby projects, they are stable enough for use in automation scripts.</p> <p><strong>Comparison</strong></p> <ul> <li> <strong>pictex/html2pic vs. Selenium/Playwright:</strong> Unlike headless browsers, this stack does not require a browser engine. It renders directly using Skia, making it significantly faster and lighter on memory for generating images.</li> <li> <strong>movielite vs. MoviePy:</strong> MoviePy is excellent and feature-rich, but <code>movielite</code> focuses on performance using Numba JIT compilation and OpenCV.</li> <li> <strong>pycaps vs. Auto-subtitle tools:</strong> Most tools offer limited styling, <code>pycaps</code> allows CSS styling while maintaining a good performance.</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/_unknownProtocol\"> /u/_unknownProtocol </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qm1kxu/how_i_went_down_a_massive_rabbit_hole_and_ended/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qm1kxu/how_i_went_down_a_massive_rabbit_hole_and_ended/\">[comments]</a></span>",
      "author": "/u/_unknownProtocol",
      "published_date": "2026-01-24T22:50:51+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 877,
      "reading_time": 4,
      "created_at": "2026-01-24T23:21:03.344958+00:00",
      "updated_at": "2026-01-25T01:25:33.565263+00:00",
      "metadata": {
        "processed_at": "2026-01-25T01:25:33.565265+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "24165d369db806bbc89ffe920086f0f1",
      "url": "https://mynorthwest.com/local/amazon-layoffs-14000-jobs-at-risk/4192118",
      "title": "Amazon braces for another major round of layoffs, 14,000 jobs at risk",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46748603\">Comments</a>",
      "author": "",
      "published_date": "2026-01-24T22:59:45+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-24T23:21:02.009238+00:00",
      "updated_at": "2026-01-25T01:25:33.565267+00:00",
      "metadata": {
        "processed_at": "2026-01-25T01:25:33.565268+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "24165d369db806bbc89ffe920086f0f1",
      "url": "https://mynorthwest.com/local/amazon-layoffs-14000-jobs-at-risk/4192118",
      "title": "Amazon braces for another major round of layoffs, 14,000 jobs at risk",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46748603\">Comments</a>",
      "author": "",
      "published_date": "2026-01-24T22:59:45+00:00",
      "source": "Hacker News",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-24T23:21:02.009238+00:00",
      "updated_at": "2026-01-25T01:25:33.565267+00:00",
      "metadata": {
        "processed_at": "2026-01-25T01:25:33.565268+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7895939116a617cfeb97b8721303d331",
      "url": "https://arstechnica.com/security/2026/01/wiper-malware-targeted-poland-energy-grid-but-failed-to-knock-out-electricity/",
      "title": "Poland's energy grid was targeted by never-before-seen wiper malware",
      "content": "<p>Article URL: <a href=\"https://arstechnica.com/security/2026/01/wiper-malware-targeted-poland-energy-grid-but-failed-to-knock-out-electricity/\">https://arstechnica.com/security/2026/01/wiper-malware-targeted-poland-energy-grid-but-failed-to-knock-out-electricity/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46747827\">https://news.ycombinator.com/item?id=46747827</a></p>\n<p>Points: 19</p>\n<p># Comments: 0</p>",
      "author": "Bender",
      "published_date": "2026-01-24T21:24:13+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-24T22:44:19.598335+00:00",
      "updated_at": "2026-01-24T22:44:19.598342+00:00"
    },
    {
      "id": "80d1f642eb4da0958753076107ac0cbe",
      "url": "http://daniellakens.blogspot.com/2025/07/easily-download-files-from-open-science.html",
      "title": "Easily download files from the Open Science Framework with Papercheck",
      "content": "<p>Researchers\nincreasingly use the <a href=\"https://osf.io/\">Open Science Framework</a> (OSF) to share files, such as data\nand code underlying scientific publications, or presentations and materials for\nscientific workshops. The OSF is an amazing service that has contributed\nimmensely to a changed research culture where psychologists share data, code, and\nmaterials. We are very grateful it exists.</p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">But it is\nnot always the most user-friendly. Specifically, downloading files from the OSF\nis a bigger hassle than we (<a href=\"https://debruine.github.io/\">Lisa DeBruine</a>\nand <a href=\"https://www.tue.nl/en/research/researchers/daniel-lakens/\">Daniel\nLakens</a>, the developers of Papercheck) would like it to be. Downloading individual\nfiles is so complex, <a href=\"https://bsky.app/profile/malte.the100.ci/post/3lpf4s6spns2i\">Malte Elson</a>\nrecently posted this meme on Bluesky. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"NL\"><!--[if gte vml 1]><v:shapetype\n id=\"_x0000_t75\" coordsize=\"21600,21600\" o:spt=\"75\" o:preferrelative=\"t\"\n path=\"m@4@5l@4@11@9@11@9@5xe\" filled=\"f\" stroked=\"f\">\n <v:stroke joinstyle=\"miter\"/>\n <v:formulas>\n  <v:f eqn=\"if lineDrawn pixelLineWidth 0\"/>\n  <v:f eqn=\"sum @0 1 0\"/>\n  <v:f eqn=\"sum 0 0 @1\"/>\n  <v:f eqn=\"prod @2 1 2\"/>\n  <v:f eqn=\"prod @3 21600 pixelWidth\"/>\n  <v:f eqn=\"prod @3 21600 pixelHeight\"/>\n  <v:f eqn=\"sum @0 0 1\"/>\n  <v:f eqn=\"prod @6 1 2\"/>\n  <v:f eqn=\"prod @7 21600 pixelWidth\"/>\n  <v:f eqn=\"sum @8 21600 0\"/>\n  <v:f eqn=\"prod @7 21600 pixelHeight\"/>\n  <v:f eqn=\"sum @10 21600 0\"/>\n </v:formulas>\n <v:path o:extrusionok=\"f\" gradientshapeok=\"t\" o:connecttype=\"rect\"/>\n <o:lock v:ext=\"edit\" aspectratio=\"t\"/>\n</v:shapetype><v:shape id=\"_x0000_i1026\" type=\"#_x0000_t75\" style='width:453.75pt;\n height:276pt;visibility:visible;mso-wrap-style:square'>\n <v:imagedata src=\"file:///C:/Users/DLakens/AppData/Local/Temp/msohtmlclip1/01/clip_image001.jpg\"\n  o:title=\"\"/>\n</v:shape><![endif]--><!--[if !vml]--><img border=\"0\" height=\"368\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEinS5tFoL5qX14Z2OcgT1APMZLGlghAD3BfBhSnJ9pleVbJyRFUFLShqReS2j7SXGozmLMcVQkoM62UhneJDtH4yx3NRnXOkVZZi-Dm-OPwbGaidxr2raF9wzjx5fU92nfPpE3jmGfwZEwHS1WGSB4gUfRfV3XWjOC2-lCjOYR108h3fwORIHOnqxIX9m8\" width=\"605\" /><!--[endif]--></span><span lang=\"EN-US\"></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span>&nbsp;</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Not only is\nthe download button for files difficult to find, but downloading all files\nrelated to a project can be surprisingly effortful. It is possible to download\nall files in a zip folder that will be called \u2018osfstorage-archive.zip\u2019 when\ndownloaded. But as the OSF supports a nested folder structure, you might miss a\nfolder, and you will quickly end up with \u2018osfstorage-archive (1).zip\u2019, \u2018osfstorage-archive\n(2).zip\u2019, etc. Unzipping these archives creates a lot of files without the\norganized folder structure, in folders with meaningless names, making it\ndifficult to understand where files are. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<h2 style=\"text-align: left;\"><b><span lang=\"EN-US\">The\nosf_file_download function in Papercheck </span></b></h2>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">We have added\na new function to our R package \u2018<a href=\"https://scienceverse.github.io/papercheck/\">Papercheck</a>\u2019 that will download all files and\nfolders in an OSF repository. It saves all files by recreating the folder\nstructure from the OSF in your download folder. Just install Papercheck, load\nthe library, and use the osf_file_download function to grab all files on the OSF:</span></p><p class=\"MsoNormal\"><span lang=\"EN-US\"><br /></span></p>\n\n<div style=\"text-align: left;\"><span style=\"font-family: courier;\"><b><span lang=\"EN-US\">devtools::install_github(\"scienceverse/papercheck\")<br /></span></b><b><span lang=\"EN-US\">library(papercheck)<br /></span></b><b><span lang=\"EN-US\">osf_file_download(\"6nt4v\")</span></b></span></div>\n\n\n\n\n\n<p class=\"MsoNormal\"><b><span lang=\"EN-US\">&nbsp;</span></b></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">All files\nwill be downloaded to your working directory. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Are you feeling\nFOMO for missing out on the <a href=\"https://www.kcl.ac.uk/events/open-research-summer-school-2025\">King Open\nResearch Summer School</a> that is going on these days, where <a href=\"https://research.tue.nl/en/persons/sajedeh-rasti\">Sajedeh Rasti</a> talked\nabout Preregistration, and <a href=\"https://research.tue.nl/en/persons/cristian-mesquida-caldentey\">Cristian\nMesquida</a> will give a workshop on using Papercheck? Well, at least it is\nvery easy to download all the files they have shared on the OSF and look at the\npresentations: </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"b7es8\")</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">In the\noutput, we see that by default large files (more than 10mb) are omitted. <span><!--[if gte vml 1]><v:shape id=\"Picture_x0020_1\"\n o:spid=\"_x0000_i1025\" type=\"#_x0000_t75\" alt=\"A screenshot of a computer&#10;&#10;AI-generated content may be incorrect.\"\n style='width:453.75pt;height:292.5pt;visibility:visible;mso-wrap-style:square'>\n <v:imagedata src=\"file:///C:/Users/DLakens/AppData/Local/Temp/msohtmlclip1/01/clip_image003.png\"\n  o:title=\"A screenshot of a computer&#10;&#10;AI-generated content may be incorrect\"/>\n</v:shape><![endif]--><!--[if !vml]--><img alt=\"A screenshot of a computer\n\nAI-generated content may be incorrect.\" border=\"0\" height=\"390\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEj2jH76ywmEeLzL43u6gJl7GKR2lEGlhYS0LOXRPMMovOsJEVdXyamYCvmq96ez3p_GXHLoFz4JDyAKHlARK9pSy8QfzVa7yt1_uEg_H9IJfKgunnYWUwlROXABNcU6TvrA0_hx0KPZfj00b3Cb-Wn_7Bf3sFu9JApZoClcWoh_Vfl5bk1GQVZUH1tuGao\" width=\"605\" /><!--[endif]--></span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">If you want\nto download all files, regardless of the size, then set the parameter to ignore\nthe maximum file size: </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"b7es8\",\nmax_file_size = NULL)</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Sometimes\nyou might want to download all files but ignore the file structure on the OSF,\nto just have all the files in one folder. Setting the parameter ignore_folder_structure\n= TRUE will give you all the files on the OSF in a single folder. By default, files will be downloaded into your working directory, but you can also specify\nwhere you want the files to be saved. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"6nt4v\",\nignore_folder_structure = TRUE, download_to = \"C:\\\\test_download\")</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">We hope\nthis function will make it easier for reviewers to access all supplementary\nfiles stored on the OSF during peer review, and for researchers who want to re-use\ndata, code, or materials shared on the OSF by downloading all the files they\nneed easily. Make sure to install the latest version of Papercheck (0.0.0.9050)\nto get access to this new function. Papercheck is still in active development,\nso report any bugs on <a href=\"https://github.com/scienceverse/papercheck\">GitHub</a>.</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>",
      "author": "noreply@blogger.com (Daniel Lakens)",
      "published_date": "2025-07-22T09:53:00+00:00",
      "source": "Twenty Percent Statistician",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 765,
      "reading_time": 3,
      "created_at": "2026-01-24T22:22:06.329461+00:00",
      "updated_at": "2026-01-24T22:22:06.329463+00:00"
    }
  ]
}