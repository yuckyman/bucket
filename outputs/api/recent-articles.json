{
  "last_updated": "2026-01-22T04:43:35.023317+00:00",
  "count": 20,
  "articles": [
    {
      "id": "b915afb406e73bb7a7c1832608ee2968",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.17.699936v1?rss=1",
      "title": "The amplitude and latency of the earliest signal in V1 encode bottom-up saliency by feature conjunction",
      "content": "The neural origin of bottom-up saliency for exogenous attention remains highly controversial. In this study, we investigated whether the earliest activity in the primary visual cortex (V1) encodes saliency signals defined by the eye-of-origin and feature-conjunction information. Electroencephalography (EEG) recordings from the human occipital cortex revealed early responses to eye-of-origin (E) and/or orientation (O) singletons, with larger response amplitudes to the double-feature (EO) singletons. The short onset latency (58-70 ms) and polarity reversal of the responses indicate a V1 origin. Importantly, the latency and amplitude of these responses predicted behavioral detection performance. Together, these findings suggest that the timing and amplitude of the earliest signals in V1 represent the saliency of combined feature contrasts for bottom-up attention. These signals unlikely originate from projections of other proposed source areas of saliency, due to the scarcity of necessary monocular neurons to process eye-of-origin information.",
      "author": "Wu, C., Li, X., Li, H., Wang, X., Yin, Z., Wang, Z., Zhang, P., Yang, Z., Zou, J.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 142,
      "reading_time": 1,
      "created_at": "2026-01-22T03:50:05.337222+00:00",
      "updated_at": "2026-01-22T04:43:34.915607+00:00",
      "metadata": {
        "processed_at": "2026-01-22T04:43:34.915617+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b37601d09093bbd8cc42468419126021",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.17.700125v1?rss=1",
      "title": "From cognitive abstraction to adaptive behavior: neural bases of concept learning in autistic adolescents",
      "content": "BACKGROUND: Learned knowledge does not consistently generalize to new contexts in autistic individuals, limiting potential for adapting to real-world demands. This challenge is hypothesized to stem from difficulties with forming abstract representations, potentially arising from perceptual processing that favors local details over the gestalt. We tested the prediction that generalization would be primarily based on exemplar-specific representations in autistic youth using computational modelling coupled with neuroimaging. METHODS: Sixty-four autistic adolescents without intellectual disability (69% males; ages 14-18 years) completed a category generalization task during functional magnetic resonance imaging at two time points. Computational models estimated abstract (prototype-based) and specific (exemplar-based) representations and underlying neural correlates. We further examined associations with adaptive functioning and moderation by autistic traits. RESULTS: Contrary to predictions, we observed a consistent prototype-dominant majority, a subgroup who generalized without consistent representational reliance, and a small minority who failed to acquire category structure. Prototypes were represented in bilateral ventromedial prefrontal cortex (VMPFC), inferior parietal lobule (IPL), right frontal pole, and right lateral occipital cortex, while exemplars were represented in bilateral cuneus. Better generalization predicted better real-world adaptive functioning. Moreover, greater prototype-related activation in left IPL predicted better adaptive functioning in participants with higher autistic traits. CONCLUSIONS: These findings challenge the prevailing view that concept learning in autism relies primarily on hyper-specific perceptual processing, identify meaningful variability in representational strategies, and reveal neural pathways through which abstract representation may support real-world adaptive behavior.",
      "author": "Chen, Y., Hawkins, B., Puckett, H., Sharp, K., Lopez, A., Zeithamova, D., Xie, H., Verbalis, A., VanMeter, A. S., Gaillard, W. D., Kenworthy, L., Vaidya, C. J.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2026-01-22T03:50:05.337190+00:00",
      "updated_at": "2026-01-22T04:43:34.915621+00:00",
      "metadata": {
        "processed_at": "2026-01-22T04:43:34.915623+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b33b472cdb8563c7c43c617b484ebda6",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.18.700161v1?rss=1",
      "title": "Divergent excitatory and inhibitory signaling in a head direction circuit",
      "content": "Neural cell types are often clustered into inhibitory, excitatory, or neuromodulatory populations. However, the signaling mechanism between two neurons ultimately depends on their neurotransmitter-receptor pairings. Here, we demonstrate an example of a glutamatergic population of neurons that appears to either excite or inhibit their downstream partners depending on the type of glutamate receptor expressed in the downstream cell type. The upstream population encodes a sinusoidal head direction signal. The downstream partners that are excited by glutamate encode the same signal while the downstream partners that are inhibited by glutamate encode a 180$^degree$ phase-shifted copy. The upstream population therefore appears to pass on two different phases of the same signal by making either excitatory or inhibitory connections to different downstream partners.",
      "author": "Eddy, J., Shenasa, A. H., Monroy Alfaro, P., Fernanda Viveros, M., Turner-Evans, D. B.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 120,
      "reading_time": 1,
      "created_at": "2026-01-22T03:50:05.337140+00:00",
      "updated_at": "2026-01-22T04:43:34.915625+00:00",
      "metadata": {
        "processed_at": "2026-01-22T04:43:34.915627+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3940bc586d485dfa10ff0242facb774a",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.17.700100v1?rss=1",
      "title": "Memory erasure by dopamine-gated retrospective learning",
      "content": "Erasing outdated memories is crucial for adaptive behavior. Yet once a cue-outcome association is learned, repeated cue exposure without outcome suppresses conditioned behavior without erasing the underlying memory. This allows rapid behavioral recovery when outcomes are reintroduced. Here, we confirm this limitation for standard prospective extinction protocols that present cues without the associated outcome, but show that true memory erasure is achieved by inverting the paradigm: presenting outcomes without associated cues, i.e., retrospective extinction. We demonstrate that orbitofrontal cortex activity at outcome is necessary for the rapid behavioral recovery following prospective extinction, and that mesolimbic dopamine activity at outcome is necessary for retrospective extinction. These findings reconceptualize extinction mechanisms and suggest complementary strategies to mitigate relapse and erase maladaptive memories.",
      "author": "Jeong, H., Zsembik, L., Farouq, F., Chakraborty, R., Belur, N., Zhou, M., Sanders, A. D., Wang, S. X., Srinivasan, A., Cox, S. M. L., Garr, E., Brooke, S., Janak, P. H., Leyton, M., Chen, R., Namboodiri, V. M. K.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 120,
      "reading_time": 1,
      "created_at": "2026-01-22T03:50:05.337110+00:00",
      "updated_at": "2026-01-22T04:43:34.915629+00:00",
      "metadata": {
        "processed_at": "2026-01-22T04:43:34.915630+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "3f1b7dbb004c0c5cfefdd86a8ce6e33f",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.18.698509v1?rss=1",
      "title": "Minimal Mimics and Maps of Natural Light for Mammals",
      "content": "Light drives processes that include perception and the regulation of circadian rhythms, sleep, metabolism, and development. These processes are initiated by photopigment molecules, each preferentially absorbing particular wavelengths. Light of a given spectrum stimulates an animal's set of photopigments in a specific profile. Natural light and its variations produce stimulation profiles that promote normal physiology. To mimic these profiles using artificial light, we consider the thermally stable, photoconvertible states of relevant photopigments: ground states of rhodopsin and cone photopigments, and three states of melanopsin. This gives a high-dimensional representation of illumination. Nevertheless, we find that two wavelengths suffice to closely mimic the effects of natural light for mammals, including humans and mice. Adjusting the wavelength ratio allows mimicry of natural light's variations, such as those from twilight to noon. Ratio adjustments also compensate for light's filtering by elements like the eye's optics and laboratory cages. Adding a third wavelength makes natural light mimicry nearly perfect. By contrast, common artificial lighting--designed for low-dimensional, human color space--stimulates photopigments in unnatural proportions. We conclude by providing species-specific maps of photopigment stimulation profiles under natural and artificial illumination, which make our observations intuitive while providing insight into the diverse visual ecologies of mammals.",
      "author": "Morquette, P., Do, M. T. H.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 199,
      "reading_time": 1,
      "created_at": "2026-01-22T03:50:05.337078+00:00",
      "updated_at": "2026-01-22T04:43:34.915633+00:00",
      "metadata": {
        "processed_at": "2026-01-22T04:43:34.915635+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "55ec7de400ee82646891185d1cd2e293",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.18.699677v1?rss=1",
      "title": "EEG-Based Decoding of Color and Visual Category Representations Is Reliable Within and Across Sessions",
      "content": "The human visual system represents stimuli in a rich and detailed manner. Traditional methods of studying visual representations in humans, such as event-related potentials (ERP), revealed numerous distinctions between the brain activity elicited by different categories of stimuli. However, these methods miss the information embedded in the spatial distributions of brain activity, or patterns, and are not always sensitive to study visual representations of different stimuli at the single participant or single trial level. Time-resolved multivariate pattern classification analysis (MVPA), or Decoding, efficiently extracts the visual representations of stimuli from the EEG topography without a-priori assumptions about the location of the effect in time and space at the single participant level. The rich information this method provides has increased its popularity dramatically in recent years. Yet, different participants show variable quality of decoding performance, and it is unclear if the accuracy of decoding is maintained within participants across multiple sessions, tasks, attentional conditions and visual features. In the current study, participants performed three visual tasks, over two sessions (1-7 days apart). We examined the correlation of decoding accuracy: within the cross-validation set, between sessions, between features (color and category) and to different measurements of the ERP signal and behavioral performance. We also examined how models generalized to different tasks and different attention conditions. We found that decoding accuracies varied substantially across participants, and that decoding accuracy was reliable within participant, over sessions, attention condition and task. This suggests the decodability behaves like an individual trait. Moreover, the spatial patterns underlying the decoding (classification weights) generalized across different tasks, attentional conditions and sessions. This suggests minimal representational drift at the resolution allowed by the EEG. We conclude that EEG decoding is a reliable method, and that visual representations are stable.",
      "author": "Frenkel, C., Deouell, L. Y.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 289,
      "reading_time": 1,
      "created_at": "2026-01-22T03:50:05.337034+00:00",
      "updated_at": "2026-01-22T03:50:05.337039+00:00"
    },
    {
      "id": "013d768606abe824c9ff3c71c3cff317",
      "url": "https://www.nature.com/articles/s41467-026-68763-z",
      "title": "No evidence of immediate or persistent analgesic effect from a single dose of psilocybin in three mouse models of pain",
      "content": "",
      "author": "",
      "published_date": "2026-01-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-22T03:50:03.999303+00:00",
      "updated_at": "2026-01-22T03:50:03.999305+00:00"
    },
    {
      "id": "50c75745ac996fd28f573f487daa6553",
      "url": "https://www.youtube.com/watch?v=Hju0H3NHxVI",
      "title": "Mote: An Interactive Ecosystem Simulation [video]",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46712547\">Comments</a>",
      "author": "",
      "published_date": "2026-01-21T22:30:18+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-22T03:49:22.782128+00:00",
      "updated_at": "2026-01-22T03:49:22.782129+00:00"
    },
    {
      "id": "821a9a1d334a8d0100c9442a15d2b8da",
      "url": "https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/",
      "title": "Threat actors expand abuse of Microsoft Visual Studio Code",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46713526\">Comments</a>",
      "author": "",
      "published_date": "2026-01-22T00:12:00+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-22T03:49:22.782053+00:00",
      "updated_at": "2026-01-22T03:49:22.782054+00:00"
    },
    {
      "id": "50c75745ac996fd28f573f487daa6553",
      "url": "https://www.youtube.com/watch?v=Hju0H3NHxVI",
      "title": "Mote: An Interactive Ecosystem Simulation [video]",
      "content": "<p><a href=\"https://www.tiktok.com/@recursecenter/video/7597943894319369502\" rel=\"nofollow\">https://www.tiktok.com/@recursecenter/video/7597943894319369...</a></p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46712547\">https://news.ycombinator.com/item?id=46712547</a></p>\n<p>Points: 9</p>\n<p># Comments: 0</p>",
      "author": "evakhoury",
      "published_date": "2026-01-21T22:30:18+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 14,
      "reading_time": 1,
      "created_at": "2026-01-22T03:49:21.427514+00:00",
      "updated_at": "2026-01-22T03:49:21.427516+00:00"
    },
    {
      "id": "821a9a1d334a8d0100c9442a15d2b8da",
      "url": "https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/",
      "title": "Threat actors expand abuse of Microsoft Visual Studio Code",
      "content": "<p>Article URL: <a href=\"https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/\">https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46713526\">https://news.ycombinator.com/item?id=46713526</a></p>\n<p>Points: 39</p>\n<p># Comments: 20</p>",
      "author": "vinnyglennon",
      "published_date": "2026-01-22T00:12:00+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-22T03:49:21.427475+00:00",
      "updated_at": "2026-01-22T03:49:21.427477+00:00"
    },
    {
      "id": "3a385b76129c5dd22525258599d0d66b",
      "url": "https://operand.online/chronicle/pass.zoom",
      "title": "I'll pass on your zoom call",
      "content": "<p>Article URL: <a href=\"https://operand.online/chronicle/pass.zoom\">https://operand.online/chronicle/pass.zoom</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46713704\">https://news.ycombinator.com/item?id=46713704</a></p>\n<p>Points: 34</p>\n<p># Comments: 23</p>",
      "author": "c4lliope",
      "published_date": "2026-01-22T00:34:20+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-22T03:49:21.427456+00:00",
      "updated_at": "2026-01-22T03:49:21.427457+00:00"
    },
    {
      "id": "4a8d76bc1c56d0d0fb4f8beabd88a7c9",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.19.700413v1?rss=1",
      "title": "Afterimages drive a shared visual motion reversal illusion in Drosophila",
      "content": "Illusions expose core computations in perception. In one visual apparent-motion illusion, perceptual direction is reversed when phase shifted gratings are interleaved with uniform frames. Here, we demonstrate that Drosophila exhibits the same direction reversal reported in mammals. Combining behavior, targeted silencing, two photon imaging, and modeling, we localize the origin of this illusion to elementary motion pathways. Silencing direction selective T4/T5 neurons abolishes the reversal, and recordings reveal that downstream wide field neurons invert their directional preference as interleave duration increases. Replacing periodic gratings with random binary patterns preserves the reversal, implicating afterimages rather than spatial periodicity. Imaging neurons upstream of T4/T5 shows signatures of an afterimage, whose emergence depends on interleave luminance. Critically, dark interleaves suppress afterimages and eliminate both the neural and behavioral reversal, whereas light interleaves preserve or enhance it. Thus, afterimages are central to this shared illusion and explain a deficiency of canonical motion energy accounts. These results link a classic apparent motion phenomenon to identified circuit elements and reveal a simple stimulus manipulation that switches an illusion on and off.",
      "author": "Wu, H., Gou, T., Clark, D.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 175,
      "reading_time": 1,
      "created_at": "2026-01-22T01:53:54.452381+00:00",
      "updated_at": "2026-01-22T03:43:45.208271+00:00",
      "metadata": {
        "processed_at": "2026-01-22T03:43:45.208281+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "7f3077de5a05a093d21d0da16e658dc9",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.17.700089v1?rss=1",
      "title": "Impact of impaired endogenous neurosteroidogenesis on outcomes following chronic alcohol exposure.",
      "content": "Alcohol use disorder is a major public health concern worldwide and there is a high comorbidity with psychiatric disorders. The basolateral amygdala (BLA) has been implicated in both mood and alcohol use disorders; however, the mechanisms contributing to the shared pathophysiology remain unknown. Extensive evidence indicates that ethanol modulates GABAergic signaling in the BLA, including actions on neurosteroid-sensitive, extrasynaptic {delta} subunit-containing GABAA receptors (GABAARs), which has been suggested to mediate many of the behavioral effects. In fact, several studies have suggested that 5-reduced neurosteroids, such as allopregnanolone, may mediate some of the behavioral effects of alcohol. Here we demonstrate that chronic intermittent ethanol (CIE) exposure impairs endogenous neurosteroidogenesis via downregulation of key neurosteroidogenic enzymes, 5-reductase type 1 and type 2. To examine the impact of impaired endogenous neurosteroidogenesis of the behavioral consequences of chronic alcohol exposure, including withdrawal-induced anxiety and increased alcohol consumption, we used CRISPR/Cas9 mediated knockdown of 5-reductase in the BLA. Reduced expression of 5-reductase in the BLA did not impact post-CIE alcohol intake or anxiety-like behaviors during withdrawal, perhaps because endogenous neurosteroidogenesis is already impaired following CIE. Therefore, we examined the impact of enhancing neurosteroid levels, treating mice post-CIE with SGE-516, a synthetic GABAAR positive allosteric modulator, which increased voluntary alcohol intake. These findings implicate endogenous neurosteroidogenesis in behavioral outcomes associated with withdrawal from chronic alcohol exposure. Further, this study suggests that targeting endogenous neurosteroidogenesis may be a novel and useful therapeutic target.",
      "author": "Blandino, K., He, Y., Htet, L., Okoudjou, S., Lee, J., Chinatti, M., Ahn, K., Lewis, M., Gray, S., Miczek, K., Maguire, J.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 236,
      "reading_time": 1,
      "created_at": "2026-01-22T01:53:54.452347+00:00",
      "updated_at": "2026-01-22T03:43:45.208285+00:00",
      "metadata": {
        "processed_at": "2026-01-22T03:43:45.208287+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a8c73f1c9051e9948b5c2819ac546f29",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.18.700150v1?rss=1",
      "title": "Perceptual reaction times are coupled to the gastric electrical rhythm",
      "content": "The gastric rhythm, a slow (0.05 Hz) electrical oscillation continuously generated by the stomach, phase-synchronises neural activity across all sensory and motor cortices. Yet the functional relevance of this coupling remains unknown. We measured in humans the gastric rhythm with electrogastrography in three independent, distinct experiments (total N = 90): a speeded supra-threshold tactile detection task, a self-paced visual exploration task, and an unspeeded near-threshold visual detection task. Leveraging Spline Generalized Linear Modelling to flexibly uncover higher-order phase-amplitude coupling modes, we found perceptual reaction times (RTs) to be coupled to gastric phase across all experiments. Coupling to gastric phase was independent from effects of cardiac and respiratory rhythms' rates and phases. By showing that the gastric rhythm contributes to the internal state shaping perceptual variability on a scale comparable to cardiac and respiratory rhythms, this work opens a new avenue for the understanding of the function of gastric-brain coupling.",
      "author": "Loescher, M., Wolpert, N., Tallon-Baudry, C.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 149,
      "reading_time": 1,
      "created_at": "2026-01-22T01:53:54.452308+00:00",
      "updated_at": "2026-01-22T03:43:45.208289+00:00",
      "metadata": {
        "processed_at": "2026-01-22T03:43:45.208291+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "89e4d8dd4499c17888db869fea49ab69",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.17.700122v1?rss=1",
      "title": "How Visual Context Influences Lateral Stepping Regulation While Walking on Winding Paths",
      "content": "Goal-directed walking involves regulating foot placements to achieve specific tasks. This requires visuomotor integration. Perceptual, cognitive, and contextual salience guide attention and motor planning for navigation. Here, we quantified how perceptual salience informs lateral foot placement while walking. Participants walked along prescribed virtual paths (straight or winding), thus keeping contextual salience (the task itself) constant. We manipulated perceptual salience by systematically altering environment richness (rich vs. sparse) and path color contrast (high vs. low). We thus tested the extent to which stepping control is determined only by the walking path (i.e. its shape), or is also influenced by task-relevant salience of the paths (only), and/or the salience of the surrounding environment. We quantified head pitch angle to approximate gaze direction. We quantified lateral stepping regulation from a Goal Equivalent Manifold framework. On winding (vs. straight) paths, participants looked down more (lower mean head pitch) and more consistently (less variable head pitch). They took narrower, more variable steps and corrected stepping errors more strongly. This confirmed the predominant influence of task (contextual salience) on stepping control. On low (vs. high) contrast paths, participants looked down more and exhibited greater variability in lateral position on their path with weaker error correction. Higher contrast paths elicited stronger and more consistent stepping control. Walking in sparse (vs. rich) environments yielded somewhat less consistent, but still significant changes in head pitch and stepping regulation. Salience manipulations affected stepping differently on straight versus winding paths. Therefore, perceptual salience influences step-to-step control during naturalistic walking.",
      "author": "Render, A. C., Singh, T., Cusumano, J. P., Dingwell, J. B.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 248,
      "reading_time": 1,
      "created_at": "2026-01-22T01:53:54.452269+00:00",
      "updated_at": "2026-01-22T03:43:45.208293+00:00",
      "metadata": {
        "processed_at": "2026-01-22T03:43:45.208294+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "de01070327e20663e6e24a2be4d2664a",
      "url": "https://www.nature.com/articles/s41537-026-00732-3",
      "title": "Substantia nigra pars reticulata involvement in auditory hallucinations of treatment-resistant schizophrenia: a deep brain stimulation case report",
      "content": "",
      "author": "",
      "published_date": "2026-01-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-22T01:53:53.116930+00:00",
      "updated_at": "2026-01-22T03:43:45.208297+00:00",
      "metadata": {
        "processed_at": "2026-01-22T03:43:45.208298+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "88285c271622739eceb1a67d95daf73f",
      "url": "https://www.nature.com/articles/s42003-026-09550-w",
      "title": "Distinct contributions of prefrontal, parietal, and cingulate signals to exploratory decisions",
      "content": "",
      "author": "",
      "published_date": "2026-01-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-22T01:53:53.116893+00:00",
      "updated_at": "2026-01-22T01:53:53.116895+00:00"
    },
    {
      "id": "7b40d396666352439a35d96d57204ff5",
      "url": "https://www.nature.com/articles/s41467-025-68059-8",
      "title": "Uncovering the molecular logic of cortical wiring between neuronal subtypes across development through ligand\u2013receptor inference",
      "content": "",
      "author": "",
      "published_date": "2026-01-22T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-22T01:53:53.116866+00:00",
      "updated_at": "2026-01-22T01:53:53.116871+00:00"
    },
    {
      "id": "2f5eee9a6409eb1523d86e96ed1866e6",
      "url": "https://www.reddit.com/r/Python/comments/1qj8n2g/built_a_file_search_engine_that_understands_your/",
      "title": "Built a file search engine that understands your documents (with OCR and Semantic Search)",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>Hey Pythonistas!</p> <h1>What My Project Does</h1> <p>I\u2019ve been working on <strong>File Brain</strong>, an open-source desktop tool that lets you search your local files using natural language. It runs 100% locally on your machine.</p> <p><strong>The Problem:</strong> We have thousands of files (PDFs, Office docs, images, archives, etc) and we constantly forget their filenames (or not named them correctly in the first place). Regular search tools won't save you when you don't use the exact keywords, and they definitely won't understand the <em>content</em> of a scanned invoice or a screenshot.</p> <p><strong>The Solution:</strong> I built a tool that indexes your files and allows you to perform queries like <em>&quot;Airplane ticket&quot;</em> or <em>&quot;Marketing 2026 Q1 report&quot;</em>, and retrieves relevant files even when their filenames are different or they don't have these words in their content.</p> <h1>Target Audience</h1> <p><strong>File Brain</strong> is useful for any individual or company that needs to locate specific files containing important information quickly and securely. This is especially useful when files don't have descriptive names (most often, it is the case) or are not placed in a well-organized directory structure.</p> <h1>Comparison</h1> <p>Here is a comparison between <strong>File Brain</strong> and other popular desktop search apps:</p> <table><thead> <tr> <th align=\"left\">App Name</th> <th align=\"left\">Price</th> <th align=\"left\">OS</th> <th align=\"left\">Indexing</th> <th align=\"left\">Search Speed</th> <th align=\"left\">File Content Search</th> <th align=\"left\">Fuzzy Search</th> <th align=\"left\">Semantic Search</th> <th align=\"left\">OCR</th> </tr> </thead><tbody> <tr> <td align=\"left\">Everything</td> <td align=\"left\">Free</td> <td align=\"left\">Windows</td> <td align=\"left\">No</td> <td align=\"left\">Instant</td> <td align=\"left\">No</td> <td align=\"left\">Wildcards/Regexp</td> <td align=\"left\">No</td> <td align=\"left\">No</td> </tr> <tr> <td align=\"left\">Listary</td> <td align=\"left\">Free</td> <td align=\"left\">Windows</td> <td align=\"left\">No</td> <td align=\"left\">Instant</td> <td align=\"left\">No</td> <td align=\"left\">Yes</td> <td align=\"left\">No</td> <td align=\"left\">No</td> </tr> <tr> <td align=\"left\">Alfred</td> <td align=\"left\">Free</td> <td align=\"left\">MacOS</td> <td align=\"left\">No</td> <td align=\"left\">Very fast</td> <td align=\"left\">No</td> <td align=\"left\">Yes</td> <td align=\"left\">No</td> <td align=\"left\">Yes</td> </tr> <tr> <td align=\"left\">Copernic</td> <td align=\"left\">25$/yr</td> <td align=\"left\">Windows</td> <td align=\"left\">Yes</td> <td align=\"left\">Fast</td> <td align=\"left\">170+ formats</td> <td align=\"left\">Partial</td> <td align=\"left\">No</td> <td align=\"left\">Yes</td> </tr> <tr> <td align=\"left\">DocFetcher</td> <td align=\"left\">Free</td> <td align=\"left\">Cross-platform</td> <td align=\"left\">Yes</td> <td align=\"left\">Fast</td> <td align=\"left\">32 formats</td> <td align=\"left\">No</td> <td align=\"left\">No</td> <td align=\"left\">No</td> </tr> <tr> <td align=\"left\">Agent Ransack</td> <td align=\"left\">Free</td> <td align=\"left\">Windows</td> <td align=\"left\">No</td> <td align=\"left\">Slow</td> <td align=\"left\">PDF and Office</td> <td align=\"left\">Wildcards/Regexp</td> <td align=\"left\">No</td> <td align=\"left\">No</td> </tr> <tr> <td align=\"left\">File Brain</td> <td align=\"left\">Free</td> <td align=\"left\">Cross-platform</td> <td align=\"left\">Yes</td> <td align=\"left\">Very fast</td> <td align=\"left\">1000+ formats</td> <td align=\"left\">Yes</td> <td align=\"left\">Yes</td> <td align=\"left\">Yes</td> </tr> </tbody></table> <p><strong>File Brain</strong> is the only file search engine that has semantic search capability, and the only free option that has OCR built in, with a very large base of supported file formats and very fast results retrieval (typically, under a second).</p> <p>Interested? Visit the repository to learn more: <a href=\"https://github.com/Hamza5/file-brain\">https://github.com/Hamza5/file-brain</a></p> <p>It\u2019s currently available for <strong>Windows</strong> and <strong>Linux</strong>. It should work on Mac too, but I haven't tested it yet.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hamza3725\"> /u/Hamza3725 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qj8n2g/built_a_file_search_engine_that_understands_your/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qj8n2g/built_a_file_search_engine_that_understands_your/\">[comments]</a></span>",
      "author": "/u/Hamza3725",
      "published_date": "2026-01-21T20:06:12+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 461,
      "reading_time": 2,
      "created_at": "2026-01-22T01:53:17.719023+00:00",
      "updated_at": "2026-01-22T01:53:17.719025+00:00"
    }
  ]
}