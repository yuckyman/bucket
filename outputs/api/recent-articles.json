{
  "last_updated": "2025-10-23T04:24:52.514598+00:00",
  "count": 20,
  "articles": [
    {
      "id": "95c7cd5d20ee3c613c3a6b1cd60208fb",
      "url": "https://arxiv.org/abs/2510.19033",
      "title": "\"Over-the-Hood\" AI Inclusivity Bugs and How 3 AI Product Teams Found and Fixed Them",
      "content": "arXiv:2510.19033v1 Announce Type: new \nAbstract: While much research has shown the presence of AI's \"under-the-hood\" biases (e.g., algorithmic, training data, etc.), what about \"over-the-hood\" inclusivity biases: barriers in user-facing AI products that disproportionately exclude users with certain problem-solving approaches? Recent research has begun to report the existence of such biases -- but what do they look like, how prevalent are they, and how can developers find and fix them? To find out, we conducted a field study with 3 AI product teams, to investigate what kinds of AI inclusivity bugs exist uniquely in user-facing AI products, and whether/how AI product teams might harness an existing (non-AI-oriented) inclusive design method to find and fix them. The teams' work resulted in identifying 6 types of AI inclusivity bugs arising 83 times, fixes covering 47 of these bug instances, and a new variation of the GenderMag inclusive design method, GenderMag-for-AI, that is especially effective at detecting certain kinds of AI inclusivity bugs.",
      "author": "Andrew Anderson, Fatima A. Moussaoui, Jimena Noa Guevara, Md Montaser Hamid, Margaret Burnett",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733673+00:00",
      "updated_at": "2025-10-23T04:24:33.733674+00:00"
    },
    {
      "id": "df2ef9da3a96611c7acaaa80f4e1c89d",
      "url": "https://arxiv.org/abs/2510.19031",
      "title": "CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients",
      "content": "arXiv:2510.19031v1 Announce Type: new \nAbstract: Simulations constitute a fundamental component of medical and nursing education and traditionally employ standardized patients (SP) and high-fidelity manikins to develop clinical reasoning and communication skills. However, these methods require substantial resources, limiting accessibility and scalability. In this study, we introduce CLiVR, a Conversational Learning system in Virtual Reality that integrates large language models (LLMs), speech processing, and 3D avatars to simulate realistic doctor-patient interactions. Developed in Unity and deployed on the Meta Quest 3 platform, CLiVR enables trainees to engage in natural dialogue with virtual patients. Each simulation is dynamically generated from a syndrome-symptom database and enhanced with sentiment analysis to provide feedback on communication tone. Through an expert user study involving medical school faculty (n=13), we assessed usability, realism, and perceived educational impact. Results demonstrated strong user acceptance, high confidence in educational potential, and valuable feedback for improvement. CLiVR offers a scalable, immersive supplement to SP-based training.",
      "author": "Akilan Amithasagaran, Sagnik Dakshit, Bhavani Suryadevara, Lindsey Stockton",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733644+00:00",
      "updated_at": "2025-10-23T04:24:33.733645+00:00"
    },
    {
      "id": "38de6e9dd74dbe9a0c20fadedcbb6dcf",
      "url": "https://arxiv.org/abs/2510.19024",
      "title": "Examining the Impact of Label Detail and Content Stakes on User Perceptions of AI-Generated Images on Social Media",
      "content": "arXiv:2510.19024v1 Announce Type: new \nAbstract: AI-generated images are increasingly prevalent on social media, raising concerns about trust and authenticity. This study investigates how different levels of label detail (basic, moderate, maximum) and content stakes (high vs. low) influence user engagement with and perceptions of AI-generated images through a within-subjects experimental study with 105 participants. Our findings reveal that increasing label detail enhances user perceptions of label transparency but does not affect user engagement. However, content stakes significantly impact user engagement and perceptions, with users demonstrating higher engagement and trust in low-stakes images. These results suggest that social media platforms can adopt detailed labels to improve transparency without compromising user engagement, offering insights for effective labeling strategies for AI-generated content.",
      "author": "Jingruo Chen, TungYen Wang, Marie Williams, Natalia Jordan, Mingyi Shao, Linda Zhang, Susan R. Fussell",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 119,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733610+00:00",
      "updated_at": "2025-10-23T04:24:33.733612+00:00"
    },
    {
      "id": "ff85bac5bc6ceaccf5814d1da62550fc",
      "url": "https://arxiv.org/abs/2510.19017",
      "title": "SocializeChat: A GPT-Based AAC Tool Grounded in Personal Memories to Support Social Communication",
      "content": "arXiv:2510.19017v1 Announce Type: new \nAbstract: Elderly people with speech impairments often face challenges in engaging in meaningful social communication, particularly when using Augmentative and Alternative Communication (AAC) tools that primarily address basic needs. Moreover, effective chats often rely on personal memories, which is hard to extract and reuse. We introduce SocializeChat, an AAC tool that generates sentence suggestions by drawing on users' personal memory records. By incorporating topic preference and interpersonal closeness, the system reuses past experience and tailors suggestions to different social contexts and conversation partners. SocializeChat not only leverages past experiences to support interaction, but also treats conversations as opportunities to create new memories, fostering a dynamic cycle between memory and communication. A user study shows its potential to enhance the inclusivity and relevance of AAC-supported social interaction.",
      "author": "Wei Xiang, Yunkai Xu, Yuyang Fang, Zhuyu Teng, Zhaoqu Jiang, Beijia Hu, Jinguo Yang",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 130,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733584+00:00",
      "updated_at": "2025-10-23T04:24:33.733586+00:00"
    },
    {
      "id": "77ae1efa4d42401078aa2f25cae40045",
      "url": "https://arxiv.org/abs/2510.19008",
      "title": "Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces",
      "content": "arXiv:2510.19008v1 Announce Type: new \nAbstract: Domestic AI agents faces ethical, autonomy, and inclusion challenges, particularly for overlooked groups like children, elderly, and Neurodivergent users. We present the Plural Voices Model (PVM), a novel single-agent framework that dynamically negotiates multi-user needs through real-time value alignment, leveraging diverse public datasets on mental health, eldercare, education, and moral reasoning. Using human+synthetic curriculum design with fairness-aware scenarios and ethical enhancements, PVM identifies core values, conflicts, and accessibility requirements to inform inclusive principles. Our privacy-focused prototype features adaptive safety scaffolds, tailored interactions (e.g., step-by-step guidance for Neurodivergent users, simple wording for children), and equitable conflict resolution. In preliminary evaluations, PVM outperforms multi-agent baselines in compliance (76% vs. 70%), fairness (90% vs. 85%), safety-violation rate (0% vs. 7%), and latency. Design innovations, including video guidance, autonomy sliders, family hubs, and adaptive safety dashboards, demonstrate new directions for ethical and inclusive domestic AI, for building user-centered agentic systems in plural domestic contexts. Our Codes and Model are been open sourced, available for reproduction: https://github.com/zade90/Agora",
      "author": "Joydeep Chandra, Satyam Kumar Navneet",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733557+00:00",
      "updated_at": "2025-10-23T04:24:33.733558+00:00"
    },
    {
      "id": "e36c31977729bf7b0ffb1426a77146f8",
      "url": "https://arxiv.org/abs/2510.18881",
      "title": "Detecting AI-Assisted Cheating in Online Exams through Behavior Analytics",
      "content": "arXiv:2510.18881v1 Announce Type: new \nAbstract: AI-assisted cheating has emerged as a significant threat in the context of online exams. Advanced browser extensions now enable large language models (LLMs) to answer questions presented in online exams within seconds, thereby compromising the security of these assessments. In this study, the behaviors of students (N = 52) on an online exam platform during a proctored, face-to-face exam were analyzed using clustering methods, with the aim of identifying groups of students exhibiting suspicious behavior potentially associated with cheating. Additionally, students in different clusters were compared in terms of their exam scores. Suspicious exam behaviors in this study were defined as selecting text within the question area, right-clicking, and losing focus on the exam page. The total frequency of these behaviors performed by each student during the exam was extracted, and k-Means clustering was employed for the analysis. The findings revealed that students were classified into six clusters based on their suspicious behaviors. It was found that students in four of the six clusters, representing approximately 33% of the total sample, exhibited suspicious behaviors at varying levels. When the exam scores of these students were compared, it was observed that those who engaged in suspicious behaviors scored, on average, 30-40 points higher than those who did not. Although further research is necessary to validate these findings, this preliminary study provides significant insights into the detection of AI-assisted cheating in online exams using behavior analytics.",
      "author": "G\\\"okhan Ak\\c{c}ap{\\i}nar",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 239,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733525+00:00",
      "updated_at": "2025-10-23T04:24:33.733527+00:00"
    },
    {
      "id": "81658c3420b4e6ec0cbe547570d0d877",
      "url": "https://arxiv.org/abs/2510.18880",
      "title": "Towards Better Health Conversations: The Benefits of Context-seeking",
      "content": "arXiv:2510.18880v1 Announce Type: new \nAbstract: Navigating health questions can be daunting in the modern information landscape. Large language models (LLMs) may provide tailored, accessible information, but also risk being inaccurate, biased or misleading. We present insights from 4 mixed-methods studies (total N=163), examining how people interact with LLMs for their own health questions. Qualitative studies revealed the importance of context-seeking in conversational AIs to elicit specific details a person may not volunteer or know to share. Context-seeking by LLMs was valued by participants, even if it meant deferring an answer for several turns. Incorporating these insights, we developed a \"Wayfinding AI\" to proactively solicit context. In a randomized, blinded study, participants rated the Wayfinding AI as more helpful, relevant, and tailored to their concerns compared to a baseline AI. These results demonstrate the strong impact of proactive context-seeking on conversational dynamics, and suggest design patterns for conversational AI to help navigate health topics.",
      "author": "Rory Sayres, Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale R. Webster, Sunny Virmani, Yun Liu, Quang Duong, Mike Schaekermann",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733490+00:00",
      "updated_at": "2025-10-23T04:24:33.733492+00:00"
    },
    {
      "id": "efdfd28b720cefb1b9ea6340f3ed14dd",
      "url": "https://arxiv.org/abs/2510.18879",
      "title": "FIRETWIN: Digital Twin Advancing Multi-Modal Sensing, Interactive Analytics for Wildfire Response",
      "content": "arXiv:2510.18879v1 Announce Type: new \nAbstract: Current wildfire management systems lack integrated virtual environments that combine historical data with immersive digital representations, hindering deep analysis and effective decision making. This paper introduces FIRETWIN, a cyber-physical Digital Twin (DT) designed to bridge complex ecological data and operationally relevant, high-fidelity visualizations for actionable incident response. FIRETWIN generates a dynamic 3D virtual globe that visualizes evolving fire behavior in real time, driven by output from physics-based fire models. The system supports multimodal perspectives, including satellite and drone viewpoints comparable to NOAA GOES-18 imagery - enabling comprehensive scenario analysis. Users interact with the environment to assess current fire conditions, anticipate progression, and evaluate available resources. Leveraging Google Maps, Unreal Engine, and pre-generated outputs from the CAWFE coupled weather-wildland fire model, we reconstruct the spread of the 2014 King Fire in California Eldorado National Forest. Procedural forest generation and particle-level fire control enable a level of realism and interactivity not possible in field training.",
      "author": "Mayamin Hamid Raha, Ali Reza Tavakkoli, Chris Webb, Mobin Habibpour, Janice Coen, Eric Rowell, Fatemeh Afghah",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733461+00:00",
      "updated_at": "2025-10-23T04:24:33.733463+00:00"
    },
    {
      "id": "da67cddba7f02e9d3658451925f70d85",
      "url": "https://arxiv.org/abs/2510.18878",
      "title": "CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant Estimation in Urban Regions Using Multi-Source Data (Software Article)",
      "content": "arXiv:2510.18878v1 Announce Type: new \nAbstract: Urban air pollution poses significant risks to public health, environmental sustainability, and policy planning. Effective air quality management requires predictive tools that can integrate diverse datasets and communicate complex spatial and temporal pollution patterns. There is a gap in interactive tools with seamless integration of forecasting and visualization of spatial distributions of air pollutant concentrations. We present CityAQVis, an interactive machine learning ML sandbox tool designed to predict and visualize pollutant concentrations at the ground level using multi-source data, which includes satellite observations, meteorological parameters, population density, elevation, and nighttime lights. While traditional air quality visualization tools often lack forecasting capabilities, CityAQVis enables users to build and compare predictive models, visualizing the model outputs and offering insights into pollution dynamics at the ground level. The pilot implementation of the tool is tested through case studies predicting nitrogen dioxide (NO2) concentrations in metropolitan regions, highlighting its adaptability to various pollutants. Through an intuitive graphical user interface (GUI), the user can perform comparative visualizations of the spatial distribution of surface-level pollutant concentration in two different urban scenarios. Our results highlight the potential of ML-driven visual analytics to improve situational awareness and support data-driven decision-making in air quality management.",
      "author": "Brij Bridhin Desai, Yukta Arvind, Aswathi Mundayatt, Jaya Sreevalsan-Nair",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 201,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733428+00:00",
      "updated_at": "2025-10-23T04:24:33.733430+00:00"
    },
    {
      "id": "c3f83ebb5e961168865d7844611e4f95",
      "url": "https://arxiv.org/abs/2510.18877",
      "title": "LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure",
      "content": "arXiv:2510.18877v1 Announce Type: new \nAbstract: For nearly two decades, conversational agents have played a critical role in structuring interactions in collaborative learning, shaping group dynamics, and supporting student engagement. The recent integration of large language models (LLMs) into these agents offers new possibilities for fostering critical thinking and collaborative problem solving. In this work, we begin with an open source collaboration support architecture called Bazaar and integrate an LLM-agent shell that enables introduction of LLM-empowered, real time, context sensitive collaborative support for group learning. This design and infrastructure paves the way for exploring how tailored LLM-empowered environments can reshape collaborative learning outcomes and interaction patterns.",
      "author": "Zhen Wu, Jiaxin Shi, R. Charles Murray, Carolyn Ros\\'e, Micah San Andres",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 105,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:33.733370+00:00",
      "updated_at": "2025-10-23T04:24:33.733374+00:00"
    },
    {
      "id": "58b30a10902f20070a11b3e8a45ee35a",
      "url": "https://arxiv.org/abs/2502.02904",
      "title": "ScholaWrite: A Dataset of End-to-End Scholarly Writing Process",
      "content": "arXiv:2502.02904v4 Announce Type: replace-cross \nAbstract: Writing is a cognitively demanding activity that requires constant decision-making, heavy reliance on working memory, and frequent shifts between tasks of different goals. To build writing assistants that truly align with writers' cognition, we must capture and decode the complete thought process behind how writers transform ideas into final texts. We present ScholaWrite, the first dataset of end-to-end scholarly writing, tracing the multi-month journey from initial drafts to final manuscripts. We contribute three key advances: (1) a Chrome extension that unobtrusively records keystrokes on Overleaf, enabling the collection of realistic, in-situ writing data; (2) a novel corpus of full scholarly manuscripts, enriched with fine-grained annotations of cognitive writing intentions. The dataset includes \\LaTeX-based edits from five computer science preprints, capturing nearly 62K text changes over four months; and (3) analyses and insights into the micro-dynamics of scholarly writing, highlighting gaps between human writing processes and the current capabilities of large language models (LLMs) in providing meaningful assistance. ScholaWrite underscores the value of capturing end-to-end writing data to develop future writing assistants that support, not replace, the cognitive work of scientists.",
      "author": "Khanh Chi Le, Linghe Wang, Minhwa Lee, Ross Volkov, Luan Tuyen Chau, Dongyeop Kang",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 185,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.654051+00:00",
      "updated_at": "2025-10-23T04:24:32.654052+00:00"
    },
    {
      "id": "72213af31dd581d898da843460c100ee",
      "url": "https://arxiv.org/abs/2509.17138",
      "title": "Analyzing Memory Effects in Large Language Models through the lens of Cognitive Psychology",
      "content": "arXiv:2509.17138v2 Announce Type: replace \nAbstract: Memory, a fundamental component of human cognition, exhibits adaptive yet fallible characteristics as illustrated by Schacter's memory \"sins\".These cognitive phenomena have been studied extensively in psychology and neuroscience, but the extent to which artificial systems, specifically Large Language Models (LLMs), emulate these cognitive phenomena remains underexplored. This study uses human memory research as a lens for understanding LLMs and systematically investigates human memory effects in state-of-the-art LLMs using paradigms drawn from psychological research. We evaluate seven key memory phenomena, comparing human behavior to LLM performance. Both people and models remember less when overloaded with information (list length effect) and remember better with repeated exposure (list strength effect). They also show similar difficulties when retrieving overlapping information, where storing too many similar facts leads to confusion (fan effect). Like humans, LLMs are susceptible to falsely \"remembering\" words that were never shown but are related to others (false memories), and they can apply prior learning to new, related situations (cross-domain generalization). However, LLMs differ in two key ways: they are less influenced by the order in which information is presented (positional bias) and more robust when processing random or meaningless material (nonsense effect). These results reveal both alignments and divergences in how LLMs and humans reconstruct memory. The findings help clarify how memory-like behavior in LLMs echoes core features of human cognition, while also highlighting the architectural differences that lead to distinct patterns of error and success.",
      "author": "Zhaoyang Cao, Lael Schooler, Reza Zafarani",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 240,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.654021+00:00",
      "updated_at": "2025-10-23T04:24:32.654022+00:00"
    },
    {
      "id": "5c0c548fc086e848392c9f8e2462af84",
      "url": "https://arxiv.org/abs/2508.18404",
      "title": "Saccade crossing avoidance as a visual search strategy",
      "content": "arXiv:2508.18404v2 Announce Type: replace \nAbstract: Although visual search appears largely random, several oculomotor biases exist such that the likelihoods of saccade directions and lengths depend on the previous scan path. Compared to the most recent fixations, the impact of the longer path history is more difficult to quantify. Using the step-selection framework commonly used in movement ecology, and analyzing data from 45-second viewings of \"Where's Waldo?\", we report a new memory-dependent effect that also varies significantly between individuals, which we term self-crossing avoidance. This is a tendency for saccades to avoid crossing those earlier in the scan path, and is most evident when both have small amplitudes. We show this by comparing real data to synthetic data generated from a memoryless approximation of the spatial statistics (i.e. a Markovian nonparametric model with a matching distribution of saccade lengths over time). Maximum likelihood fitting indicates that this effect is strongest when including the last $\\approx 7$ seconds of a scan path. The effect size is comparable to well-known forms of history dependence such as inhibition of return. A parametric probabilistic model including a self-crossing penalty term was able to reproduce joint statistics of saccade lengths and self-crossings. We also quantified individual strategic differences, and their consistency over the six images viewed per participant, using mixed-effect regressions. Participants with a higher tendency to avoid crossings displayed smaller saccade lengths and shorter fixation durations on average, but did not display more horizontal, vertical, forward or reverse saccades. Together, these results indicate that the avoidance of crossings is a local orienting strategy that facilitates and complements inhibition of return, and hence exploration of visual scenes.",
      "author": "Alex Szorkovszky, Rujeena Mathema, Pedro Lencastre, Pedro Lind, Anis Yazidi",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 271,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.653986+00:00",
      "updated_at": "2025-10-23T04:24:32.653988+00:00"
    },
    {
      "id": "fc4ac6f5a63d3515c203f50542b1000b",
      "url": "https://arxiv.org/abs/2507.09024",
      "title": "CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience",
      "content": "arXiv:2507.09024v4 Announce Type: replace \nAbstract: Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets. CNeuroMod-THINGS meets this need by capturing neural representations for a wide set of semantic concepts using well-characterized images in a new densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS exploits synergies between two existing projects: the THINGS initiative (THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has developed a common set of thoroughly annotated images broadly sampling natural and man-made objects which is used to acquire a growing collection of large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring hundreds of hours of fMRI data from a core set of participants during controlled and naturalistic tasks, including visual tasks like movie watching and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each completed 33-36 sessions of a continuous recognition paradigm using approximately 4000 images from the THINGS stimulus set spanning 720 categories. We report behavioural and neuroimaging metrics that showcase the quality of the data. By bridging together large existing resources, CNeuroMod-THINGS expands our capacity to model broad slices of the human visual experience.",
      "author": "Marie St-Laurent, Basile Pinsard, Oliver Contier, Elizabeth DuPre, Katja Seeliger, Valentina Borghesani, Julie A. Boyle, Lune Bellec, Martin N. Hebart",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 175,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.653945+00:00",
      "updated_at": "2025-10-23T04:24:32.653947+00:00"
    },
    {
      "id": "b9b5bd6dd2f9c23b25398cb3de640072",
      "url": "https://arxiv.org/abs/2505.11309",
      "title": "Decomposing stimulus-specific sensory neural information via diffusion models",
      "content": "arXiv:2505.11309v2 Announce Type: replace \nAbstract: To understand sensory coding, we must ask not only how much information neurons encode, but also what that information is about. This requires decomposing mutual information into contributions from individual stimuli and stimulus features: a fundamentally ill-posed problem with infinitely many possible solutions. We address this by introducing three core axioms, additivity, positivity, and locality that any meaningful stimulus-wise decomposition should satisfy. We then derive a decomposition that meets all three criteria and remains tractable for high-dimensional stimuli. Our decomposition can be efficiently estimated using diffusion models, allowing for scaling up to complex, structured and naturalistic stimuli. Applied to a model of visual neurons, our method quantifies how specific stimuli and features contribute to encoded information. Our approach provides a scalable, interpretable tool for probing representations in both biological and artificial neural systems.",
      "author": "Steeve Laquitaine, Simone Azeglio, Carlo Paris, Ulisse Ferrari, Matthew Chalk",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 138,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.653915+00:00",
      "updated_at": "2025-10-23T04:24:32.653917+00:00"
    },
    {
      "id": "922747c2fa8f99631108196f2f4fd5b2",
      "url": "https://arxiv.org/abs/2409.08303",
      "title": "Interpretable Features for the Assessment of Neurodegenerative Diseases through Handwriting Analysis",
      "content": "arXiv:2409.08303v4 Announce Type: replace \nAbstract: Motor dysfunction is a common sign of neurodegenerative diseases (NDs) such as Parkinson's disease (PD) and Alzheimer's disease (AD), but may be difficult to detect, especially in the early stages. In this work, we examine the behavior of a wide array of interpretable features extracted from the handwriting signals of 113 subjects performing multiple tasks on a digital tablet, as part of the Neurological Signals dataset. The aim is to measure their effectiveness in characterizing NDs, including AD and PD. To this end, task-agnostic and task-specific features are extracted from 14 distinct tasks. Subsequently, through statistical analysis and a series of classification experiments, we investigate which features provide greater discriminative power between NDs and healthy controls and amongst different NDs. Preliminary results indicate that the tasks at hand can all be effectively leveraged to distinguish between the considered set of NDs, specifically by measuring the stability, the speed of writing, the time spent not writing, and the pressure variations between groups from our handcrafted interpretable features, which shows a statistically significant difference between groups, across multiple tasks. Using various binary classification algorithms on the computed features, we obtain up to 87% accuracy for the discrimination between AD and healthy controls (CTL), and up to 69% for the discrimination between PD and CTL.",
      "author": "Thomas Thebaud, Anna Favaro, Casey Chen, Gabrielle Chavez, Laureano Moro-Velazquez, Ankur Butala, Najim Dehak",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.653888+00:00",
      "updated_at": "2025-10-23T04:24:32.653889+00:00"
    },
    {
      "id": "f4da78c1908a36a69ae527bb155b49b5",
      "url": "https://arxiv.org/abs/2408.00782",
      "title": "Dynamic transitions of blind spots in the Hermann grid illusion",
      "content": "arXiv:2408.00782v2 Announce Type: replace \nAbstract: Hermann discovered the grid illusion in 1870, but its cause has remained a mystery for more than 150 years. In 1960, Baumgartner proposed a hypothesis for the illusion based on neural receptive fields, but Geier presented a counterexample in 2008. In 1995, Schrauf devised the scintillating grid illusion, an improvement on the Hermann grid illusion. I propose that a hypothesis involving blind spots (optic discs) can significantly contribute to unraveling the mystery of the grid illusion.",
      "author": "Yutaka Nishiyama",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 81,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.653854+00:00",
      "updated_at": "2025-10-23T04:24:32.653855+00:00"
    },
    {
      "id": "e47b520b482b172812b0e35f9ed48146",
      "url": "https://arxiv.org/abs/2510.19021",
      "title": "Category learning in deep neural networks: Information content and geometry of internal representations",
      "content": "arXiv:2510.19021v1 Announce Type: cross \nAbstract: In animals, category learning enhances discrimination between stimuli close to the category boundary. This phenomenon, called categorical perception, was also empirically observed in artificial neural networks trained on classification tasks. In previous modeling works based on neuroscience data, we show that this expansion/compression is a necessary outcome of efficient learning. Here we extend our theoretical framework to artificial networks. We show that minimizing the Bayes cost (mean of the cross-entropy loss) implies maximizing the mutual information between the set of categories and the neural activities prior to the decision layer. Considering structured data with an underlying feature space of small dimension, we show that maximizing the mutual information implies (i) finding an appropriate projection space, and, (ii) building a neural representation with the appropriate metric. The latter is based on a Fisher information matrix measuring the sensitivity of the neural activity to changes in the projection space. Optimal learning makes this neural Fisher information follow a category-specific Fisher information, measuring the sensitivity of the category membership. Category learning thus induces an expansion of neural space near decision boundaries. We characterize the properties of the categorical Fisher information, showing that its eigenvectors give the most discriminant directions at each point of the projection space. We find that, unexpectedly, its maxima are in general not exactly at, but near, the class boundaries. Considering toy models and the MNIST dataset, we numerically illustrate how after learning the two Fisher information matrices match, and essentially align with the category boundaries. Finally, we relate our approach to the Information Bottleneck one, and we exhibit a bias-variance decomposition of the Bayes cost, of interest on its own.",
      "author": "Laurent Bonnasse-Gahot, Jean-Pierre Nadal",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 276,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:32.653824+00:00",
      "updated_at": "2025-10-23T04:24:32.653828+00:00"
    },
    {
      "id": "22d26c2df6fb0991175c161333d8d288",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005294?dgcid=rss_sd_all",
      "title": "Impact of high-frequency sampling rate and stimulation intensity on early TMS artifacts: considerations for immediate TMS-EEG responses",
      "content": "<p>Publication date: 1 November 2025</p><p><b>Source:</b> NeuroImage, Volume 321</p><p>Author(s): Antonietta Stango, Agnese Zazio, Guido Barchiesi, Natale Salvatore Bonfiglio, Marta Bortoletto</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 19,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:25.127625+00:00",
      "updated_at": "2025-10-23T04:24:25.127634+00:00"
    },
    {
      "id": "7e6c9aa6336afda57645c6245f8b595f",
      "url": "https://www.biorxiv.org/content/10.1101/2025.10.21.683769v1?rss=1",
      "title": "Visceral signaling of post-ingestive malaise directs memory updating in Drosophila",
      "content": "Consolidation is a time when labile memories transition to a stable form. Malaise learning in Drosophila reveals consolidation to also permit memory updating. Flies taught to associate one of two odors with toxin-tainted sugar initially express conditioned odor approach, that following consolidation switches to avoidance. Behavioral reversal emerges from dopaminergic update of parallel memories for the two trained odors. Differential serotoninergic modulation of specific aversive and rewarding dopaminergic neuron subtypes permits post-ingestive intoxication to suppress consolidation of initial odor-sugar memory and simultaneously invert reward memory plasticity into \"safety\" memory for the odor experienced without food. Fat body release of the Toll-ligand activating protease modSP, and resilience factor Turandot A, instruct malaise updates by triggering autocrine Toll signaling in the same brain dopaminergic neurons that form and consolidate initial sugar memory. This neural mechanism overcomes the credit assignment problem of delayed post-ingestive reinforcement by updating earlier memories of the trained odors.",
      "author": "Senapati, B., Treiber, C. D., Waddell, S.",
      "published_date": "2025-10-22T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2025-10-23T04:24:23.904190+00:00",
      "updated_at": "2025-10-23T04:24:23.904191+00:00"
    }
  ]
}