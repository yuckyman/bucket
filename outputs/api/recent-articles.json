{
  "last_updated": "2025-10-23T22:13:53.532471+00:00",
  "count": 20,
  "articles": [
    {
      "id": "7cd5915b404adde9692f79fbf455044d",
      "url": "http://ieeexplore.ieee.org/document/11037651",
      "title": "A Force/Torque Taxonomy for Classifying States During Physical Co-Manipulation",
      "content": "Achieving seamless human-robot collaboration requires a deeper understanding of how agents manage and communicate forces during shared tasks. Force interactions during collaborative manipulation are inherently complex, especially when considering how they evolve over time. To address this complexity, we propose a taxonomy of decomposed force and torque components, providing a structured framework for examining haptic communication and informing the development of robots capable of performing meaningful collaborative manipulation tasks with human partners. We propose a standardized terminology for force decomposition and classification, bridging the varied language in previous literature in the field, and conduct a review of physical human-human interaction and haptic communication. The proposed taxonomy allows for a more effective and nuanced discussion of important force combinations that we expect to occur during collaborative manipulation (between human-human or human-robot teams). We also include example scenarios to illustrate the value of the proposed taxonomy in describing interactions between agents.",
      "author": "",
      "published_date": "2025-06-17T13:16:38+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 149,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:30.345070+00:00",
      "updated_at": "2025-10-23T22:13:53.428634+00:00",
      "metadata": {
        "processed_at": "2025-10-23T22:13:53.428643+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "bffad7ff78c038ce61497f8206575f26",
      "url": "http://ieeexplore.ieee.org/document/11045422",
      "title": "Haptic Relocation Away From the Fingertip: Where, Why, and How",
      "content": "Tactile haptic devices are often designed to render meaningful, complex, and realistic touch-based information on users\u2019 skin. While fingertips and hands are the most preferred body locations to render haptic feedback, recent trends allow such feedback to be extended to alternative body locations (e.g., wrist, arm, torso, foot) for various scenarios due to reasons such as wearability and needs of the application. In this paper, I address the new concept of haptic relocation. It refers to scenarios in which the expected feedback is related to the fingertips but rendered on a different body location instead \u2013 e.g., contact forces registered by two robotic fingers during teleoperation rendered to the users\u2019 wrist instead of the fingers. I investigated the design choices of wearable haptic devices for haptic relocation concerning different body locations, targeted applications, and actuator selection. I discuss approaches and design choices from the literature by speculating on the possible reasons, and conclude the paper by highlighting some challenges and issues to be mindful of in the future. This paper will guide engineers and researchers in searching for alternative haptic rendering solutions \u2013 especially when fingers and hands are not available for haptic interaction.",
      "author": "",
      "published_date": "2025-06-20T13:16:43+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 194,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:30.345038+00:00",
      "updated_at": "2025-10-23T22:13:53.428647+00:00",
      "metadata": {
        "processed_at": "2025-10-23T22:13:53.428649+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "654c67a50800e0d8df1eb841fa063ae1",
      "url": "http://ieeexplore.ieee.org/document/10918829",
      "title": "Tactile\u2013Thermal Interactions: Cooperation and Competition",
      "content": "This review focuses on the interactions between the cutaneous senses, and in particular touch and temperature, as these are the most relevant for developing skin-based display technologies for use in virtual reality (VR) and for designing multimodal haptic devices. A broad spectrum of research is reviewed ranging from studies that have examined the mechanisms involved in thermal intensification and tactile masking, to more applied work that has focused on implementing thermal-tactile illusions such as thermal referral and illusory wetness in VR environments. Research on these tactile-thermal illusions has identified the differences between the senses of cold and warmth in terms of their effects on the perception of object properties and the prevalence of the perceptual experiences elicited. They have also underscored the fundamental spatial and temporal differences between the tactile and thermal senses. The wide-ranging body of research on compound sensations such as wetness and stickiness has highlighted the mechanisms involved in sensing moisture and provided a framework for measuring these sensations in a variety of contexts. Although the interactions between the two senses are complex, it is clear that the addition of thermal inputs to a tactile display enhances both user experience and enables novel sensory experiences.",
      "author": "",
      "published_date": "2025-03-10T13:16:41+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 198,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:30.344997+00:00",
      "updated_at": "2025-10-23T22:13:53.428654+00:00",
      "metadata": {
        "processed_at": "2025-10-23T22:13:53.428656+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "604cffaea5736d8c2935253598862e29",
      "url": "http://ieeexplore.ieee.org/document/11174044",
      "title": "Twenty Years of World Haptics: Retrospective and Future Directions",
      "content": "null",
      "author": "",
      "published_date": "2025-09-19T13:16:57+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:30.344955+00:00",
      "updated_at": "2025-10-23T22:13:53.428658+00:00",
      "metadata": {
        "processed_at": "2025-10-23T22:13:53.428660+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "65fcee7a3858adcf1bad71db41168384",
      "url": "http://ieeexplore.ieee.org/document/11174043",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-09-19T13:16:57+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:30.344934+00:00",
      "updated_at": "2025-10-23T22:13:53.428662+00:00",
      "metadata": {
        "processed_at": "2025-10-23T22:13:53.428663+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "df2ef9da3a96611c7acaaa80f4e1c89d",
      "url": "https://arxiv.org/abs/2510.19031",
      "title": "CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients",
      "content": "arXiv:2510.19031v1 Announce Type: new \nAbstract: Simulations constitute a fundamental component of medical and nursing education and traditionally employ standardized patients (SP) and high-fidelity manikins to develop clinical reasoning and communication skills. However, these methods require substantial resources, limiting accessibility and scalability. In this study, we introduce CLiVR, a Conversational Learning system in Virtual Reality that integrates large language models (LLMs), speech processing, and 3D avatars to simulate realistic doctor-patient interactions. Developed in Unity and deployed on the Meta Quest 3 platform, CLiVR enables trainees to engage in natural dialogue with virtual patients. Each simulation is dynamically generated from a syndrome-symptom database and enhanced with sentiment analysis to provide feedback on communication tone. Through an expert user study involving medical school faculty (n=13), we assessed usability, realism, and perceived educational impact. Results demonstrated strong user acceptance, high confidence in educational potential, and valuable feedback for improvement. CLiVR offers a scalable, immersive supplement to SP-based training.",
      "author": "Akilan Amithasagaran, Sagnik Dakshit, Bhavani Suryadevara, Lindsey Stockton",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112790+00:00",
      "updated_at": "2025-10-23T21:38:23.112792+00:00"
    },
    {
      "id": "38de6e9dd74dbe9a0c20fadedcbb6dcf",
      "url": "https://arxiv.org/abs/2510.19024",
      "title": "Examining the Impact of Label Detail and Content Stakes on User Perceptions of AI-Generated Images on Social Media",
      "content": "arXiv:2510.19024v1 Announce Type: new \nAbstract: AI-generated images are increasingly prevalent on social media, raising concerns about trust and authenticity. This study investigates how different levels of label detail (basic, moderate, maximum) and content stakes (high vs. low) influence user engagement with and perceptions of AI-generated images through a within-subjects experimental study with 105 participants. Our findings reveal that increasing label detail enhances user perceptions of label transparency but does not affect user engagement. However, content stakes significantly impact user engagement and perceptions, with users demonstrating higher engagement and trust in low-stakes images. These results suggest that social media platforms can adopt detailed labels to improve transparency without compromising user engagement, offering insights for effective labeling strategies for AI-generated content.",
      "author": "Jingruo Chen, TungYen Wang, Marie Williams, Natalia Jordan, Mingyi Shao, Linda Zhang, Susan R. Fussell",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 119,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112761+00:00",
      "updated_at": "2025-10-23T21:38:23.112763+00:00"
    },
    {
      "id": "ff85bac5bc6ceaccf5814d1da62550fc",
      "url": "https://arxiv.org/abs/2510.19017",
      "title": "SocializeChat: A GPT-Based AAC Tool Grounded in Personal Memories to Support Social Communication",
      "content": "arXiv:2510.19017v1 Announce Type: new \nAbstract: Elderly people with speech impairments often face challenges in engaging in meaningful social communication, particularly when using Augmentative and Alternative Communication (AAC) tools that primarily address basic needs. Moreover, effective chats often rely on personal memories, which is hard to extract and reuse. We introduce SocializeChat, an AAC tool that generates sentence suggestions by drawing on users' personal memory records. By incorporating topic preference and interpersonal closeness, the system reuses past experience and tailors suggestions to different social contexts and conversation partners. SocializeChat not only leverages past experiences to support interaction, but also treats conversations as opportunities to create new memories, fostering a dynamic cycle between memory and communication. A user study shows its potential to enhance the inclusivity and relevance of AAC-supported social interaction.",
      "author": "Wei Xiang, Yunkai Xu, Yuyang Fang, Zhuyu Teng, Zhaoqu Jiang, Beijia Hu, Jinguo Yang",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 130,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112735+00:00",
      "updated_at": "2025-10-23T21:38:23.112737+00:00"
    },
    {
      "id": "77ae1efa4d42401078aa2f25cae40045",
      "url": "https://arxiv.org/abs/2510.19008",
      "title": "Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces",
      "content": "arXiv:2510.19008v1 Announce Type: new \nAbstract: Domestic AI agents faces ethical, autonomy, and inclusion challenges, particularly for overlooked groups like children, elderly, and Neurodivergent users. We present the Plural Voices Model (PVM), a novel single-agent framework that dynamically negotiates multi-user needs through real-time value alignment, leveraging diverse public datasets on mental health, eldercare, education, and moral reasoning. Using human+synthetic curriculum design with fairness-aware scenarios and ethical enhancements, PVM identifies core values, conflicts, and accessibility requirements to inform inclusive principles. Our privacy-focused prototype features adaptive safety scaffolds, tailored interactions (e.g., step-by-step guidance for Neurodivergent users, simple wording for children), and equitable conflict resolution. In preliminary evaluations, PVM outperforms multi-agent baselines in compliance (76% vs. 70%), fairness (90% vs. 85%), safety-violation rate (0% vs. 7%), and latency. Design innovations, including video guidance, autonomy sliders, family hubs, and adaptive safety dashboards, demonstrate new directions for ethical and inclusive domestic AI, for building user-centered agentic systems in plural domestic contexts. Our Codes and Model are been open sourced, available for reproduction: https://github.com/zade90/Agora",
      "author": "Joydeep Chandra, Satyam Kumar Navneet",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112708+00:00",
      "updated_at": "2025-10-23T21:38:23.112710+00:00"
    },
    {
      "id": "e36c31977729bf7b0ffb1426a77146f8",
      "url": "https://arxiv.org/abs/2510.18881",
      "title": "Detecting AI-Assisted Cheating in Online Exams through Behavior Analytics",
      "content": "arXiv:2510.18881v1 Announce Type: new \nAbstract: AI-assisted cheating has emerged as a significant threat in the context of online exams. Advanced browser extensions now enable large language models (LLMs) to answer questions presented in online exams within seconds, thereby compromising the security of these assessments. In this study, the behaviors of students (N = 52) on an online exam platform during a proctored, face-to-face exam were analyzed using clustering methods, with the aim of identifying groups of students exhibiting suspicious behavior potentially associated with cheating. Additionally, students in different clusters were compared in terms of their exam scores. Suspicious exam behaviors in this study were defined as selecting text within the question area, right-clicking, and losing focus on the exam page. The total frequency of these behaviors performed by each student during the exam was extracted, and k-Means clustering was employed for the analysis. The findings revealed that students were classified into six clusters based on their suspicious behaviors. It was found that students in four of the six clusters, representing approximately 33% of the total sample, exhibited suspicious behaviors at varying levels. When the exam scores of these students were compared, it was observed that those who engaged in suspicious behaviors scored, on average, 30-40 points higher than those who did not. Although further research is necessary to validate these findings, this preliminary study provides significant insights into the detection of AI-assisted cheating in online exams using behavior analytics.",
      "author": "G\\\"okhan Ak\\c{c}ap{\\i}nar",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 239,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112676+00:00",
      "updated_at": "2025-10-23T21:38:23.112678+00:00"
    },
    {
      "id": "81658c3420b4e6ec0cbe547570d0d877",
      "url": "https://arxiv.org/abs/2510.18880",
      "title": "Towards Better Health Conversations: The Benefits of Context-seeking",
      "content": "arXiv:2510.18880v1 Announce Type: new \nAbstract: Navigating health questions can be daunting in the modern information landscape. Large language models (LLMs) may provide tailored, accessible information, but also risk being inaccurate, biased or misleading. We present insights from 4 mixed-methods studies (total N=163), examining how people interact with LLMs for their own health questions. Qualitative studies revealed the importance of context-seeking in conversational AIs to elicit specific details a person may not volunteer or know to share. Context-seeking by LLMs was valued by participants, even if it meant deferring an answer for several turns. Incorporating these insights, we developed a \"Wayfinding AI\" to proactively solicit context. In a randomized, blinded study, participants rated the Wayfinding AI as more helpful, relevant, and tailored to their concerns compared to a baseline AI. These results demonstrate the strong impact of proactive context-seeking on conversational dynamics, and suggest design patterns for conversational AI to help navigate health topics.",
      "author": "Rory Sayres, Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale R. Webster, Sunny Virmani, Yun Liu, Quang Duong, Mike Schaekermann",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112639+00:00",
      "updated_at": "2025-10-23T21:38:23.112641+00:00"
    },
    {
      "id": "efdfd28b720cefb1b9ea6340f3ed14dd",
      "url": "https://arxiv.org/abs/2510.18879",
      "title": "FIRETWIN: Digital Twin Advancing Multi-Modal Sensing, Interactive Analytics for Wildfire Response",
      "content": "arXiv:2510.18879v1 Announce Type: new \nAbstract: Current wildfire management systems lack integrated virtual environments that combine historical data with immersive digital representations, hindering deep analysis and effective decision making. This paper introduces FIRETWIN, a cyber-physical Digital Twin (DT) designed to bridge complex ecological data and operationally relevant, high-fidelity visualizations for actionable incident response. FIRETWIN generates a dynamic 3D virtual globe that visualizes evolving fire behavior in real time, driven by output from physics-based fire models. The system supports multimodal perspectives, including satellite and drone viewpoints comparable to NOAA GOES-18 imagery - enabling comprehensive scenario analysis. Users interact with the environment to assess current fire conditions, anticipate progression, and evaluate available resources. Leveraging Google Maps, Unreal Engine, and pre-generated outputs from the CAWFE coupled weather-wildland fire model, we reconstruct the spread of the 2014 King Fire in California Eldorado National Forest. Procedural forest generation and particle-level fire control enable a level of realism and interactivity not possible in field training.",
      "author": "Mayamin Hamid Raha, Ali Reza Tavakkoli, Chris Webb, Mobin Habibpour, Janice Coen, Eric Rowell, Fatemeh Afghah",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112609+00:00",
      "updated_at": "2025-10-23T21:38:23.112611+00:00"
    },
    {
      "id": "da67cddba7f02e9d3658451925f70d85",
      "url": "https://arxiv.org/abs/2510.18878",
      "title": "CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant Estimation in Urban Regions Using Multi-Source Data (Software Article)",
      "content": "arXiv:2510.18878v1 Announce Type: new \nAbstract: Urban air pollution poses significant risks to public health, environmental sustainability, and policy planning. Effective air quality management requires predictive tools that can integrate diverse datasets and communicate complex spatial and temporal pollution patterns. There is a gap in interactive tools with seamless integration of forecasting and visualization of spatial distributions of air pollutant concentrations. We present CityAQVis, an interactive machine learning ML sandbox tool designed to predict and visualize pollutant concentrations at the ground level using multi-source data, which includes satellite observations, meteorological parameters, population density, elevation, and nighttime lights. While traditional air quality visualization tools often lack forecasting capabilities, CityAQVis enables users to build and compare predictive models, visualizing the model outputs and offering insights into pollution dynamics at the ground level. The pilot implementation of the tool is tested through case studies predicting nitrogen dioxide (NO2) concentrations in metropolitan regions, highlighting its adaptability to various pollutants. Through an intuitive graphical user interface (GUI), the user can perform comparative visualizations of the spatial distribution of surface-level pollutant concentration in two different urban scenarios. Our results highlight the potential of ML-driven visual analytics to improve situational awareness and support data-driven decision-making in air quality management.",
      "author": "Brij Bridhin Desai, Yukta Arvind, Aswathi Mundayatt, Jaya Sreevalsan-Nair",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 201,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112575+00:00",
      "updated_at": "2025-10-23T21:38:23.112577+00:00"
    },
    {
      "id": "c3f83ebb5e961168865d7844611e4f95",
      "url": "https://arxiv.org/abs/2510.18877",
      "title": "LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure",
      "content": "arXiv:2510.18877v1 Announce Type: new \nAbstract: For nearly two decades, conversational agents have played a critical role in structuring interactions in collaborative learning, shaping group dynamics, and supporting student engagement. The recent integration of large language models (LLMs) into these agents offers new possibilities for fostering critical thinking and collaborative problem solving. In this work, we begin with an open source collaboration support architecture called Bazaar and integrate an LLM-agent shell that enables introduction of LLM-empowered, real time, context sensitive collaborative support for group learning. This design and infrastructure paves the way for exploring how tailored LLM-empowered environments can reshape collaborative learning outcomes and interaction patterns.",
      "author": "Zhen Wu, Jiaxin Shi, R. Charles Murray, Carolyn Ros\\'e, Micah San Andres",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 105,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:23.112532+00:00",
      "updated_at": "2025-10-23T21:38:23.112537+00:00"
    },
    {
      "id": "58b30a10902f20070a11b3e8a45ee35a",
      "url": "https://arxiv.org/abs/2502.02904",
      "title": "ScholaWrite: A Dataset of End-to-End Scholarly Writing Process",
      "content": "arXiv:2502.02904v4 Announce Type: replace-cross \nAbstract: Writing is a cognitively demanding activity that requires constant decision-making, heavy reliance on working memory, and frequent shifts between tasks of different goals. To build writing assistants that truly align with writers' cognition, we must capture and decode the complete thought process behind how writers transform ideas into final texts. We present ScholaWrite, the first dataset of end-to-end scholarly writing, tracing the multi-month journey from initial drafts to final manuscripts. We contribute three key advances: (1) a Chrome extension that unobtrusively records keystrokes on Overleaf, enabling the collection of realistic, in-situ writing data; (2) a novel corpus of full scholarly manuscripts, enriched with fine-grained annotations of cognitive writing intentions. The dataset includes \\LaTeX-based edits from five computer science preprints, capturing nearly 62K text changes over four months; and (3) analyses and insights into the micro-dynamics of scholarly writing, highlighting gaps between human writing processes and the current capabilities of large language models (LLMs) in providing meaningful assistance. ScholaWrite underscores the value of capturing end-to-end writing data to develop future writing assistants that support, not replace, the cognitive work of scientists.",
      "author": "Khanh Chi Le, Linghe Wang, Minhwa Lee, Ross Volkov, Luan Tuyen Chau, Dongyeop Kang",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 185,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:21.935874+00:00",
      "updated_at": "2025-10-23T21:38:21.935875+00:00"
    },
    {
      "id": "72213af31dd581d898da843460c100ee",
      "url": "https://arxiv.org/abs/2509.17138",
      "title": "Analyzing Memory Effects in Large Language Models through the lens of Cognitive Psychology",
      "content": "arXiv:2509.17138v2 Announce Type: replace \nAbstract: Memory, a fundamental component of human cognition, exhibits adaptive yet fallible characteristics as illustrated by Schacter's memory \"sins\".These cognitive phenomena have been studied extensively in psychology and neuroscience, but the extent to which artificial systems, specifically Large Language Models (LLMs), emulate these cognitive phenomena remains underexplored. This study uses human memory research as a lens for understanding LLMs and systematically investigates human memory effects in state-of-the-art LLMs using paradigms drawn from psychological research. We evaluate seven key memory phenomena, comparing human behavior to LLM performance. Both people and models remember less when overloaded with information (list length effect) and remember better with repeated exposure (list strength effect). They also show similar difficulties when retrieving overlapping information, where storing too many similar facts leads to confusion (fan effect). Like humans, LLMs are susceptible to falsely \"remembering\" words that were never shown but are related to others (false memories), and they can apply prior learning to new, related situations (cross-domain generalization). However, LLMs differ in two key ways: they are less influenced by the order in which information is presented (positional bias) and more robust when processing random or meaningless material (nonsense effect). These results reveal both alignments and divergences in how LLMs and humans reconstruct memory. The findings help clarify how memory-like behavior in LLMs echoes core features of human cognition, while also highlighting the architectural differences that lead to distinct patterns of error and success.",
      "author": "Zhaoyang Cao, Lael Schooler, Reza Zafarani",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 240,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:21.935837+00:00",
      "updated_at": "2025-10-23T21:38:21.935839+00:00"
    },
    {
      "id": "5c0c548fc086e848392c9f8e2462af84",
      "url": "https://arxiv.org/abs/2508.18404",
      "title": "Saccade crossing avoidance as a visual search strategy",
      "content": "arXiv:2508.18404v2 Announce Type: replace \nAbstract: Although visual search appears largely random, several oculomotor biases exist such that the likelihoods of saccade directions and lengths depend on the previous scan path. Compared to the most recent fixations, the impact of the longer path history is more difficult to quantify. Using the step-selection framework commonly used in movement ecology, and analyzing data from 45-second viewings of \"Where's Waldo?\", we report a new memory-dependent effect that also varies significantly between individuals, which we term self-crossing avoidance. This is a tendency for saccades to avoid crossing those earlier in the scan path, and is most evident when both have small amplitudes. We show this by comparing real data to synthetic data generated from a memoryless approximation of the spatial statistics (i.e. a Markovian nonparametric model with a matching distribution of saccade lengths over time). Maximum likelihood fitting indicates that this effect is strongest when including the last $\\approx 7$ seconds of a scan path. The effect size is comparable to well-known forms of history dependence such as inhibition of return. A parametric probabilistic model including a self-crossing penalty term was able to reproduce joint statistics of saccade lengths and self-crossings. We also quantified individual strategic differences, and their consistency over the six images viewed per participant, using mixed-effect regressions. Participants with a higher tendency to avoid crossings displayed smaller saccade lengths and shorter fixation durations on average, but did not display more horizontal, vertical, forward or reverse saccades. Together, these results indicate that the avoidance of crossings is a local orienting strategy that facilitates and complements inhibition of return, and hence exploration of visual scenes.",
      "author": "Alex Szorkovszky, Rujeena Mathema, Pedro Lencastre, Pedro Lind, Anis Yazidi",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 271,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:21.935802+00:00",
      "updated_at": "2025-10-23T21:38:21.935803+00:00"
    },
    {
      "id": "fc4ac6f5a63d3515c203f50542b1000b",
      "url": "https://arxiv.org/abs/2507.09024",
      "title": "CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience",
      "content": "arXiv:2507.09024v4 Announce Type: replace \nAbstract: Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets. CNeuroMod-THINGS meets this need by capturing neural representations for a wide set of semantic concepts using well-characterized images in a new densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS exploits synergies between two existing projects: the THINGS initiative (THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has developed a common set of thoroughly annotated images broadly sampling natural and man-made objects which is used to acquire a growing collection of large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring hundreds of hours of fMRI data from a core set of participants during controlled and naturalistic tasks, including visual tasks like movie watching and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each completed 33-36 sessions of a continuous recognition paradigm using approximately 4000 images from the THINGS stimulus set spanning 720 categories. We report behavioural and neuroimaging metrics that showcase the quality of the data. By bridging together large existing resources, CNeuroMod-THINGS expands our capacity to model broad slices of the human visual experience.",
      "author": "Marie St-Laurent, Basile Pinsard, Oliver Contier, Elizabeth DuPre, Katja Seeliger, Valentina Borghesani, Julie A. Boyle, Lune Bellec, Martin N. Hebart",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 175,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:21.935764+00:00",
      "updated_at": "2025-10-23T21:38:21.935766+00:00"
    },
    {
      "id": "b9b5bd6dd2f9c23b25398cb3de640072",
      "url": "https://arxiv.org/abs/2505.11309",
      "title": "Decomposing stimulus-specific sensory neural information via diffusion models",
      "content": "arXiv:2505.11309v2 Announce Type: replace \nAbstract: To understand sensory coding, we must ask not only how much information neurons encode, but also what that information is about. This requires decomposing mutual information into contributions from individual stimuli and stimulus features: a fundamentally ill-posed problem with infinitely many possible solutions. We address this by introducing three core axioms, additivity, positivity, and locality that any meaningful stimulus-wise decomposition should satisfy. We then derive a decomposition that meets all three criteria and remains tractable for high-dimensional stimuli. Our decomposition can be efficiently estimated using diffusion models, allowing for scaling up to complex, structured and naturalistic stimuli. Applied to a model of visual neurons, our method quantifies how specific stimuli and features contribute to encoded information. Our approach provides a scalable, interpretable tool for probing representations in both biological and artificial neural systems.",
      "author": "Steeve Laquitaine, Simone Azeglio, Carlo Paris, Ulisse Ferrari, Matthew Chalk",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 138,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:21.935732+00:00",
      "updated_at": "2025-10-23T21:38:21.935734+00:00"
    },
    {
      "id": "922747c2fa8f99631108196f2f4fd5b2",
      "url": "https://arxiv.org/abs/2409.08303",
      "title": "Interpretable Features for the Assessment of Neurodegenerative Diseases through Handwriting Analysis",
      "content": "arXiv:2409.08303v4 Announce Type: replace \nAbstract: Motor dysfunction is a common sign of neurodegenerative diseases (NDs) such as Parkinson's disease (PD) and Alzheimer's disease (AD), but may be difficult to detect, especially in the early stages. In this work, we examine the behavior of a wide array of interpretable features extracted from the handwriting signals of 113 subjects performing multiple tasks on a digital tablet, as part of the Neurological Signals dataset. The aim is to measure their effectiveness in characterizing NDs, including AD and PD. To this end, task-agnostic and task-specific features are extracted from 14 distinct tasks. Subsequently, through statistical analysis and a series of classification experiments, we investigate which features provide greater discriminative power between NDs and healthy controls and amongst different NDs. Preliminary results indicate that the tasks at hand can all be effectively leveraged to distinguish between the considered set of NDs, specifically by measuring the stability, the speed of writing, the time spent not writing, and the pressure variations between groups from our handcrafted interpretable features, which shows a statistically significant difference between groups, across multiple tasks. Using various binary classification algorithms on the computed features, we obtain up to 87% accuracy for the discrimination between AD and healthy controls (CTL), and up to 69% for the discrimination between PD and CTL.",
      "author": "Thomas Thebaud, Anna Favaro, Casey Chen, Gabrielle Chavez, Laureano Moro-Velazquez, Ankur Butala, Najim Dehak",
      "published_date": "2025-10-23T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-10-23T21:38:21.935704+00:00",
      "updated_at": "2025-10-23T21:38:21.935705+00:00"
    }
  ]
}