{
  "last_updated": "2025-10-14T10:22:29.150334+00:00",
  "count": 20,
  "articles": [
    {
      "id": "78a543035cc17e2ac6f73ecd95cce1ae",
      "url": "http://ieeexplore.ieee.org/document/10750441",
      "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities and Future Directions",
      "content": "Foundation model, trained on a diverse range of data and adaptable to a myriad of tasks, is advancing healthcare. It fosters the development of healthcare artificial intelligence (AI) models tailored to the intricacies of the medical field, bridging the gap between limited AI models and the varied nature of healthcare practices. The advancement of a healthcare foundation model (HFM) brings forth tremendous potential to augment intelligent healthcare services across a broad spectrum of scenarios. However, despite the imminent widespread deployment of HFMs, there is currently a lack of clear understanding regarding their operation in the healthcare field, their existing challenges, and their future trajectory. To answer these critical inquiries, we present a comprehensive and in-depth examination that delves into the landscape of HFMs. It begins with a comprehensive overview of HFMs, encompassing their methods, data, and applications, to provide a quick understanding of the current progress. Subsequently, it delves into a thorough exploration of the challenges associated with data, algorithms, and computing infrastructures in constructing and widely applying foundation models in healthcare. Furthermore, this survey identifies promising directions for future development in this field. We believe that this survey will enhance the community's understanding of the current progress of HFMs and serve as a valuable source of guidance for future advancements in this domain.",
      "author": "",
      "published_date": "2024-11-12T13:16:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437924+00:00",
      "updated_at": "2025-10-14T10:22:12.437926+00:00"
    },
    {
      "id": "f3ebee0a159c29e785b8640ab568613e",
      "url": "http://ieeexplore.ieee.org/document/10729663",
      "title": "Data- and Physics-Driven Deep Learning Based Reconstruction for Fast MRI: Fundamentals and Methodologies",
      "content": "Magnetic Resonance Imaging (MRI) is a pivotal clinical diagnostic tool, yet its extended scanning times often compromise patient comfort and image quality, especially in volumetric, temporal and quantitative scans. This review elucidates recent advances in MRI acceleration via data and physics-driven models, leveraging techniques from algorithm unrolling models, enhancement-based methods, and plug-and-play models to the emerging full spectrum of generative model-based methods. We also explore the synergistic integration of data models with physics-based insights, encompassing the advancements in multi-coil hardware accelerations like parallel imaging and simultaneous multi-slice imaging, and the optimization of sampling patterns. We then focus on domain-specific challenges and opportunities, including image redundancy exploitation, image integrity, evaluation metrics, data heterogeneity, and model generalization. This work also discusses potential solutions and future research directions, with an emphasis on the role of data harmonization and federated learning for further improving the general applicability and performance of these methods in MRI reconstruction.",
      "author": "",
      "published_date": "2024-10-22T13:18:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437889+00:00",
      "updated_at": "2025-10-14T10:22:12.437891+00:00"
    },
    {
      "id": "b71ad97ebddb2087936b4010c1aaf456",
      "url": "http://ieeexplore.ieee.org/document/10746601",
      "title": "Artificial General Intelligence for Medical Imaging Analysis",
      "content": "Large-scale Artificial General Intelligence (AGI) models, including Large Language Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented success in a variety of general domain tasks. Yet, when applied directly to specialized domains like medical imaging, which require in-depth expertise, these models face notable challenges arising from the medical field's inherent complexities and unique characteristics. In this review, we delve into the potential applications of AGI models in medical imaging and healthcare, with a primary focus on LLMs, Large Vision Models, and Large Multimodal Models. We provide a thorough overview of the key features and enabling techniques of LLMs and AGI, and further examine the roadmaps guiding the evolution and implementation of AGI models in the medical sector, summarizing their present applications, potentialities, and associated challenges. In addition, we highlight potential future research directions, offering a holistic view on upcoming ventures. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare, and beyond.",
      "author": "",
      "published_date": "2024-11-07T13:17:37+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437859+00:00",
      "updated_at": "2025-10-14T10:22:12.437861+00:00"
    },
    {
      "id": "3970a7e47edc49703b34feadbd5d1dab",
      "url": "http://ieeexplore.ieee.org/document/10720187",
      "title": "Exhaled Breath Analysis: From Laboratory Test to Wearable Sensing",
      "content": "Breath analysis and monitoring have emerged as pivotal components in both clinical research and daily health management, particularly in addressing the global health challenges posed by respiratory and metabolic disorders. The advancement of breath analysis strategies necessitates a multidisciplinary approach, seamlessly integrating expertise from medicine, biology, engineering, and materials science. Recent innovations in laboratory methodologies and wearable sensing technologies have ushered in an era of precise, real-time, and in situ breath analysis and monitoring. This comprehensive review elucidates the physical and chemical aspects of breath analysis, encompassing respiratory parameters and both volatile and non-volatile constituents. It emphasizes their physiological and clinical significance, while also exploring cutting-edge laboratory testing techniques and state-of-the-art wearable devices. Furthermore, the review delves into the application of sophisticated data processing technologies in the burgeoning field of breathomics and examines the potential of breath control in human-machine interaction paradigms. Additionally, it provides insights into the challenges of translating innovative laboratory and wearable concepts into mainstream clinical and daily practice. Continued innovation and interdisciplinary collaboration will drive progress in breath analysis, potentially revolutionizing personalized medicine through entirely non-invasive breath methodology.",
      "author": "",
      "published_date": "2024-10-16T13:15:55+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 182,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437829+00:00",
      "updated_at": "2025-10-14T10:22:12.437830+00:00"
    },
    {
      "id": "483769689d304d6940ab358e0b085a8c",
      "url": "http://ieeexplore.ieee.org/document/10771694",
      "title": "Earable Multimodal Sensing and Stimulation: A Prospective Toward Unobtrusive Closed-Loop Biofeedback",
      "content": "The human ear has emerged as a bidirectional gateway to the brain's and body's signals. Recent advances in around-the-ear and in-ear sensors have enabled the assessment of biomarkers and physiomarkers derived from brain and cardiac activity using ear-electroencephalography (ear-EEG), photoplethysmography (ear-PPG), and chemical sensing of analytes from the ear, with ear-EEG having been taken beyond-the-lab to outer space. Parallel advances in non-invasive and minimally invasive brain stimulation techniques have leveraged the ear's access to two cranial nerves to modulate brain and body activity. The vestibulocochlear nerve stimulates the auditory cortex and limbic system with sound, while the auricular branch of the vagus nerve indirectly but significantly couples to the autonomic nervous system and cardiac output. Acoustic and current mode stimuli delivered using discreet and unobtrusive earables are an active area of research, aiming to make biofeedback and bioelectronic medicine deliverable outside of the clinic, with remote and continuous monitoring of therapeutic responsivity and long-term adaptation. Leveraging recent advances in ear-EEG, transcutaneous auricular vagus nerve stimulation (taVNS), and unobtrusive acoustic stimulation, we review accumulating evidence that combines their potential into an integrated earable platform for closed-loop multimodal sensing and neuromodulation, towards personalized and holistic therapies that are near, in- and around-the-ear.",
      "author": "",
      "published_date": "2024-11-29T13:16:54+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437795+00:00",
      "updated_at": "2025-10-14T10:22:12.437796+00:00"
    },
    {
      "id": "1d8d5e8cf0c2514bbeb45a8e0b9c28f5",
      "url": "http://ieeexplore.ieee.org/document/10856220",
      "title": "Editorial: Harnessing Reviews to Advance Biomedical Engineering's New Horizons",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437750+00:00",
      "updated_at": "2025-10-14T10:22:12.437752+00:00"
    },
    {
      "id": "6071ce99ab68021ed48d4600bdeec843",
      "url": "http://ieeexplore.ieee.org/document/10856214",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437731+00:00",
      "updated_at": "2025-10-14T10:22:12.437733+00:00"
    },
    {
      "id": "f18dbf7099a24df1b7e9875d0258e8eb",
      "url": "http://ieeexplore.ieee.org/document/10856213",
      "title": "IEEE Engineering in Medicine and Biology Society",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:20+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:12.437710+00:00",
      "updated_at": "2025-10-14T10:22:12.437713+00:00"
    },
    {
      "id": "a26bf16b66c97163157b9e77eece47f8",
      "url": "https://arxiv.org/abs/2510.10173",
      "title": "Chord Colourizer: A Near Real-Time System for Visualizing Musical Key",
      "content": "arXiv:2510.10173v1 Announce Type: new \nAbstract: This paper introduces Chord Colourizer, a near real-time system that detects the musical key of an audio signal and visually represents it through a novel graphical user interface (GUI). The system assigns colours to musical notes based on Isaac Newton's original colour wheel, preserving historical links between pitch and hue, and also integrates an Arduino-controlled LED display using 3D-printed star-shaped diffusers to offer a physical ambient media representation. The method employs Constant-Q Transform (CQT) chroma features for chord estimation and visualization, followed by threshold-based filtering and tonal enhancement to isolate the root, third, and fifth. A confidence score is computed for each detection to ensure reliability, and only chords with moderate to very strong certainty are visualized. The graphical interface dynamically updates a colour-coded keyboard layout, while the LED display provides the same colour information via spatial feedback. This multi-modal system enhances user interaction with harmonic content, offering innovative possibilities for education and artistic performance. Limitations include slight latency and the inability to detect extended chords, which future development will aim to address through refined filtering, adaptive thresholds, and support for more complex harmonies such as sevenths and augmented chords. Future work will also explore integration with alternative visualization styles, and the comparison of audio analysis libraries to improve detection speed and precision. Plans also include formal user testing to evaluate perception, usability, and cross-cultural interpretations of colour-pitch mappings.",
      "author": "Paul Haimes",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116570+00:00",
      "updated_at": "2025-10-14T10:22:08.116571+00:00"
    },
    {
      "id": "a90200a0740732a66657cc732eea0d85",
      "url": "https://arxiv.org/abs/2510.10169",
      "title": "BrainForm: a Serious Game for BCI Training and Data Collection",
      "content": "arXiv:2510.10169v1 Announce Type: new \nAbstract: $\\textit{BrainForm}$ is a gamified Brain-Computer Interface (BCI) training system designed for scalable data collection using consumer hardware and a minimal setup. We investigated (1) how users develop BCI control skills across repeated sessions and (2) perceptual and performance effects of two visual stimulation textures. Game Experience Questionnaire (GEQ) scores for Flow}, Positive Affect, Competence and Challenge were strongly positive, indicating sustained engagement. A within-subject study with multiple runs, two task complexities, and post-session questionnaires revealed no significant performance differences between textures but increased ocular irritation over time. Online metrics$\\unicode{x2013}$Task Accuracy, Task Time, and Information Transfer Rate$\\unicode{x2013}$improved across sessions, confirming learning effects for symbol spelling, even under pressure conditions. Our results highlight the potential of $\\textit{BrainForm}$ as a scalable, user-friendly BCI research tool and offer guidance for sustained engagement and reduced training fatigue.",
      "author": "Michele Romani, Devis Zanoni, Elisabetta Farella, Luca Turchet",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 137,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116534+00:00",
      "updated_at": "2025-10-14T10:22:08.116535+00:00"
    },
    {
      "id": "02d894c10dea13361198625d9ad89008",
      "url": "https://arxiv.org/abs/2510.10079",
      "title": "How AI Companionship Develops: Evidence from a Longitudinal Study",
      "content": "arXiv:2510.10079v1 Announce Type: new \nAbstract: The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.",
      "author": "Angel Hsing-Chi Hwang, Fiona Li, Jacy Reese Anthis, Hayoun Noh",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116505+00:00",
      "updated_at": "2025-10-14T10:22:08.116507+00:00"
    },
    {
      "id": "8b50c49af24c77fed69cf1aab8ddd70f",
      "url": "https://arxiv.org/abs/2510.10049",
      "title": "ALLOY: Generating Reusable Agent Workflows from User Demonstration",
      "content": "arXiv:2510.10049v1 Announce Type: new \nAbstract: Large language models (LLMs) enable end-users to delegate complex tasks to autonomous agents through natural language. However, prompt-based interaction faces critical limitations: Users often struggle to specify procedural requirements for tasks, especially those that don't have a factually correct solution but instead rely on personal preferences, such as posting social media content or planning a trip. Additionally, a ''successful'' prompt for one task may not be reusable or generalizable across similar tasks. We present ALLOY, a system inspired by classical HCI theories on Programming by Demonstration (PBD), but extended to enhance adaptability in creating LLM-based web agents. ALLOY enables users to express procedural preferences through natural demonstrations rather than prompts, while making these procedures transparent and editable through visualized workflows that can be generalized across task variations. In a study with 12 participants, ALLOY's demonstration--based approach outperformed prompt-based agents and manual workflows in capturing user intent and procedural preferences in complex web tasks. Insights from the study also show how demonstration--based interaction complements the traditional prompt-based approach.",
      "author": "Jiawen Li, Zheng Ning, Yuan Tian, Toby Jia-jun Li",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 172,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116476+00:00",
      "updated_at": "2025-10-14T10:22:08.116478+00:00"
    },
    {
      "id": "f15b692c58c4f6e66d11738e7c1f70c0",
      "url": "https://arxiv.org/abs/2510.10048",
      "title": "Between Knowledge and Care: Evaluating Generative AI-Based IUI in Type 2 Diabetes Management Through Patient and Physician Perspectives",
      "content": "arXiv:2510.10048v1 Announce Type: new \nAbstract: Generative AI systems are increasingly adopted by patients seeking everyday health guidance, yet their reliability and clinical appropriateness remain uncertain. Taking Type 2 Diabetes Mellitus (T2DM) as a representative chronic condition, this paper presents a two-part mixed-methods study that examines how patients and physicians in China evaluate the quality and usability of AI-generated health information. Study~1 analyzes 784 authentic patient questions to identify seven core categories of informational needs and five evaluation dimensions -- \\textit{Accuracy, Safety, Clarity, Integrity}, and \\textit{Action Orientation}. Study~2 involves seven endocrinologists who assess responses from four mainstream AI models across these dimensions. Quantitative and qualitative findings reveal consistent strengths in factual and lifestyle guidance but significant weaknesses in medication interpretation, contextual reasoning, and empathy. Patients view AI as an accessible ``pre-visit educator,'' whereas clinicians highlight its lack of clinical safety and personalization. Together, the findings inform design implications for interactive health systems, advocating for multi-model orchestration, risk-aware fallback mechanisms, and emotionally attuned communication to ensure trustworthy AI assistance in chronic disease care.",
      "author": "Yibo Meng, Ruiqi Chen, Zhiming Liu, Xiaolan Ding, Yan Guan",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116441+00:00",
      "updated_at": "2025-10-14T10:22:08.116442+00:00"
    },
    {
      "id": "3653041d719657ec99cf54c85afa8073",
      "url": "https://arxiv.org/abs/2510.10019",
      "title": "\"Can I Decorate My Teeth With Diamonds?\": Exploring Multi-Stakeholder Perspectives on Using VR to Reduce Children's Dental Anxiety",
      "content": "arXiv:2510.10019v1 Announce Type: new \nAbstract: Dental anxiety is prevalent among children, often leading to missed treatment and potential negative effects on their mental well-being. While several interventions (e.g., pharmacological and psychotherapeutic techniques) have been introduced for anxiety alleviation, the recently emerged virtual reality (VR) technology, with its immersive and playful nature, opened new opportunities for complementing and enhancing the therapeutic effects of existing interventions. In this light, we conducted a series of co-design workshops with 13 children aged 10-12 to explore how they envisioned using VR to address their fear and stress associated with dental visits, followed by interviews with parents (n = 13) and two dentists. Our findings revealed that children expected VR to provide immediate relief, social support, and a sense of control during dental treatment, parents sought educational opportunities for their children to learn about oral health, and dentists prioritized treatment efficiency and safety issues. Drawing from the findings, we discuss the considerations of multi-stakeholders for developing VR-assisted anxiety management applications for children within and beyond dental settings.",
      "author": "Yaxuan Mao, Yanheng Li, Duo Gong, Pengcheng An, Yuhan Luo",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116408+00:00",
      "updated_at": "2025-10-14T10:22:08.116410+00:00"
    },
    {
      "id": "de08e7cf4675090fcbfd329303de404c",
      "url": "https://arxiv.org/abs/2510.09944",
      "title": "Read the Room or Lead the Room: Understanding Socio-Cognitive Dynamics in Human-AI Teaming",
      "content": "arXiv:2510.09944v1 Announce Type: new \nAbstract: Research on Collaborative Problem Solving (CPS) has traditionally examined how humans rely on one another cognitively and socially to accomplish tasks together. With the rapid advancement of AI and large language models, however, a new question emerge: what happens to team dynamics when one of the \"teammates\" is not human? In this study, we investigate how the integration of an AI teammate -- a fully autonomous GPT-4 agent with social, cognitive, and affective capabilities -- shapes the socio-cognitive dynamics of CPS. We analyze discourse data collected from human-AI teaming (HAT) experiments conducted on a novel platform specifically designed for HAT research. Using two natural language processing (NLP) methods, specifically Linguistic Inquiry and Word Count (LIWC) and Group Communication Analysis (GCA), we found that AI teammates often assumed the role of dominant cognitive facilitators, guiding, planning, and driving group decision-making. However, they did so in a socially detached manner, frequently pushing agenda in a verbose and repetitive way. By contrast, humans working with AI used more language reflecting social processes, suggesting that they assumed more socially oriented roles. Our study highlights how learning analytics can provide critical insights into the socio-cognitive dynamics of human-AI collaboration.",
      "author": "Jaeyoon Choi, Mohammad Amin Samadi, Spencer JaQuay, Seehee Park, Nia Nixon",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 199,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116361+00:00",
      "updated_at": "2025-10-14T10:22:08.116363+00:00"
    },
    {
      "id": "49d6772591e38f5826d4d8d1141ef744",
      "url": "https://arxiv.org/abs/2510.09874",
      "title": "ROBOPSY PL[AI]: Using Role-Play to Investigate how LLMs Present Collective Memory",
      "content": "arXiv:2510.09874v1 Announce Type: new \nAbstract: The paper presents the first results of an artistic research project investigating how Large Language Models (LLMs) curate and present collective memory. In a public installation exhibited during two months in Vienna in 2025, visitors could interact with five different LLMs (ChatGPT with GPT 4o and GPT 4o mini, Mistral Large, DeepSeek-Chat, and a locally run Llama 3.1 model), which were instructed to act as narrators, implementing a role-playing game revolving around the murder of Austrian philosopher Moritz Schlick in 1936. Results of the investigation include protocols of LLM-user interactions during the game and qualitative conversations after the play experience to get insight into the players' reactions to the game. In a quantitative analysis 115 introductory texts for role-playing generated by the LLMs were examined by different methods of natural language processing, including semantic similarity and sentiment analysis. While the qualitative player feedback allowed to distinguish three distinct types of users, the quantitative text analysis showed significant differences between how the different LLMs presented the historical content. Our study thus adds to ongoing efforts to analyse LLM performance, but also suggests a way of how these efforts can be disseminated in a playful way to a general audience.",
      "author": "Margarete Jahrmann, Thomas Brandstetter, Stefan Glasauer",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 203,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116328+00:00",
      "updated_at": "2025-10-14T10:22:08.116330+00:00"
    },
    {
      "id": "0f629f67bc755472a703f20cf4fe1d70",
      "url": "https://arxiv.org/abs/2510.09791",
      "title": "PRAXA: A Framework for What-If Analysis",
      "content": "arXiv:2510.09791v1 Announce Type: new \nAbstract: Various analytical techniques-such as scenario modeling, sensitivity analysis, perturbation-based analysis, counterfactual analysis, and parameter space analysis-are used across domains to explore hypothetical scenarios, examine input-output relationships, and identify pathways to desired results. Although termed differently, these methods share common concepts and methods, suggesting unification under what-if analysis. Yet a unified framework to define motivations, core components, and its distinct types is lacking. To address this gap, we reviewed 141 publications from leading visual analytics and HCI venues (2014-2024). Our analysis (1) outlines the motivations for what-if analysis, (2) introduces Praxa, a structured framework that identifies its fundamental components and characterizes its distinct types, and (3) highlights challenges associated with the application and implementation. Together, our findings establish a standardized vocabulary and structural understanding, enabling more consistent use across domains and communicate with greater conceptual clarity. Finally, we identify open research problems and future directions to advance what-if analysis.",
      "author": "Sneha Gathani, Kevin Li, Raghav Thind, Sirui Zeng, Matthew Xu, Peter J. Haas, Cagatay Demiralp, Zhicheng Liu",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116291+00:00",
      "updated_at": "2025-10-14T10:22:08.116293+00:00"
    },
    {
      "id": "1663cf63830b26cce98ecc5918375ed4",
      "url": "https://arxiv.org/abs/2510.09763",
      "title": "Network Traffic as a Scalable Ethnographic Lens for Understanding University Students' AI Tool Practices",
      "content": "arXiv:2510.09763v1 Announce Type: new \nAbstract: AI-driven applications have become woven into students' academic and creative workflows, influencing how they learn, write, and produce ideas. Gaining a nuanced understanding of these usage patterns is essential, yet conventional survey and interview methods remain limited by recall bias, self-presentation effects, and the underreporting of habitual behaviors. While ethnographic methods offer richer contextual insights, they often face challenges of scale and reproducibility. To bridge this gap, we introduce a privacy-conscious approach that repurposes VPN-based network traffic analysis as a scalable ethnographic technique for examining students' real-world engagement with AI tools. By capturing anonymized metadata rather than content, this method enables fine-grained behavioral tracing while safeguarding personal information, thereby complementing self-report data. A three-week field deployment with university students reveals fragmented, short-duration interactions across multiple tools and devices, with intense bursts of activity coinciding with exam periods-patterns mirroring institutional rhythms of academic life. We conclude by discussing methodological, ethical, and empirical implications, positioning network traffic analysis as a promising avenue for large-scale digital ethnography on technology-in-practice.",
      "author": "Donghan Hu, Rameen Mahmood, Annabelle David, Danny Yuxing Huang",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:08.116253+00:00",
      "updated_at": "2025-10-14T10:22:08.116257+00:00"
    },
    {
      "id": "670ef74e2c1c3de09434a748acf2d884",
      "url": "https://arxiv.org/abs/2510.11503",
      "title": "People use fast, flat goal-directed simulation to reason about novel problems",
      "content": "arXiv:2510.11503v1 Announce Type: new \nAbstract: Games have long been a microcosm for studying planning and reasoning in both natural and artificial intelligence, especially with a focus on expert-level or even super-human play. But real life also pushes human intelligence along a different frontier, requiring people to flexibly navigate decision-making problems that they have never thought about before. Here, we use novice gameplay to study how people make decisions and form judgments in new problem settings. We show that people are systematic and adaptively rational in how they play a game for the first time, or evaluate a game (e.g., how fair or how fun it is likely to be) before they have played it even once. We explain these capacities via a computational cognitive model that we call the \"Intuitive Gamer\". The model is based on mechanisms of fast and flat (depth-limited) goal-directed probabilistic simulation--analogous to those used in Monte Carlo tree-search models of expert game-play, but scaled down to use very few stochastic samples, simple goal heuristics for evaluating actions, and no deep search. In a series of large-scale behavioral studies with over 1000 participants and 121 two-player strategic board games (almost all novel to our participants), our model quantitatively captures human judgments and decisions varying the amount and kind of experience people have with a game--from no experience at all (\"just thinking\"), to a single round of play, to indirect experience watching another person and predicting how they should play--and does so significantly better than much more compute-intensive expert-level models. More broadly, our work offers new insights into how people rapidly evaluate, act, and make suggestions when encountering novel problems, and could inform the design of more flexible and human-like AI systems that can determine not just how to solve new tasks, but whether a task is worth thinking about at all.",
      "author": "Katherine M. Collins, Cedegao E. Zhang, Lionel Wong, Mauricio Barba da Costa, Graham Todd, Adrian Weller, Samuel J. Cheyette, Thomas L. Griffiths, Joshua B. Tenenbaum",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 304,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:07.037676+00:00",
      "updated_at": "2025-10-14T10:22:07.037678+00:00"
    },
    {
      "id": "8595e31d22af6c7653e0fd0407ccd1c1",
      "url": "https://arxiv.org/abs/2510.10791",
      "title": "A compressed code for memory discrimination",
      "content": "arXiv:2510.10791v1 Announce Type: new \nAbstract: The ability to discriminate similar visual stimuli is an important index of memory function. This ability is widely thought to be supported by expanding the dimensionality of relevant neural codes, such that neural representations for similar stimuli are maximally distinct, or ``separated.'' An alternative hypothesis is that discrimination is supported by lossy compression of visual inputs, efficiently coding sensory information by discarding seemingly irrelevant details. A benefit of compression, relative to expansion, is that it allows individuals to retain fewer essential dimensions underlying stimulus variation -- a process linked to higher-order visual processing -- without hindering discrimination. Under this hypothesis, pattern separation is facilitated when more information from similar stimuli can be discarded, rather than preserved. We test the compression versus expansion hypotheses by predicting performance on the canonical mnemonic similarity task. We train neural networks to compress perceptual and semantic factors of stimuli, measuring lossiness using the mathematical framework underlying compression. Consistent with the compression hypothesis, and not the expansion hypothesis, greater lossiness predicts the ease and performance of lure discrimination, especially in deeper convolutional network layers that predict higher-order visual brain activity. We then confirm these predictions across two image sets, four behavioral datasets, and alternative lossiness metrics. Finally, using task fMRI, we identify signatures of lossy compression -- neural dimensionality reduction and information loss -- in higher-order visual regions V4 and IT and hippocampal DG/CA3 and CA1 linked to lure discrimination. These results suggest lossy compression supports mnemonic discrimination by discarding redundant and overlapping information.",
      "author": "Dale Zhou, Sharon Mina Noh, Nora C Harhen, Nidhi V Banavar, C. Brock Kirwan, Michael A Yassa, Aaron M Bornstein",
      "published_date": "2025-10-14T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 253,
      "reading_time": 1,
      "created_at": "2025-10-14T10:22:07.037637+00:00",
      "updated_at": "2025-10-14T10:22:07.037638+00:00"
    }
  ]
}