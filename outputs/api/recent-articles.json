{
  "last_updated": "2025-10-17T05:20:16.651432+00:00",
  "count": 20,
  "articles": [
    {
      "id": "8c8de5475ea3620d478c2a6e551a6507",
      "url": "https://www.nature.com/articles/s41539-025-00363-w",
      "title": "The distinct functions of working memory and intelligence in model-based and model-free reinforcement learning",
      "content": "",
      "author": "",
      "published_date": "2025-10-16T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-10-17T05:19:45.973154+00:00",
      "updated_at": "2025-10-17T05:19:45.973155+00:00"
    },
    {
      "id": "be638130f513bbfd1811d0f917f08aa5",
      "url": "https://www.nature.com/articles/s41562-025-02319-x",
      "title": "The nature of the relation between mental well-being and ill-being",
      "content": "",
      "author": "",
      "published_date": "2025-10-16T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-10-17T05:19:45.973135+00:00",
      "updated_at": "2025-10-17T05:19:45.973137+00:00"
    },
    {
      "id": "fe885c0e4985c4164f554025e6a51b65",
      "url": "https://www.nature.com/articles/s41598-025-23878-z",
      "title": "Exploring the relationship between somatosensory-evoked potentials, resting-state theta power, and acute balance performance",
      "content": "",
      "author": "",
      "published_date": "2025-10-16T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-10-17T05:19:45.973116+00:00",
      "updated_at": "2025-10-17T05:19:45.973117+00:00"
    },
    {
      "id": "8f3884ea9a51ea9b3b8b1b539e7a6b78",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1693279",
      "title": "Using noise to distinguish between system and observer effects in multimodal neuroimaging",
      "content": "IntroductionIt has become increasingly common to record brain activity simultaneously at more than one spatiotemporal scale. Here, we address a central question raised by such cross-scale datasets: do they reflect the same underlying dynamics observed in different ways, or different dynamics observed in the same way? In other words, to what extent can variation between modalities be attributed to system-level versus observer-level effects? System-level effects reflect genuine differences in neural dynamics at the resolution sampled by each device. Observer-level effects, by contrast, reflect artefactual differences introduced by the nonlinear transformations each device imposes on the signal. We demonstrate that noise, when incorporated into generative models, can help disentangle these two sources of variation.MethodsWe apply this noise-based approach to simultaneously recorded high-frequency broadband signals from macroelectrodes and microwires in the human hippocampus.ResultsMost subjects show a complex mixture of system- and observer-level contributions to their time series. However, in one subject, the cross-scale difference is statistically attributable to an observer-level effect\u2014i.e., consistent with the same dynamics at both microwire and macroelectrode scales.DiscussionThis study shows that noise can be used in empirical datasets to determine whether cross-scale variation arises from differences in neural dynamics or differences in observer functions.",
      "author": "Milan Br\u00e1zdil",
      "published_date": "2025-10-17T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 196,
      "reading_time": 1,
      "created_at": "2025-10-17T05:19:37.927142+00:00",
      "updated_at": "2025-10-17T05:19:37.927144+00:00"
    },
    {
      "id": "d426945fbcca824bcdfee29c9c1e84e9",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1668358",
      "title": "Advancing epileptic seizure recognition through bidirectional LSTM networks",
      "content": "Seizure detection in a timely and accurate manner remains a primary challenge in clinical neurology, affecting diagnosis planning and patient management. Most of the traditional methods rely on feature extraction and traditional machine learning techniques, which are not efficient in capturing the dynamic characteristics of neural signals. It is the aim of this study to address such limitations by designing a deep learning model from bidirectional Long Short-Term Memory (BiLSTM) networks in a bid to enhance epileptic seizure identification reliability and accuracy. The dataset used, drawn from Kaggle\u2019s Epileptic Seizure Recognition challenge, consists of 11,500 samples with 179 features per sample corresponding to different electroencephalogram (EEG) readings. Data preprocessing was utilized to normalize and structure the input to the deep learning model. The proposed BiLSTM model employs sophisticated architecture to leverage temporal dependency and bidirectional data flows. It incorporates multiple dense and dropout layers alongside batch normalization to enhance the capability of the model in learning from the EEG data in an efficient manner. It supports end-to-end feature learning from the raw EEG signals without the need for intensive preprocessing and feature engineering. BiLSTM model performed better than others with 98.70% accuracy on the validation set and surpassed traditional techniques. The F1-score and other statistical metrics also validated the performance of the model as the confusion matrix achieved high values for recall and precision. The results confirm the capability of bidirectional LSTM networks to better identify seizures with significant improvements over conventional practices. Apart from facilitating seizure detection in a reliable fashion, the method improves the overall field of biomedical signal processing and can also be used in real-time observation and intervention protocols.",
      "author": "Sanaa Al-Marzouki",
      "published_date": "2025-10-17T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 273,
      "reading_time": 1,
      "created_at": "2025-10-17T05:19:37.927091+00:00",
      "updated_at": "2025-10-17T05:19:37.927096+00:00"
    },
    {
      "id": "3b471c8331cd0407e3465d55e48f29f4",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1670883",
      "title": "Nicotine and neuronal nicotinic acetylcholine receptors: unraveling the mechanisms of nicotine addiction",
      "content": "Nicotine, recognized as the principal addictive component in tobacco, is mechanistically linked to its interaction with neuronal nicotinic acetylcholine receptors (nAChRs). nAChRs are ligand-gated ion channels composed of five transmembrane subunits, with the \u03b14\u03b22 receptor subtype being the most common in the brain, playing a crucial role in the behavioral effects of nicotine. When nicotine binds to \u03b14\u03b22 nAChR, it significantly enhances the firing rate and burst firing of dopamine neurons in the brain, thereby activating the mesolimbic dopamine system. This system promotes the formation of nicotine addiction in the early stages of addiction through rewarding sensory stimulation and associative learning. The \u03b14\u03b22 nAChR subunit has been identified as the principal subtype implicated in the pathogenesis of nicotine addiction. However, other nAChRs subtypes also play important roles in the onset and maintenance of nicotine addiction. Understanding the relationship between nicotine addiction and nAChR subtypes is crucial for fully uncovering the neurobiological mechanism behind its addictive properties and lays the foundation for developing more targeted smoking cessation strategies.",
      "author": "Hong-Juan Wang",
      "published_date": "2025-10-17T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2025-10-17T05:19:33.605426+00:00",
      "updated_at": "2025-10-17T05:19:33.605431+00:00"
    },
    {
      "id": "a1061b6836efe8289f791e46cd30d3b6",
      "url": "https://www.reddit.com/r/Python/comments/1o8pn4t/made_a_video_media_player_that_plays_multitrack/",
      "title": "Made A Video Media Player that Plays Multi-Track Audio with Python",
      "content": "<!-- SC_OFF --><div class=\"md\"><h1>Crusty Media Player</h1> <p>I made a media player that was built to be able to take Multi-Track Video Files (ex: If you clip Recordings with separate Audio Tracks like System Audio and Microphone Audio) and give you the ability to play them back with both tracks synced without the use of an external editing software like Premiere Pro! And it's Open Source!</p> <h1>What This Project Does.</h1> <p>It utilizes <a href=\"https://ffmpeg.org/\">ffmpeg</a> bundled in to rip apart audio tracks from multi-tracked video media and <a href=\"https://www.pythonguis.com/tutorials/pyqt6-widgets\">PyQt6</a> to build the application and display video media.</p> <p><a href=\"https://github.com/CrustyMonk/Crusty-Media-Player\">GitHub</a> <strong>&lt;---- Repo Here</strong></p> <p><a href=\"https://github.com/CrustyMonk/Crusty-Media-Player/releases/tag/MediaPlayer\">Crusty Media Player v0.1.1</a> <strong>&lt;---- First Downloadable Release Here</strong></p> <h1>Why Did I Make This?</h1> <p>It's simple really lol. I like clipping funny and cool parts of when my friends and I play video games and such. I also like sometimes editing the videos as a hobby! To make the video editing simpler I have my recording settings set to record two tracks of audio, my system audio, and my microphone audio separate. The problem lies in that, if I ever want to just pull up a clip to show a friend or something, with any other media player I've used I am only able to select one track or the other! I have to open Premiere pro with my game running (Making my machine use a lot of resources!) and drag the clip into Premiere. This solves that problem by being able to just open the file with the low resource app and watch the clip with all the audio goods!</p> <h1>Target Audience?</h1> <p>If you really have that niche issue that I have, then Crusty Media Player might be perfect for you! I just have the .exe pinned to my task bar so I can run it whenever I get the urge to show off or even just view a clip!</p> <h1>Quick Start</h1> <ol> <li><p>Download the packaged zip folder containing the .exe and bundled packages from the <a href=\"https://github.com/CrustyMonk/Crusty-Media-Player/releases/tag/MediaPlayer\">Downloadable Release</a></p></li> <li><p>Extract zip folder contents to desired location</p></li> <li><p>Run the <strong>Crusty_Media_Player.exe</strong></p></li> <li><p>If prompted with &quot;Windows protected your PC&quot; Pop-up, just click &quot;More Info&quot; and then &quot;Run Anyway&quot;</p></li> <li><p>Open Video Files that contain up to two tracks of audio (i.e. System and Microphone Audio)</p></li> <li><p>Watch the media all in sync! (Without the use of an editing software!)</p></li> </ol> <p>I would really appreciate any constructive criticism and any suggestions on things that I could add it for ease of use in future releases as well!</p> <h1>Comparison</h1> <p>Media Players like VLC and such also play video files from your computer. When using these tools though, you are always unable to play both audio tracks for multi-tracked videos simultaneously! Crusty Media Player fixes this problem, making you able to view multi-track audio media with both tracks simultaneously without the use of any resource heavy editing software like Premiere Pro or Filmora.</p> <h1>TLDR</h1> <p><strong>Crusty Media Player</strong> is a media player that was built to be able to take Multi-Track Video Files (ex: If you clip Recordings with separate Audio Tracks like System Audio and Microphone Audio) and give you the ability to play them back with both tracks synced without the use of an external editing software like Premiere Pro!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Crusty_Monk\"> /u/Crusty_Monk </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1o8pn4t/made_a_video_media_player_that_plays_multitrack/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1o8pn4t/made_a_video_media_player_that_plays_multitrack/\">[comments]</a></span>",
      "author": "/u/Crusty_Monk",
      "published_date": "2025-10-17T02:33:48+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 553,
      "reading_time": 2,
      "created_at": "2025-10-17T05:18:41.877545+00:00",
      "updated_at": "2025-10-17T05:18:41.877546+00:00"
    },
    {
      "id": "1f7e14e15afbffffa26325d7f76e94b1",
      "url": "https://meow.camera/",
      "title": "Meow.camera",
      "content": "<p>Article URL: <a href=\"https://meow.camera/\">https://meow.camera/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=45613047\">https://news.ycombinator.com/item?id=45613047</a></p>\n<p>Points: 5</p>\n<p># Comments: 0</p>",
      "author": "southwindcg",
      "published_date": "2025-10-17T03:27:24+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-10-17T05:18:39.352161+00:00",
      "updated_at": "2025-10-17T05:18:39.352163+00:00"
    },
    {
      "id": "78a543035cc17e2ac6f73ecd95cce1ae",
      "url": "http://ieeexplore.ieee.org/document/10750441",
      "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities and Future Directions",
      "content": "Foundation model, trained on a diverse range of data and adaptable to a myriad of tasks, is advancing healthcare. It fosters the development of healthcare artificial intelligence (AI) models tailored to the intricacies of the medical field, bridging the gap between limited AI models and the varied nature of healthcare practices. The advancement of a healthcare foundation model (HFM) brings forth tremendous potential to augment intelligent healthcare services across a broad spectrum of scenarios. However, despite the imminent widespread deployment of HFMs, there is currently a lack of clear understanding regarding their operation in the healthcare field, their existing challenges, and their future trajectory. To answer these critical inquiries, we present a comprehensive and in-depth examination that delves into the landscape of HFMs. It begins with a comprehensive overview of HFMs, encompassing their methods, data, and applications, to provide a quick understanding of the current progress. Subsequently, it delves into a thorough exploration of the challenges associated with data, algorithms, and computing infrastructures in constructing and widely applying foundation models in healthcare. Furthermore, this survey identifies promising directions for future development in this field. We believe that this survey will enhance the community's understanding of the current progress of HFMs and serve as a valuable source of guidance for future advancements in this domain.",
      "author": "",
      "published_date": "2024-11-12T13:16:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819345+00:00",
      "updated_at": "2025-10-17T04:43:49.819347+00:00"
    },
    {
      "id": "f3ebee0a159c29e785b8640ab568613e",
      "url": "http://ieeexplore.ieee.org/document/10729663",
      "title": "Data- and Physics-Driven Deep Learning Based Reconstruction for Fast MRI: Fundamentals and Methodologies",
      "content": "Magnetic Resonance Imaging (MRI) is a pivotal clinical diagnostic tool, yet its extended scanning times often compromise patient comfort and image quality, especially in volumetric, temporal and quantitative scans. This review elucidates recent advances in MRI acceleration via data and physics-driven models, leveraging techniques from algorithm unrolling models, enhancement-based methods, and plug-and-play models to the emerging full spectrum of generative model-based methods. We also explore the synergistic integration of data models with physics-based insights, encompassing the advancements in multi-coil hardware accelerations like parallel imaging and simultaneous multi-slice imaging, and the optimization of sampling patterns. We then focus on domain-specific challenges and opportunities, including image redundancy exploitation, image integrity, evaluation metrics, data heterogeneity, and model generalization. This work also discusses potential solutions and future research directions, with an emphasis on the role of data harmonization and federated learning for further improving the general applicability and performance of these methods in MRI reconstruction.",
      "author": "",
      "published_date": "2024-10-22T13:18:56+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819312+00:00",
      "updated_at": "2025-10-17T04:43:49.819313+00:00"
    },
    {
      "id": "b71ad97ebddb2087936b4010c1aaf456",
      "url": "http://ieeexplore.ieee.org/document/10746601",
      "title": "Artificial General Intelligence for Medical Imaging Analysis",
      "content": "Large-scale Artificial General Intelligence (AGI) models, including Large Language Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented success in a variety of general domain tasks. Yet, when applied directly to specialized domains like medical imaging, which require in-depth expertise, these models face notable challenges arising from the medical field's inherent complexities and unique characteristics. In this review, we delve into the potential applications of AGI models in medical imaging and healthcare, with a primary focus on LLMs, Large Vision Models, and Large Multimodal Models. We provide a thorough overview of the key features and enabling techniques of LLMs and AGI, and further examine the roadmaps guiding the evolution and implementation of AGI models in the medical sector, summarizing their present applications, potentialities, and associated challenges. In addition, we highlight potential future research directions, offering a holistic view on upcoming ventures. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare, and beyond.",
      "author": "",
      "published_date": "2024-11-07T13:17:37+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819282+00:00",
      "updated_at": "2025-10-17T04:43:49.819283+00:00"
    },
    {
      "id": "483769689d304d6940ab358e0b085a8c",
      "url": "http://ieeexplore.ieee.org/document/10771694",
      "title": "Earable Multimodal Sensing and Stimulation: A Prospective Toward Unobtrusive Closed-Loop Biofeedback",
      "content": "The human ear has emerged as a bidirectional gateway to the brain's and body's signals. Recent advances in around-the-ear and in-ear sensors have enabled the assessment of biomarkers and physiomarkers derived from brain and cardiac activity using ear-electroencephalography (ear-EEG), photoplethysmography (ear-PPG), and chemical sensing of analytes from the ear, with ear-EEG having been taken beyond-the-lab to outer space. Parallel advances in non-invasive and minimally invasive brain stimulation techniques have leveraged the ear's access to two cranial nerves to modulate brain and body activity. The vestibulocochlear nerve stimulates the auditory cortex and limbic system with sound, while the auricular branch of the vagus nerve indirectly but significantly couples to the autonomic nervous system and cardiac output. Acoustic and current mode stimuli delivered using discreet and unobtrusive earables are an active area of research, aiming to make biofeedback and bioelectronic medicine deliverable outside of the clinic, with remote and continuous monitoring of therapeutic responsivity and long-term adaptation. Leveraging recent advances in ear-EEG, transcutaneous auricular vagus nerve stimulation (taVNS), and unobtrusive acoustic stimulation, we review accumulating evidence that combines their potential into an integrated earable platform for closed-loop multimodal sensing and neuromodulation, towards personalized and holistic therapies that are near, in- and around-the-ear.",
      "author": "",
      "published_date": "2024-11-29T13:16:54+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819250+00:00",
      "updated_at": "2025-10-17T04:43:49.819252+00:00"
    },
    {
      "id": "1d8d5e8cf0c2514bbeb45a8e0b9c28f5",
      "url": "http://ieeexplore.ieee.org/document/10856220",
      "title": "Editorial: Harnessing Reviews to Advance Biomedical Engineering's New Horizons",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819207+00:00",
      "updated_at": "2025-10-17T04:43:49.819209+00:00"
    },
    {
      "id": "6071ce99ab68021ed48d4600bdeec843",
      "url": "http://ieeexplore.ieee.org/document/10856214",
      "title": "Table of Contents",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819188+00:00",
      "updated_at": "2025-10-17T04:43:49.819190+00:00"
    },
    {
      "id": "f18dbf7099a24df1b7e9875d0258e8eb",
      "url": "http://ieeexplore.ieee.org/document/10856213",
      "title": "IEEE Engineering in Medicine and Biology Society",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:20+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:43:49.819168+00:00",
      "updated_at": "2025-10-17T04:43:49.819170+00:00"
    },
    {
      "id": "9b7968741403d6b479424052728c8879",
      "url": "http://ieeexplore.ieee.org/document/10856260",
      "title": "Front Cover",
      "content": "null",
      "author": "",
      "published_date": "2025-01-28T13:17:19+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:11.746080+00:00",
      "updated_at": "2025-10-17T04:24:11.746086+00:00"
    },
    {
      "id": "d2f24a3d09c41e9d05c4b4acdcd73e85",
      "url": "https://arxiv.org/abs/2510.14308",
      "title": "ReUseIt: Synthesizing Reusable AI Agent Workflows for Web Automation",
      "content": "arXiv:2510.14308v1 Announce Type: new \nAbstract: AI-powered web agents have the potential to automate repetitive tasks, such as form filling, information retrieval, and scheduling, but they struggle to reliably execute these tasks without human intervention, requiring users to provide detailed guidance during every run. We address this limitation by automatically synthesizing reusable workflows from an agent's successful and failed attempts. These workflows incorporate execution guards that help agents detect and fix errors while keeping users informed of progress and issues. Our approach enables agents to successfully complete repetitive tasks of the same type with minimal intervention, increasing the success rates from 24.2% to 70.1% across fifteen tasks. To evaluate this approach, we invited nine users and found that our agent helped them complete web tasks with a higher success rate and less guidance compared to two baseline methods, as well as allowed users to easily monitor agent behavior and understand failures.",
      "author": "Yimeng Liu, Misha Sra, Jeevana Priya Inala, Chenglong Wang",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415850+00:00",
      "updated_at": "2025-10-17T04:24:06.415852+00:00"
    },
    {
      "id": "132a662524e3f66c5f49e360eddb4c4b",
      "url": "https://arxiv.org/abs/2510.14277",
      "title": "GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds and Characters",
      "content": "arXiv:2510.14277v1 Announce Type: new \nAbstract: We introduce GenLARP, a virtual reality (VR) system that transforms personalized stories into immersive live action role-playing (LARP) experiences. GenLARP enables users to act as both creators and players, allowing them to design characters based on their descriptions and live in the story world. Generative AI and agents powered by Large Language Models (LLMs) enrich these experiences.",
      "author": "Yichen Yu, Yifan Jiang, Mandy Lui, Qiao Jin",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 62,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415821+00:00",
      "updated_at": "2025-10-17T04:24:06.415822+00:00"
    },
    {
      "id": "eaab92b0431b3ef22abe65172c04d246",
      "url": "https://arxiv.org/abs/2510.14267",
      "title": "TapNav: Adaptive Spatiotactile Screen Readers for Tactually Guided Touchscreen Interactions for Blind and Low Vision People",
      "content": "arXiv:2510.14267v1 Announce Type: new \nAbstract: Screen readers are audio-based software that Blind and Low Vision (BLV) people use to interact with computing devices, such as tablets and smartphones. Although this technology has significantly improved the accessibility of touchscreen devices, the sequential nature of audio limits the bandwidth of information users can receive and process. We introduce TapNav, an adaptive spatiotactile screen reader prototype developed to interact with touchscreen interfaces spatially. TapNav's screen reader provides adaptive auditory feedback that, in combination with a tactile overlay, conveys spatial information and location of interface elements on-screen. We evaluated TapNav with 12 BLV users who interacted with TapNav to explore a data visualization and interact with a bank transactions application. Our qualitative findings show that touch points and spatially constrained navigation helped users anticipate outcomes for faster exploration, and offload cognitive load to touch. We provide design guidelines for creating tactile overlays for adaptive spatiotactile screen readers and discuss their generalizability beyond our exploratory data analysis and everyday application navigation scenarios.",
      "author": "Ricardo Gonzalez, Fannie Liu, Blair MacIntyre, David Saffo",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415798+00:00",
      "updated_at": "2025-10-17T04:24:06.415800+00:00"
    },
    {
      "id": "1f46a7f742e44396ae60bdcb0995d517",
      "url": "https://arxiv.org/abs/2510.14247",
      "title": "VisAider: AI-Assisted Context-Aware Visualization Support for Data Presentations",
      "content": "arXiv:2510.14247v1 Announce Type: new \nAbstract: Effective real-time data presentation is essential in small-group interactive contexts, where discussions evolve dynamically and presenters must adapt visualizations to shifting audience interests. However, most existing interactive visualization systems rely on fixed mappings between user actions and visualization commands, limiting their ability to support richer operations such as changing visualization types, adjusting data transformations, or incorporating additional datasets on the fly during live presentations. This work-in-progress paper presents VisAider, an AI-assisted interactive data presentation prototype that continuously analyzes the live presentation context, including the available dataset, active visualization, ongoing conversation, and audience profile, to generate ranked suggestions for relevant visualization aids. Grounded in a formative study with experienced data analysts, we identified key challenges in adapting visual content in real time and distilled design considerations to guide system development. A prototype implementation demonstrates the feasibility of this approach in simulated scenarios, and preliminary testing highlights challenges in inferring appropriate data transformations, resolving ambiguous visualization tasks, and achieving low-latency responsiveness. Ongoing work focuses on addressing these limitations, integrating the system into presentation environments, and preparing a summative user study to evaluate usability and communicative impact.",
      "author": "Kentaro Takahira, Yuki Ueno",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2025-10-17T04:24:06.415768+00:00",
      "updated_at": "2025-10-17T04:24:06.415769+00:00"
    }
  ]
}