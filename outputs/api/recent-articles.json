{
  "last_updated": "2025-10-29T11:19:30.013142+00:00",
  "count": 20,
  "articles": [
    {
      "id": "69a981fc0bd4f82123c01313f3e8ff3d",
      "url": "http://ieeexplore.ieee.org/document/11210870",
      "title": "IEEE Transactions on Biomedical Engineering Information for Authors",
      "content": "null",
      "author": "",
      "published_date": "2025-10-21T13:16:30+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:14.354696+00:00",
      "updated_at": "2025-10-29T11:19:14.354698+00:00"
    },
    {
      "id": "c6f0d0b84e49febca3da2525cc567a0d",
      "url": "http://ieeexplore.ieee.org/document/11210864",
      "title": "IEEE Engineering in Medicine and Biology Society Publication Information",
      "content": "null",
      "author": "",
      "published_date": "2025-10-21T13:16:29+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:14.354677+00:00",
      "updated_at": "2025-10-29T11:19:14.354679+00:00"
    },
    {
      "id": "f81ce0f85283a86d6b8262d8422a9d77",
      "url": "http://ieeexplore.ieee.org/document/11210866",
      "title": "Front Cover",
      "content": "null",
      "author": "",
      "published_date": "2025-10-21T13:16:29+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:14.354652+00:00",
      "updated_at": "2025-10-29T11:19:14.354656+00:00"
    },
    {
      "id": "1a98eae853ecfd8f8f7641114ac67d2d",
      "url": "http://ieeexplore.ieee.org/document/10764720",
      "title": "Immunomechanobiology: Engineering the Activation and Function of Immune Cells With the Mechanical Signal of Fluid Shear Stress",
      "content": "Immunomechanobiology, the study of how physical forces influence the behavior and function of immune cells, is a rapidly growing area of research. It is becoming increasingly recognized that mechanical stimuli, such as fluid shear forces, are a critical determinant of immune cell regulation. In this review, we discuss the principles and significance of various mechanical forces present within the human body, with a focus on fluid shear flow and its impact on immune cell activation and function. Moreover, we discuss engineering approaches used to study immune cell mechanobiology, and their implications in health and diseases such as cancer, autoimmune disorders, and infectious disease.",
      "author": "",
      "published_date": "2024-11-22T13:16:46+00:00",
      "source": "Reviews Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 103,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:12.937397+00:00",
      "updated_at": "2025-10-29T11:19:12.937398+00:00"
    },
    {
      "id": "34b996c0e0c43288b9a47eaba2f43783",
      "url": "https://arxiv.org/abs/2510.24070",
      "title": "Building AI Literacy at Home: How Families Navigate Children's Self-Directed Learning with AI",
      "content": "arXiv:2510.24070v1 Announce Type: new \nAbstract: As generative AI becomes embedded in children's learning spaces, families face new challenges in guiding its use. Middle childhood (ages 7-13) is a critical stage where children seek autonomy even as parental influence remains strong. Using self-directed learning (SDL) as a lens, we examine how parents perceive and support children's developing AI literacy through focus groups with 13 parent-child pairs. Parents described evolving phases of engagement driven by screen time, self-motivation, and growing knowledge. While many framed AI primarily as a study tool, few considered its non-educational roles or risks, such as privacy and infrastructural embedding. Parents also noted gaps in their own AI understanding, often turning to joint exploration and engagement as a form of co-learning. Our findings reveal how families co-construct children's AI literacy, exposing tensions between practical expectations and critical literacies, and provide design implications that foster SDL while balancing autonomy and oversight.",
      "author": "Jingyi Xie, Chuhao Wu, Ge Wang, Rui Yu, He Zhang, Ronald Metoyer, Si Chen",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.716083+00:00",
      "updated_at": "2025-10-29T11:19:08.716084+00:00"
    },
    {
      "id": "f175d9396f70671d66971e35b3191918",
      "url": "https://arxiv.org/abs/2510.24057",
      "title": "VR-Assisted Guide Dog Training: A 360{\\deg} PanoHaptic System for Right-Hand Commands Analysis",
      "content": "arXiv:2510.24057v1 Announce Type: new \nAbstract: This paper presents a VR-based guide dog training system designed to assist novice trainers in understanding guide dog behavior and issuing appropriate training commands. Guide dogs play a vital role in supporting independent mobility for visually impaired individuals, yet the limited number of skilled trainers restricts their availability. Training is highly demanding, requiring accurate observation of the dog's status and precise command issuance, especially through right-hand gestures. While the trainer's left hand holds the harness to perceive haptic cues, the right hand is used to indicate directions, maintain attention, and provide comfort, with motion patterns varying by scenario and the dog's progress. Currently, novices learn mainly by observing experts or watching videos, which lacks immersion and makes it difficult to adopt the trainer's perspective for understanding behavior or synchronizing command timing.\n  To address these limitations, the proposed system introduces a VR-based assistive platform integrating panoramic visuals and haptic feedback to create an immersive training environment. The visual module provides contextual guidance, including cues for command execution and real-time comparison of the user's posture with standard actions, while the haptic module delivers tactile feedback for command gestures. Users can re-experience training sessions across diverse scenarios and dog proficiency levels, allowing independent and repeated practice. By improving the timing, accuracy, and expressiveness of right-hand commands, the system aims to accelerate skill acquisition, enhance training quality, and mitigate the shortage of qualified trainers, ultimately increasing the availability of guide dogs for visually impaired individuals.",
      "author": "Qirong Zhu, Ansheng Wang, Shinji Tanaka, Yasutoshi Makino, Hiroyuki Shinoda",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 246,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.716050+00:00",
      "updated_at": "2025-10-29T11:19:08.716052+00:00"
    },
    {
      "id": "5208b19e2c070d96ec42cf953c682da0",
      "url": "https://arxiv.org/abs/2510.24011",
      "title": "Understanding Reader Perception Shifts upon Disclosure of AI Authorship",
      "content": "arXiv:2510.24011v1 Announce Type: new \nAbstract: As AI writing support becomes ubiquitous, how disclosing its use affects reader perception remains a critical, underexplored question. We conducted a study with 261 participants to examine how revealing varying levels of AI involvement shifts author impressions across six distinct communicative acts. Our analysis of 990 responses shows that disclosure generally erodes perceptions of trustworthiness, caring, competence, and likability, with the sharpest declines in social and interpersonal writing. A thematic analysis of participants' feedback links these negative shifts to a perceived loss of human sincerity, diminished author effort, and the contextual inappropriateness of AI. Conversely, we find that higher AI literacy mitigates these negative perceptions, leading to greater tolerance or even appreciation for AI use. Our results highlight the nuanced social dynamics of AI-mediated authorship and inform design implications for creating transparent, context-sensitive writing systems that better preserve trust and authenticity.",
      "author": "Hiroki Nakano, Jo Takezawa, Fabrice Matulic, Chi-Lan Yang, Koji Yatani",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.716015+00:00",
      "updated_at": "2025-10-29T11:19:08.716016+00:00"
    },
    {
      "id": "fe77731744ed82caa4735c9ad41fae23",
      "url": "https://arxiv.org/abs/2510.24004",
      "title": "Modeling Object Attention in Mobile AR for Intrinsic Cognitive Security",
      "content": "arXiv:2510.24004v1 Announce Type: new \nAbstract: We study attention in mobile Augmented Reality (AR) using object recall as a proxy outcome. We observe that the ability to recall an object (physical or virtual) that was encountered in a mobile AR experience depends on many possible impact factors and attributes, with some objects being readily recalled while others are not, and some people recalling objects overall much better or worse than others. This opens up a potential cognitive attack in which adversaries might create conditions that make an AR user not recall certain potentially mission-critical objects. We explore whether a calibrated predictor of object recall can help shield against such cognitive attacks. We pool data from four mobile AR studies (with a total of 1,152 object recall probes) and fit a Partial Least Squares Structural Equation Model (PLS-SEM) with formative Object, Scene, and User State composites predicting recall, also benchmarking against Random Forest and multilayer perceptron classifiers. PLS-SEM attains the best F1 score in three of four studies. Additionally, path estimates identify lighting, augmentation density, AR registration stability, cognitive load, and AR familiarity as primary drivers. The model outputs per-object recall probabilities that can drive interface adjustments when predicted recall falls. Overall, PLS-SEM provides competitive accuracy with interpretable levers for design and evaluation in mobile AR.",
      "author": "Shane Dirksen, Radha Kumaran, You-Jin Kim, Yilin Wang, Tobias H\\\"ollerer",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.715986+00:00",
      "updated_at": "2025-10-29T11:19:08.715988+00:00"
    },
    {
      "id": "5f426c07174c242eb0eabd439995bbcb",
      "url": "https://arxiv.org/abs/2510.23947",
      "title": "Toward Socially-Aware LLMs: A Survey of Multimodal Approaches to Human Behavior Understanding",
      "content": "arXiv:2510.23947v1 Announce Type: new \nAbstract: LLM-powered multimodal systems are increasingly used to interpret human social behavior, yet how researchers apply the models' 'social competence' remains poorly understood. This paper presents a systematic literature review of 176 publications across different application domains (e.g., healthcare, education, and entertainment). Using a four-dimensional coding framework (application, technical, evaluative, and ethical), we find (1) frequent use of pattern recognition and information extraction from multimodal sources, but limited support for adaptive, interactive reasoning; (2) a dominant 'modality-to-text' pipeline that privileges language over rich audiovisual cues, striping away nuanced social cues; (3) evaluation practices reliant on static benchmarks, with socially grounded, human-centered assessments rare; and (4) Ethical discussions focused mainly on legal and rights-related risks (e.g., privacy), leaving societal risks (e.g., deception) overlooked--or at best acknowledged but left unaddressed. We outline a research agenda for evaluating socially competent, ethically informed, and interaction-aware multi-modal systems.",
      "author": "Zihan Liu, Parisa Rabbani, Veda Duddu, Kyle Fan, Madison Lee, Yun Huang",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 147,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.715954+00:00",
      "updated_at": "2025-10-29T11:19:08.715955+00:00"
    },
    {
      "id": "13ef90c3c003f11b1f3aba6eac08ea82",
      "url": "https://arxiv.org/abs/2510.23904",
      "title": "Towards AI as Colleagues: Multi-Agent System Improves Structured Professional Ideation",
      "content": "arXiv:2510.23904v1 Announce Type: new \nAbstract: Most AI systems today are designed to manage tasks and execute predefined steps. This makes them effective for process coordination but limited in their ability to engage in joint problem-solving with humans or contribute new ideas. We introduce MultiColleagues, a multi-agent conversational system that shows how AI agents can act as colleagues by conversing with each other, sharing new ideas, and actively involving users in collaborative ideation. In a within-subjects study with 20 participants, we compared MultiColleagues to a single-agent baseline. Results show that MultiColleagues fostered stronger perceptions of social presence, produced ideas rated significantly higher in quality and novelty, and encouraged deeper elaboration. These findings demonstrate the potential of AI agents to move beyond process partners toward colleagues that share intent, strengthen group dynamics, and collaborate with humans to advance ideas.",
      "author": "Kexin Quan, Dina Albassam, Mengke Wu, Zijian Ding, Jessie Chin",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 137,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.715925+00:00",
      "updated_at": "2025-10-29T11:19:08.715927+00:00"
    },
    {
      "id": "7cfb090e0a1697e9501226d2edc7805c",
      "url": "https://arxiv.org/abs/2510.23887",
      "title": "MORA: AI-Mediated Story-Based practice for Speech Sound Disorder from Clinic to Home",
      "content": "arXiv:2510.23887v1 Announce Type: new \nAbstract: Speech sound disorder is among the most common communication challenges in preschool children. Home-based practice is essential for effective therapy and for acquiring generalization of target sounds, yet sustaining engaging and consistent practice remains difficult. Existing story-based activities, despite their potential for sound generalization and educational benefits, are often underutilized due to limited interactivity. Moreover, many practice tools fail to sufficiently integrate speech--language pathologists into the process, resulting in weak alignment with clinical treatment plans. To address these limitations, we present MORA, an interactive story-based practice system. MORA introduces three key innovations. First, it embeds target sounds and vocabulary into dynamic, character-driven conversational narratives, requiring children to actively produce speech to progress the story, thereby creating natural opportunities for exposure, repetition, and generalization. Second, it provides visual cues, explicit instruction, and feedback, allowing children to practice effectively either independently or with caregivers. Third, it supports an AI-in-the-loop workflow, enabling SLPs to configure target materials, review logged speech with phoneme-level scoring, and adapt therapy plans asynchronously -- bridging the gap between clinic and home practice while respecting professional expertise. A formative study with six licensed SLPs informed the system's design rationale, and an expert review with seven SLPs demonstrated strong alignment with established articulation-based treatments, as well as potential to enhance children's engagement and literacy. Furthermore, discussions highlight the design considerations for professional support and configurability, adaptive and multimodal child interaction, while highlighting MORA's broader applicability across speech and language disorders.",
      "author": "Sumin Hong, Xavier Briggs, Qingxiao Zheng, Yao Du, Jinjun Xiong, Toby Jia-jun Li",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 245,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.715897+00:00",
      "updated_at": "2025-10-29T11:19:08.715899+00:00"
    },
    {
      "id": "d55991c08f4afda43133adc983f4f73b",
      "url": "https://arxiv.org/abs/2510.23875",
      "title": "Large Language Model Agent Personality and Response Appropriateness: Evaluation by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model",
      "content": "arXiv:2510.23875v1 Announce Type: new \nAbstract: While Large Language Model (LLM)-based agents can be used to create highly engaging interactive applications through prompting personality traits and contextual data, effectively assessing their personalities has proven challenging. This novel interdisciplinary approach addresses this gap by combining agent development and linguistic analysis to assess the prompted personality of LLM-based agents in a poetry explanation task. We developed a novel, flexible question bank, informed by linguistic assessment criteria and human cognitive learning levels, offering a more comprehensive evaluation than current methods. By evaluating agent responses with natural language processing models, other LLMs, and human experts, our findings illustrate the limitations of purely deep learning solutions and emphasize the critical role of interdisciplinary design in agent development.",
      "author": "Eswari Jayakumar, Niladri Sekhar Dash, Debasmita Mukherjee",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 121,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.715860+00:00",
      "updated_at": "2025-10-29T11:19:08.715862+00:00"
    },
    {
      "id": "e5e7aee1c880c31a86f4a37ae90c8b37",
      "url": "https://arxiv.org/abs/2510.23848",
      "title": "Spatial Orchestra: Locomotion Music Instruments through Spatial Exploration",
      "content": "arXiv:2510.23848v1 Announce Type: new \nAbstract: Spatial Orchestra demonstrates how easy it is to play musical instruments using basic input like natural locomotion, which is accessible to most. Unlike many musical instruments, our work allows individuals of all skill levels to effortlessly create music by walking into virtual bubbles. Our Augmented Reality experience involves interacting with ever-shifting sound bubbles that the user engages with by stepping into color-coded bubbles within the assigned area using a standalone AR headset. Each bubble corresponds to a cello note, and omits sound from the center of the bubble, and lets the user hear and express in spatial audio, effectively transforming participants into musicians. This interactive element enables users to explore the intersection of spatial awareness, musical rhythm that extends to bodily expression through playful movements and dance-like gestures within the bubble-filled environment. This unique experience illuminates the intricate relationship between spatial awareness and the art of musical performance.",
      "author": "You-Jin Kim, Myungin Lee, Marko Peljhan, JoAnn Kuchera-Morin, Tobias H\\\"ollerer",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.715832+00:00",
      "updated_at": "2025-10-29T11:19:08.715834+00:00"
    },
    {
      "id": "83b369db88083c78c05ecfb53d7b290a",
      "url": "https://arxiv.org/abs/2510.23840",
      "title": "Reality Distortion Room: A Study of User Locomotion Responses to Spatial Augmented Reality Effects",
      "content": "arXiv:2510.23840v1 Announce Type: new \nAbstract: Reality Distortion Room (RDR) is a proof-of-concept augmented reality system using projection mapping and unencumbered interaction with the Microsoft RoomAlive system to study a user's locomotive response to visual effects that seemingly transform the physical room the user is in. This study presents five effects that augment the appearance of a physical room to subtly encourage user motion. Our experiment demonstrates users' reactions to the different distortion and augmentation effects in a standard living room, with the distortion effects projected as wall grids, furniture holograms, and small particles in the air. The augmented living room can give the impression of becoming elongated, wrapped, shifted, elevated, and enlarged. The study results support the implementation of AR experiences in limited physical spaces by providing an initial understanding of how users can be subtly encouraged to move throughout a room.",
      "author": "You-Jin Kim, Andrew D. Wilson, Jennifer Jacobs, Tobias H\\\"ollerer",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 142,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:08.715795+00:00",
      "updated_at": "2025-10-29T11:19:08.715799+00:00"
    },
    {
      "id": "eec868a6e2fc777b5af395ead81d36b3",
      "url": "https://arxiv.org/abs/2510.20958",
      "title": "NeuroPilot: A Realtime Brain-Computer Interface system to enhance concentration of students in online learning",
      "content": "arXiv:2510.20958v2 Announce Type: replace-cross \nAbstract: The prevalence of online learning poses a vital challenge in real-time monitoring of students' concentration. Traditional methods such as questionnaire assessments require manual intervention, and webcam-based monitoring fails to provide accurate insights about learners' mental focus as it is deceived by mere screen fixation without cognitive engagement. Existing BCI-based approaches lack real-time validation and evaluation procedures. To address these limitations, a Brain-Computer Interface (BCI) system is developed using a non-invasive Electroencephalogram (EEG) headband, FocusCalm, to record brainwave activity under attentive and non-attentive states. 20 minutes of data were collected from each of 20 participants watching a pre-recorded educational video. The data validation employed a novel intra-video questionnaire assessment. Subsequently, collected signals were segmented (sliding window), filtered (Butterworth bandpass), and cleaned (removal of high- amplitude and EOG artifacts such as eye blinks). Time, frequency, wavelet, and statistical features were extracted, followed by recursive feature elimination (RFE) with support vector machines (SVMs) to classify attention and non-attention states. The leave-one-subject-out (LOSO) cross-validation accuracy was found to be 88.77%. The system provides feedback alerts upon detection of a non-attention state and maintains focus profile logs. A pilot study was conducted to evaluate the effectiveness of real-time feedback. Five participants underwent a 10-minute session comprising a 5-minute baseline phase devoid of feedback, succeeded by a 5-minute feedback phase, during which alerts were activated if participants exhibited inattention for approximately 8 consecutive seconds. A paired t-test (t = 5.73, p = 0.007) indicated a statistically significant improvement in concentration during the feedback phase.",
      "author": "Asif Islam, Farhan Ishtiaque, Md. Muhyminul Haque, Farhana Sarker, Ravi Vaidyanathan, Khondaker A. Mamun",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 253,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:07.647071+00:00",
      "updated_at": "2025-10-29T11:19:07.647073+00:00"
    },
    {
      "id": "047afacfb1a2c5e1e89bd72dd8f29ad0",
      "url": "https://arxiv.org/abs/2506.04536",
      "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models",
      "content": "arXiv:2506.04536v3 Announce Type: replace-cross \nAbstract: Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.",
      "author": "Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 249,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:07.647035+00:00",
      "updated_at": "2025-10-29T11:19:07.647036+00:00"
    },
    {
      "id": "95fdcd717b0a1ac22cbc9cbcde1dfe7e",
      "url": "https://arxiv.org/abs/2502.21142",
      "title": "Multimodal Dreaming: A Global Workspace Approach to World Model-Based Reinforcement Learning",
      "content": "arXiv:2502.21142v2 Announce Type: replace-cross \nAbstract: Humans leverage rich internal models of the world to reason about the future, imagine counterfactuals, and adapt flexibly to new situations. In Reinforcement Learning (RL), world models aim to capture how the environment evolves in response to the agent's actions, facilitating planning and generalization. However, typical world models directly operate on the environment variables (e.g. pixels, physical attributes), which can make their training slow and cumbersome; instead, it may be advantageous to rely on high-level latent dimensions that capture relevant multimodal variables. Global Workspace (GW) Theory offers a cognitive framework for multimodal integration and information broadcasting in the brain, and recent studies have begun to introduce efficient deep learning implementations of GW. Here, we evaluate the capabilities of an RL system combining GW with a world model. We compare our GW-Dreamer with various versions of the standard PPO and the original Dreamer algorithms. We show that performing the dreaming process (i.e., mental simulation) inside the GW latent space allows for training with fewer environment steps. As an additional emergent property, the resulting model (but not its comparison baselines) displays strong robustness to the absence of one of its observation modalities (images or simulation attributes). We conclude that the combination of GW with World Models holds great potential for improving decision-making in RL agents.",
      "author": "L\\'eopold Mayti\\'e, Roland Bertin Johannet, Rufin VanRullen",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 218,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:07.646996+00:00",
      "updated_at": "2025-10-29T11:19:07.646998+00:00"
    },
    {
      "id": "52a03d63a84a1c913071e1c67126117a",
      "url": "https://arxiv.org/abs/2510.24709",
      "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?",
      "content": "arXiv:2510.24709v1 Announce Type: cross \nAbstract: Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of \"which parts belong together\" emerges naturally in a connectionist system.",
      "author": "Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 254,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:07.646963+00:00",
      "updated_at": "2025-10-29T11:19:07.646964+00:00"
    },
    {
      "id": "88c6bddb14f2a0bb8edcc38933d40769",
      "url": "https://arxiv.org/abs/2510.24647",
      "title": "Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia",
      "content": "arXiv:2510.24647v1 Announce Type: cross \nAbstract: We ask where, and under what conditions, dyslexic reading costs arise in a large-scale naturalistic reading dataset. Using eye-tracking aligned to word-level features (word length, frequency, and predictability), we model how each feature influences dyslexic time costs. We find that all three features robustly change reading times in both typical and dyslexic readers, and that dyslexic readers show stronger sensitivities to each, especially predictability. Counterfactual manipulations of these features substantially narrow the dyslexic-control gap by about one third, with predictability showing the strongest effect, followed by length and frequency. These patterns align with dyslexia theories that posit heightened demands on linguistic working memory and phonological encoding, and they motivate further work on lexical complexity and parafoveal preview benefits to explain the remaining gap. In short, we quantify when extra dyslexic costs arise, how large they are, and offer actionable guidance for interventions and computational models for dyslexics.",
      "author": "Hugo Rydel-Johnston, Alex Kafkas",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 152,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:07.646914+00:00",
      "updated_at": "2025-10-29T11:19:07.646916+00:00"
    },
    {
      "id": "656f1b9e445ebf075bf806687dffe45b",
      "url": "https://arxiv.org/abs/2510.24029",
      "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model",
      "content": "arXiv:2510.24029v1 Announce Type: cross \nAbstract: Boundary Vector Cells (BVCs) are a class of neurons in the brains of vertebrates that encode environmental boundaries at specific distances and allocentric directions, playing a central role in forming place fields in the hippocampus. Most computational BVC models are restricted to two-dimensional (2D) environments, making them prone to spatial ambiguities in the presence of horizontal symmetries in the environment. To address this limitation, we incorporate vertical angular sensitivity into the BVC framework, thereby enabling robust boundary detection in three dimensions, and leading to significantly more accurate spatial localization in a biologically-inspired robot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby disambiguating locations that would be indistinguishable under a purely 2D representation. Experimental results show that in environments with minimal vertical variation, the proposed 3D model matches the performance of a 2D baseline; yet, as 3D complexity increases, it yields substantially more distinct place fields and markedly reduces spatial aliasing. These findings show that adding a vertical dimension to BVC-based localization can significantly enhance navigation and mapping in real-world 3D spaces while retaining performance parity in simpler, near-planar scenarios.",
      "author": "Andrew Gerstenslager, Bekarys Dukenbaev, Ali A. Minai",
      "published_date": "2025-10-29T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 188,
      "reading_time": 1,
      "created_at": "2025-10-29T11:19:07.646877+00:00",
      "updated_at": "2025-10-29T11:19:07.646881+00:00"
    }
  ]
}