{
  "last_updated": "2026-01-15T07:47:30.642293+00:00",
  "count": 20,
  "articles": [
    {
      "id": "366ce651cd84440b89acdc276fad26cc",
      "url": "https://www.nature.com/articles/s41746-026-02346-6",
      "title": "Remote digital cognitive assessment for aging and dementia using the Oxford Cognitive Testing Portal OCTAL",
      "content": "",
      "author": "",
      "published_date": "2026-01-15T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-15T07:27:11.996465+00:00",
      "updated_at": "2026-01-15T07:27:11.996469+00:00"
    },
    {
      "id": "ed6cd510886ff02fc4c2c364d6129a7a",
      "url": "https://github.com/cjpais/Handy",
      "title": "Handy \u2013 free open source speech-to-text app",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46628397\">Comments</a>",
      "author": "",
      "published_date": "2026-01-15T05:23:18+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-15T07:26:34.012435+00:00",
      "updated_at": "2026-01-15T07:26:34.012437+00:00"
    },
    {
      "id": "ed6cd510886ff02fc4c2c364d6129a7a",
      "url": "https://github.com/cjpais/Handy",
      "title": "Handy \u2013 free open source speech-to-text app",
      "content": "<p>Article URL: <a href=\"https://github.com/cjpais/Handy\">https://github.com/cjpais/Handy</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46628397\">https://news.ycombinator.com/item?id=46628397</a></p>\n<p>Points: 14</p>\n<p># Comments: 8</p>",
      "author": "tin7in",
      "published_date": "2026-01-15T05:23:18+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-15T07:26:32.692959+00:00",
      "updated_at": "2026-01-15T07:26:32.692966+00:00"
    },
    {
      "id": "cf1895af51cf4c2e97e9e112e5aea548",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.14.699350v1?rss=1",
      "title": "A glucocorticoid-responsive polygenic signature in the anterior cingulate cortex moderates the association of early-life adversity and vulnerability for depression",
      "content": "Stress exposure is a major risk factor for psychopathology, yet how stress mediators shape long-term psychiatric vulnerability in humans remains unclear. Glucocorticoids, central effectors of the stress response, regulate gene expression through tissue-specific transcriptional programs, suggesting that glucocorticoid-responsive networks may shape sensitivity to adversity. Using RNA-sequencing following chronic glucocorticoid exposure in a non-human primate model, we identified a gene co-expression network specific to the anterior cingulate cortex (ACC) that was highly preserved across human post-mortem brain datasets relevant to depression. We derived an expression-based polygenic score (ePGS) reflecting genetic variation in network activity and tested its interaction with adversity in the UK Biobank. The ACC-specific glucocorticoid-responsive ePGS moderated the association between adversity and depressive symptoms in adult females, with the strongest effects for early-life adversity. Network genes were enriched for neurodevelopmental processes and showed stronger co-expression during childhood, highlighting a developmentally sensitive, region-specific mechanism linking stress exposure to depression risk.",
      "author": "Mar Arcego, D., Pokhvisneva, I., Buschdorf, J.-P., O'Toole, N., Patel, S., Sobreira de Lima, R. M., Elgbeili, G., Lee, P., Frosi, G., Fiori, L., Nagy, C., Silveira, P. P., Turecki, G., Meaney, M. J.",
      "published_date": "2026-01-14T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2026-01-15T06:36:41.081611+00:00",
      "updated_at": "2026-01-15T06:36:41.081617+00:00"
    },
    {
      "id": "55f59f5a68319425ee190739948f2714",
      "url": "https://www.frontiersin.org/articles/10.3389/fnhum.2025.1718713",
      "title": "Levels of shared autonomy in brain-robot interfaces: enabling multi-robot multi-human collaboration for activities of daily living",
      "content": "Individuals with ALS and other severe motor impairments often rely on caregivers for daily tasks, which limits their independence and sense of control. Brain-robot interfaces (BRIs) have the potential to restore autonomy, but many existing systems are task-specific and highly automated, which reduces the users' sense of empowerment and limits opportunities to exercise autonomy. In particular, shared autonomy approaches hold promise for overcoming current BRI limitations, by balancing user control with increased robot capabilities. In this work, we introduce a collaborative BRI that integrates non-invasive EEG, EMG, and eye tracking to enable multi-user, multi-robot interaction in a shared kitchen environment with mobile manipulators. Our system modulates assistance through three levels of autonomy\u2014Assisted Teleoperation, Shared Autonomy, and Full Automation\u2014allowing users to retain meaningful control over task execution while reducing effort for routine operations. We conducted a controlled user study comparing autonomy conditions, evaluating performance, workload, ease of use, and agency. Our results show that, while Full Automation was generally preferred by users due to lower workload and higher usability, Shared Autonomy provided higher reliability and preserved user agency, especially in the presence of noisy EEG decoding. Although there was significant individual variability in EEG decoding performance, our post-hoc analysis revealed the potential benefits of customizing pipelines for each user. Finally, we note that our findings are specific to the multi-modal configuration tested and should not be interpreted as a universal claim about the superiority of any autonomy level, and, furthermore, our user study was limited by the use of healthy adults rather than target population (e.g., individuals with ALS), gender imbalance, and a relatively small sample size, which may affect generalizability. Project website: https://coopopen.github.io/.",
      "author": "Kai Arulkumaran",
      "published_date": "2026-01-15T00:00:00+00:00",
      "source": "Frontiers Human Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 273,
      "reading_time": 1,
      "created_at": "2026-01-15T06:36:30.935959+00:00",
      "updated_at": "2026-01-15T06:36:30.935961+00:00"
    },
    {
      "id": "c7c7b0af493f17e8dfdfba764c8b40ed",
      "url": "https://www.frontiersin.org/articles/10.3389/fnhum.2025.1641421",
      "title": "Predicting progression from SeLECTS with SWAS to EE-SWAS: risk factor identification and model development",
      "content": "IntroductionThis study sought to identify early risk factors and develop a predictive model for progression from self-limited epilepsy with centrotemporal spikes (SeLECTS) accompanied by spike-and-wave activation in sleep (SWAS) to epileptic encephalopathy with SWAS (EE-SWAS), aiming to facilitate early clinical intervention.MethodsFrom a pediatric cohort with spike-and-wave index >50%, we analyzed 77 SeLECTS patients (33 progressed to EE-SWAS, 36 remained stable over \u22652 years of follow-up). Baseline clinical and EEG features were comprehensively evaluated. Multivariate logistic regression identified independent predictors of cognitive regression, which were incorporated into a nomogram-based predictive model. Model performance was assessed using the C-index in both derivation and external validation cohorts.ResultsProlonged spike-and-wave clusters, high-amplitude spikes with secondary generalization, and younger age at first seizure emerged as independent predictors of EE-SWAS progression. The nomogram model demonstrated high discriminative ability, with a C-index of 0.932 in the derivation cohort and 0.934 in external validation.ConclusionThis study provides the first validated tool for early risk stratification in SWAS-associated SeLECTS, enabling clinicians to anticipate EE-SWAS progression and optimize therapeutic strategies. The model\u2019s robustness supports its potential utility in clinical decision-making to mitigate cognitive decline.",
      "author": "Li Jiang",
      "published_date": "2026-01-15T00:00:00+00:00",
      "source": "Frontiers Human Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 182,
      "reading_time": 1,
      "created_at": "2026-01-15T06:36:30.935902+00:00",
      "updated_at": "2026-01-15T06:36:30.935907+00:00"
    },
    {
      "id": "90a28e671570aebed7195f5c2eb52c4f",
      "url": "https://news.ycombinator.com/item?id=46616529",
      "title": "Ask HN: How are you doing RAG locally?",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46616529\">Comments</a>",
      "author": "",
      "published_date": "2026-01-14T14:38:29+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-15T06:36:03.578620+00:00",
      "updated_at": "2026-01-15T06:36:03.578622+00:00"
    },
    {
      "id": "a0cdec8f425cce44ad16c7dd9d465efb",
      "url": "https://ianservin.com/2026/01/13/project-skywatch-aka-wescam-at-home/",
      "title": "Project SkyWatch (a.k.a. Wescam at Home)",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46618503\">Comments</a>",
      "author": "",
      "published_date": "2026-01-14T16:54:40+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-15T06:36:03.578581+00:00",
      "updated_at": "2026-01-15T06:36:03.578583+00:00"
    },
    {
      "id": "a25fcb675852568aedcfaf75b4f7dcfa",
      "url": "https://www.nature.com/articles/s12276-025-01618-7",
      "title": "Microglial CX3CR1 deficiency regulates the selective vulnerability of cone photoreceptors via STAT3/CCL\u2013ACKR1 signaling in the mouse retina",
      "content": "",
      "author": "",
      "published_date": "2026-01-15T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-15T05:50:54.458704+00:00",
      "updated_at": "2026-01-15T06:27:02.827602+00:00",
      "metadata": {
        "processed_at": "2026-01-15T06:27:02.827614+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "8eaad47b7eb0ba26431c4fee808e214a",
      "url": "https://www.nature.com/articles/s41593-025-02162-3",
      "title": "Spatial transcriptomics of the developing mouse brain immune landscape reveals effects of maternal immune activation and microbiome depletion",
      "content": "<p>Nature Neuroscience, Published online: 06 January 2026; <a href=\"https://www.nature.com/articles/s41593-025-02162-3\">doi:10.1038/s41593-025-02162-3</a></p>Kukreja, Jeon et al. leverage spatial transcriptomics to map immune gene expression in the developing mouse brain, identifying key sex-specific changes in the context of maternal gut\u2212immune perturbations.",
      "author": "Brian T. Kalish",
      "published_date": "2026-01-06T00:00:00+00:00",
      "source": "Nature Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 36,
      "reading_time": 1,
      "created_at": "2026-01-15T05:50:52.847618+00:00",
      "updated_at": "2026-01-15T06:27:02.827619+00:00",
      "metadata": {
        "processed_at": "2026-01-15T06:27:02.827621+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "40b9010a5dd0459c98bcb3b6327847c1",
      "url": "https://iopscience.iop.org/article/10.1088/1741-2552/ae30ab",
      "title": "Modulating speech tracking through brain state-dependent changes in audio loudness",
      "content": "Objective. To determine whether the perceptual intensity of speech signals\u2014manipulated via loudness and dynamically adjusted through a brain state-dependent stimulation (BSDS) paradigm\u2014modulates neural speech tracking and short-term memory. Approach. We implemented an EEG brain state-dependent design in which real-time variations in alpha power were used to modulate the loudness of pre-recorded digits during a task modelled on the digit span test. Speech tracking was quantified using lagged Gaussian copula mutual information (2\u201310 Hz), and behavioural performance was assessed through recall accuracy. Main results. Contrary to our initial hypothesis that higher loudness would enhance speech tracking and memory via bottom\u2013up attention, digit recall accuracy was stable across loudness conditions. Speech tracking revealed an unexpected pattern: louder stimuli presented during high alpha power (low attention) elicited reduced tracking magnitudes and shorter peak latencies, whereas quieter stimuli delivered during low alpha power (high attention) produced stronger and more temporally extended tracking responses. Significance. These findings may suggest that internal attentional state, rather than external stimulus salience, plays a dominant role in shaping speech encoding. The study provides proof-of-concept evidence for BSDS in auditory paradigms, showing the importance of attentional fluctuations and stimulus loudness in determining the strength and timing of neural speech tracking, with implications for the design of adaptive speech-enhancement strategies.",
      "author": "Alejandro P\u00e9rez, Ainhoa Insausti-Delgado, Hyojin Park and Ander Ramos-Murguialday",
      "published_date": "2026-01-06T00:00:00+00:00",
      "source": "Journal Neural Engineering",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 209,
      "reading_time": 1,
      "created_at": "2026-01-15T05:50:40.536223+00:00",
      "updated_at": "2026-01-15T06:27:02.827623+00:00",
      "metadata": {
        "processed_at": "2026-01-15T06:27:02.827625+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "5b46d7cf0b0030cf012f7c2d1d5b3185",
      "url": "http://doi.org/10.1037/drm0000321",
      "title": "The significance of dreams and dreaming in the traditional culture of Luo people in Kenya.",
      "content": "This article comprises two parts. Part I deals with Augustine Nwoye\u2019s classification of dreams in Africa, a theoretical background that has been used here purposively to explain the significance of dreams and dreaming to the Luo people in Kenya. This article is therefore deliberately anchored on Nwoye\u2019s African theoretical framework on dreams and dreaming. Part II of this article examines different types of dreams from the cultural perspective of the Luo traditional community, dreams that exemplify Nwoye\u2019s tripartite division of dreams and their sources. This article is particularly concerned with the meaning and significance of dreams and dreaming and what cultural practices and beliefs about dreams and dreaming might reveal about the ethical, epistemological, and metaphysical worldview of the Luo. Culturally, dreams serve various functions such as: sourcing names, counseling, healing, communication with ancestral spirits, mourning, solving personal and societal problems, prediction, divination, and fortune-telling, among many other pertinent issues. Dreams have social, cultural, epistemological, and metaphysical functions. These functions have been articulated in this article, \u201cThe Significance of Dreams and Dreaming.\u201d This article is one among only a few publications on the topic concerning the value of dreams and dreaming in the beliefs, customs, traditions, and beliefs of this community, the Luo in Kenya. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2025-10-23T00:00:00+00:00",
      "source": "Dreaming",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2026-01-15T05:50:23.844756+00:00",
      "updated_at": "2026-01-15T06:27:02.827627+00:00",
      "metadata": {
        "processed_at": "2026-01-15T06:27:02.827629+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "072d9ecafdf5205d1cca27f35ec40241",
      "url": "https://arxiv.org/abs/2601.09632",
      "title": "Perceptually-Guided Adjusted Teleporting: Perceptual Thresholds for Teleport Displacements in Virtual Environments",
      "content": "arXiv:2601.09632v1 Announce Type: new \nAbstract: Teleportation is one of the most common locomotion techniques in virtual reality, yet its perceptual properties remain underexplored. While redirected walking research has shown that users' movements can be subtly manipulated without detection, similar imperceptible adjustments for teleportation have not been systematically investigated. This study examines the thresholds at which teleportation displacements become noticeable to users. We conducted a repeated-measures experiment in which participants' selected teleport destinations were altered in both direction (forwards, backwards) and at different ranges (small, large). Detection thresholds for these positional adjustments were estimated using a psychophysical staircase method with a two-alternative forced choice (2AFC) task. Results show that teleport destinations can be shifted without detection, with larger tolerances for backward adjustments and across longer teleport ranges. These findings establish baseline perceptual limits for redirected teleportation and highlight its potential as a design technique. Applications include supporting interpersonal distance management in social VR, guiding players toward objectives in games, and assisting novice users with navigation. By identifying the limits of imperceptible teleportation adjustments, this work extends redirection principles beyond walking to teleportation and opens new opportunities for adaptive and socially aware VR locomotion systems.",
      "author": "Rose Connolly, Victor Zordan, Rachel McDonnell",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 193,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600584+00:00",
      "updated_at": "2026-01-15T06:27:02.827631+00:00",
      "metadata": {
        "processed_at": "2026-01-15T06:27:02.827633+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "927a5380462f1057ec59b11ff2249586",
      "url": "https://arxiv.org/abs/2601.09620",
      "title": "Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust",
      "content": "arXiv:2601.09620v1 Announce Type: new \nAbstract: As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \\textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3$\\times$2$\\times$2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers' desire for more transparency and their trust in AI-assisted news content.",
      "author": "Pooja Prajod, Hannes Cools, Thomas R\\\"oggla, Karthikeya Puttur Venkatraj, Amber Kusters, Alia ElKattan, Pablo Cesar, Abdallah El Ali",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 243,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600552+00:00",
      "updated_at": "2026-01-15T05:28:43.600553+00:00"
    },
    {
      "id": "2d9472b8bb7a6f64c547cfe7f49e7c10",
      "url": "https://arxiv.org/abs/2601.09610",
      "title": "Technological Advances in Two Generations of Consumer-Grade VR Systems: Effects on User Experience and Task Performance",
      "content": "arXiv:2601.09610v1 Announce Type: new \nAbstract: Integrated VR (IVR) systems consist of a head-mounted display (HMD) and body-tracking capabilities. They enable users to translate their physical movements into corresponding avatar movements in real-time, allowing them to perceive their avatars via the displays. Consumer-grade IVR systems have been available for 10 years, significantly fostering VR research worldwide. However, the effects of even apparently significant technological advances of IVR systems on user experience and the overall validity of prior embodiment research using such systems often remain unclear. We ran a user-centered study comparing two comparable IVR generations: a nearly 10-year-old hardware (HTC Vive, 6-point tracking) and a modern counterpart (HTC Vive Pro 2, 6-point tracking). To ensure ecological validity, we evaluated the systems in their commercially available, as-is configurations. In a 2x5 mixed design, participants completed five tasks covering different use cases on either the old or new system. We assessed presence, sense of embodiment, appearance and behavior plausibility, workload, task performance, and gathered qualitative feedback. Results showed no significant system differences, with only small effect sizes. Bayesian analysis further supported the null hypothesis, suggesting that the investigated generational hardware improvements offer limited benefits for user experience and task performance. For the 10-year generational step examined here, excluding potential technological progress in the necessary software components, this supports the validity of conclusions from prior work and underscores the applicability of older configurations for research in embodied VR.",
      "author": "Marie Luisa Fiedler, Christian Merz, Jonathan Tschanter, Carolin Wienrich, Marc Erich Latoschik",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600517+00:00",
      "updated_at": "2026-01-15T05:28:43.600518+00:00"
    },
    {
      "id": "3d2b1c71b28ea9e345386fbfe7c80ca2",
      "url": "https://arxiv.org/abs/2601.09208",
      "title": "Mikasa: A Character-Driven Emotional AI Companion Inspired by Japanese Oshi Culture",
      "content": "arXiv:2601.09208v1 Announce Type: new \nAbstract: Recent progress in large language models and multimodal interaction has made it possible to develop AI companions that can have fluent and emotionally expressive conversations. However, many of these systems have problems keeping users satisfied and engaged over long periods. This paper argues that these problems do not come mainly from weak models, but from poor character design and unclear definitions of the user-AI relationship. I present Mikasa, an emotional AI companion inspired by Japanese Oshi culture-specifically its emphasis on long-term, non-exclusive commitment to a stable character-as a case study of character-driven companion design. Mikasa does not work as a general-purpose assistant or a chatbot that changes roles. Instead, Mikasa is designed as a coherent character with a stable personality and a clearly defined relationship as a partner. This relationship does not force exclusivity or obligation. Rather, it works as a reference point that stabilizes interaction norms and reduces the work users must do to keep redefining the relationship. Through an exploratory evaluation, I see that users describe their preferences using surface-level qualities such as conversational naturalness, but they also value relationship control and imaginative engagement in ways they do not state directly. These results suggest that character coherence and relationship definition work as latent structural elements that shape how good the interaction feels, without users recognizing them as main features. The contribution of this work is to show that character design is a functional part of AI companion systems, not just decoration. Mikasa is one example based on a specific cultural context, but the design principles-commitment to a consistent personality and clear relationship definition-can be used for many emotionally grounded AI companions.",
      "author": "Miki Ueno",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 278,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600481+00:00",
      "updated_at": "2026-01-15T05:28:43.600483+00:00"
    },
    {
      "id": "25e02724c405b5edd9849f0a16dbef3c",
      "url": "https://arxiv.org/abs/2601.09150",
      "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text",
      "content": "arXiv:2601.09150v1 Announce Type: new \nAbstract: Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.",
      "author": "Jianwen Sun, Yukang Feng, Kaining Ying, Chuanhao Li, Zizhen Li, Fanrui Zhang, Jiaxin Ai, Yifan Chang, Yu Dai, Yifei Huang, Kaipeng Zhang",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 204,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600443+00:00",
      "updated_at": "2026-01-15T05:28:43.600444+00:00"
    },
    {
      "id": "d34bf12109dbd8a338867cc0f053033d",
      "url": "https://arxiv.org/abs/2601.09053",
      "title": "Evaluating local large language models for structured extraction from endometriosis-specific transvaginal ultrasound reports",
      "content": "arXiv:2601.09053v1 Announce Type: new \nAbstract: In this study, we evaluate a locally-deployed large-language model (LLM) to convert unstructured endometriosis transvaginal ultrasound (eTVUS) scan reports into structured data for imaging informatics workflows. Across 49 eTVUS reports, we compared three LLMs (7B/8B and a 20B-parameter model) against expert human extraction. The 20B model achieved a mean accuracy of 86.02%, substantially outperforming smaller models and confirming the importance of scale in handling complex clinical text. Crucially, we identified a highly complementary error profile: the LLM excelled at syntactic consistency (e.g., date/numeric formatting) where humans faltered, while human experts provided superior semantic and contextual interpretation. We also found that the LLM's semantic errors were fundamental limitations that could not be mitigated by simple prompt engineering. These findings strongly support a human-in-the-loop (HITL) workflow in which the on-premise LLM serves as a collaborative tool, not a full replacement. It automates routine structuring and flags potential human errors, enabling imaging specialists to focus on high-level semantic validation. We discuss implications for structured reporting and interactive AI systems in clinical practice.",
      "author": "Haiyi Li, Yutong Li, Yiheng Chi, Alison Deslandes, Mathew Leonardi, Shay Freger, Yuan Zhang, Jodie Avery, M. Louise Hull, Hsiang-Ting Chen",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 174,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600408+00:00",
      "updated_at": "2026-01-15T05:28:43.600410+00:00"
    },
    {
      "id": "87c714f1d9a060dc0aad04a6c6ba6e45",
      "url": "https://arxiv.org/abs/2601.09048",
      "title": "Immersive XR That Moves People: How XR Advertising Transforms Comprehension, Empathy, and Behavioural Intention",
      "content": "arXiv:2601.09048v1 Announce Type: new \nAbstract: Extended Reality (XR) affords an enhanced sense of bodily presence that supports experiential modes of comprehension and affective engagement which exceed the possibilities of conventional information delivery. Nevertheless, the psychological processes engendered by XR, and the manner in which these processes inform subsequent behavioural intentions, remain only partially delineated. The present study addresses this issue within an applied context by comparing non-immersive 2D viewing advertising with immersive XR experiential advertising. We examined whether XR strengthens internal responses to a product, specifically perceived comprehension and empathy, and whether these responses, in turn, influence the behavioural outcome of purchase intention. A repeated-measures two-way ANOVA demonstrated a significant main effect of advertising modality, with XR yielding higher ratings on all evaluative dimensions. Mediation analysis further indicated that the elevation in purchase intention was mediated by empathy, whereas no significant mediating effect was observed for comprehension within the scope of this study. These findings suggest that immersive XR experiences augment empathic engagement with virtual products, and that this enhanced empathy plays a pivotal role in shaping subsequent behavioural intentions.",
      "author": "Yuki Kobayashi, Koichi Toida",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 180,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600360+00:00",
      "updated_at": "2026-01-15T05:28:43.600362+00:00"
    },
    {
      "id": "62f7160f79a21062ad5ce0a00f4f8ad7",
      "url": "https://arxiv.org/abs/2601.09045",
      "title": "Exploring Organizational Readiness and Ecosystem Coordination for Industrial XR",
      "content": "arXiv:2601.09045v1 Announce Type: new \nAbstract: Extended Reality (XR) offers transformative potential for industrial support, training, and maintenance; yet, widespread adoption lags despite demonstrated occupational value and hardware maturity. Organizations successfully implement XR in isolated pilots, yet struggle to scale these into sustained operational deployment, a phenomenon we characterize as the ``Pilot Trap.'' This study examines this phenomenon through a qualitative ecosystem analysis of 17 expert interviews across technology providers, solution integrators, and industrial adopters. We identify a ``Great Inversion'' in adoption barriers: critical constraints have shifted from technological maturity to organizational readiness (e.g., change management, key performance indicator alignment, and political resistance). While hardware ergonomics and usability remain relevant, our findings indicate that systemic misalignments between stakeholder incentives are the primary cause of friction preventing enterprise integration. We conclude that successful industrial XR adoption requires a shift from technology-centric piloting to a problem-first, organizational transformation approach, necessitating explicit ecosystem-level coordination.",
      "author": "Hasan Tarik Akbaba, Efe Bozkir, Anna Puhl, S\\\"uleyman \\\"Ozdel, Enkelejda Kasneci",
      "published_date": "2026-01-15T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2026-01-15T05:28:43.600329+00:00",
      "updated_at": "2026-01-15T05:28:43.600330+00:00"
    }
  ]
}