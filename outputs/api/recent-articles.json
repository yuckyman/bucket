{
  "last_updated": "2025-09-09T04:24:46.674283+00:00",
  "count": 20,
  "articles": [
    {
      "id": "fc5774b79d314241273bd6911ab15e9a",
      "url": "https://arxiv.org/abs/2509.05962",
      "title": "The Reel Deal: Designing and Evaluating LLM-Generated Short-Form Educational Videos",
      "content": "arXiv:2509.05962v1 Announce Type: new \nAbstract: Short-form videos are gaining popularity in education due to their concise and accessible format that enables microlearning. Yet, most of these videos are manually created. Even for those automatically generated using artificial intelligence (AI), it is not well understood whether or how they affect learning outcomes, user experience, and trust. To address this gap, we developed ReelsEd, which is a web-based system that uses large language models (LLMs) to automatically generate structured short-form video (i.e., reels) from lecture long-form videos while preserving instructor-authored material. In a between-subject user study with 62 university students, we evaluated ReelsEd and demonstrated that it outperformed traditional long-form videos in engagement, quiz performance, and task efficiency without increasing cognitive load. Learners expressed high trust in our system and valued its clarity, usefulness, and ease of navigation. Our findings point to new design opportunities for integrating generative AI into educational tools that prioritize usability, learner agency, and pedagogical alignment.",
      "author": "Lazaros Stavrinou, Argyris Constantinides, Marios Belk, Vasos Vassiliou, Fotis Liarokapis, Marios Constantinides",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.878249+00:00",
      "updated_at": "2025-09-09T04:24:27.878251+00:00"
    },
    {
      "id": "e13867b552465cf221f93759b1a29d80",
      "url": "https://arxiv.org/abs/2509.05961",
      "title": "A Longitudinal Evaluation of Heart Rate Efficiency for Amateur Runners",
      "content": "arXiv:2509.05961v1 Announce Type: new \nAbstract: Amateur runners are increasingly using wearable devices to track their training, and often do so through simple metrics such as heart rate and pace. However, these metrics are typically analyzed in isolation and lack the explainability needed for long-term self-monitoring. In this paper, we first present Fitplotter, which is a client-side web application designed for the visualization and analysis of data associated with fitness and activity tracking devices. Next, we revisited and formalized Heart Rate Efficiency (HRE), defined as the product of pace and heart rate, as a practical and explainable metric to track aerobic fitness in everyday running. Drawing on more than a decade of training data from one athlete, and supplemented by publicly available logs from twelve runners, we showed that HRE provides more stable and meaningful feedback on aerobic development than heart rate or pace alone. We showed that HRE correlates with training volume, reflects seasonal progress, and remains stable during long runs in well-trained individuals. We also discuss how HRE can support everyday training decisions, improve the user experience in fitness tracking, and serve as an explainable metric to proprietary ones of commercial platforms. Our findings have implications for designing user-centered fitness tools that empower amateur athletes to understand and manage their own performance data.",
      "author": "Evgeny V. Votyakov, Marios Constantinides, Fotis Liarokapis",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.878217+00:00",
      "updated_at": "2025-09-09T04:24:27.878219+00:00"
    },
    {
      "id": "c2e0fc276d538e7af142d9394f61b117",
      "url": "https://arxiv.org/abs/2509.05943",
      "title": "DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification",
      "content": "arXiv:2509.05943v1 Announce Type: new \nAbstract: Motor imagery (MI) based brain-computer interfaces (BCIs) hold significant potential for assistive technologies and neurorehabilitation. However, the precise and efficient decoding of MI remains challenging due to their non-stationary nature and low signal-to-noise ratio. This paper introduces a novel end-to-end deep learning framework of Discriminative Residual Dense Convolutional Autoencoder with Spatio-Temporal Graph Neural Network (DRDCAE-STGNN) to enhance the MI feature learning and classification. Specifically, the DRDCAE module leverages residual-dense connections to learn discriminative latent representations through joint reconstruction and classifica-tion, while the STGNN module captures dynamic spatial dependencies via a learnable graph adjacency matrix and models temporal dynamics using bidirectional long short-term memory (LSTM). Extensive evaluations on BCI Competition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-art performance, with average accuracies of 95.42%, 97.51%, and 90.15%, respectively. Ablation studies confirm the contribution of each component, and interpreta-bility analysis reveals neurophysiologically meaningful connectivity patterns. Moreover, despite its complexity, the model maintains a feasible parameter count and an inference time of 0.32 ms per sample. These results indicate that our method offers a robust, accurate, and interpretable solution for MI-EEG decoding, with strong generalizability across subjects and tasks and meeting the requirements for potential real-time BCI applications.",
      "author": "Yi Wang, Haodong Zhang, Hongqi Li",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.878165+00:00",
      "updated_at": "2025-09-09T04:24:27.878166+00:00"
    },
    {
      "id": "301acf64baca8d184725ccc5ead83dc3",
      "url": "https://arxiv.org/abs/2509.05898",
      "title": "Attention, Action, and Memory: How Multi-modal Interfaces and Cognitive Load Alter Information Retention",
      "content": "arXiv:2509.05898v1 Announce Type: new \nAbstract: Each year, multi-modal interaction continues to grow within both industry and academia. However, researchers have yet to fully explore the impact of multi-modal systems on learning and memory retention. This research investigates how combining gaze-based controls with gesture navigation affects information retention when compared to standard track-pad usage. A total of twelve participants read four textual articles through two different user interfaces which included a track-pad and a multi-modal interface that tracked eye movements and hand gestures for scrolling, zooming, and revealing content. Participants underwent two assessment sessions that measured their information retention immediately and after a twenty-four hour period along with the NASA-TLX workload evaluation and the System Usability Scale assessment. The initial analysis indicates that multi-modal interaction produces similar targeted information retention to traditional track-pad usage, but this neutral effect comes with higher cognitive workload demands and seems to deteriorate with long-term retention. The research results provide new knowledge about how multi-modal systems affect cognitive engagement while providing design recommendations for future educational and assistive technologies that require effective memory performance.",
      "author": "Omar Elgohary,  Zhu-Tien",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 178,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.878130+00:00",
      "updated_at": "2025-09-09T04:24:27.878132+00:00"
    },
    {
      "id": "be8c3c6f1a2425514a7779d61d1f7abf",
      "url": "https://arxiv.org/abs/2509.05829",
      "title": "Augmenting Human-Centered Racial Covenant Detection and Georeferencing with Plug-and-Play NLP Pipelines",
      "content": "arXiv:2509.05829v1 Announce Type: new \nAbstract: Though no longer legally enforceable, racial covenants in twentieth-century property deeds continue to shape spatial and socioeconomic inequalities. Understanding this legacy requires identifying racially restrictive language and geolocating affected properties. The Mapping Prejudice project addresses this by engaging volunteers on the Zooniverse crowdsourcing platform to transcribe covenants from scanned deeds and link them to modern parcel maps using transcribed legal descriptions. While the project has explored automation, it values crowdsourcing for its social impact and technical advantages. Historically, Mapping Prejudice relied on lexicon-based searching and, more recently, fuzzy matching to flag suspected covenants. However, fuzzy matching has increased false positives, burdening volunteers and raising scalability concerns. Additionally, while many properties can be mapped automatically, others still require time-intensive manual geolocation.\n  We present a human-centered computing approach with two plug-and-play NLP pipelines: (1) a context-aware text labeling model that flags racially restrictive language with high precision and (2) a georeferencing module that extracts geographic descriptions from deeds and resolves them to real-world locations. Evaluated on historical deed documents from six counties in Minnesota and Wisconsin, our system reduces false positives in racial term detection by 25.96% while maintaining 91.73% recall and achieves 85.58% georeferencing accuracy within 1x1 square-mile ranges. These tools enhance document filtering and enrich spatial annotations, accelerating volunteer participation and reducing manual cleanup while strengthening public engagement.",
      "author": "Jiyoon Pyo, Yuankun Jiao, Yao-Yi Chiang, Michael Corey",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 223,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.878098+00:00",
      "updated_at": "2025-09-09T04:24:27.878100+00:00"
    },
    {
      "id": "78a983ec5bfaba30d6f53ac476a732d6",
      "url": "https://arxiv.org/abs/2509.05721",
      "title": "A Composable Agentic System for Automated Visual Data Reporting",
      "content": "arXiv:2509.05721v1 Announce Type: new \nAbstract: To address the brittleness of monolithic AI agents, our prototype for automated visual data reporting explores a Human-AI Partnership model. Its hybrid, multi-agent architecture strategically externalizes logic from LLMs to deterministic modules, leveraging the rule-based system Draco for principled visualization design. The system delivers a dual-output: an interactive Observable report with Mosaic for reader exploration, and executable Marimo notebooks for deep, analyst-facing traceability. This granular architecture yields a fully automatic yet auditable and steerable system, charting a path toward a more synergistic partnership between human experts and AI. For reproducibility, our implementation and examples are available at https://peter-gy.github.io/VISxGenAI-2025/.",
      "author": "P\\'eter Ferenc Gyarmati, Dominik Moritz, Torsten M\\\"oller, Laura Koesten",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 103,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.878056+00:00",
      "updated_at": "2025-09-09T04:24:27.878058+00:00"
    },
    {
      "id": "d63f3c62366f018c3a6d2f253a71bfd7",
      "url": "https://arxiv.org/abs/2509.05718",
      "title": "Do Vision-Language Models See Visualizations Like Humans? Alignment in Chart Categorization",
      "content": "arXiv:2509.05718v1 Announce Type: new \nAbstract: Vision-language models (VLMs) hold promise for enhancing visualization tools, but effective human-AI collaboration hinges on a shared perceptual understanding of visual content. Prior studies assessed VLM visualization literacy through interpretive tasks, revealing an over-reliance on textual cues rather than genuine visual analysis. Our study investigates a more foundational skill underpinning such literacy: the ability of VLMs to recognize a chart's core visual properties as humans do. We task 13 diverse VLMs with classifying scientific visualizations based solely on visual stimuli, according to three criteria: purpose (e.g., schematic, GUI, visualization), encoding (e.g., bar, point, node-link), and dimensionality (e.g., 2D, 3D). Using expert labels from the human-centric VisType typology as ground truth, we find that VLMs often identify purpose and dimensionality accurately but struggle with specific encoding types. Our preliminary results show that larger models do not always equate to superior performance and highlight the need for careful integration of VLMs in visualization tasks, with human supervision to ensure reliable outcomes.",
      "author": "P\\'eter Ferenc Gyarmati, Manfred Klaffenb\\\"ock, Laura Koesten, Torsten M\\\"oller",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 164,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.878030+00:00",
      "updated_at": "2025-09-09T04:24:27.878032+00:00"
    },
    {
      "id": "bdfe0688d5cb3774ef0978999876e4f4",
      "url": "https://arxiv.org/abs/2509.05619",
      "title": "GestoBrush: Facilitating Graffiti Artists' Digital Creation Experiences through Embodied AR Interactions",
      "content": "arXiv:2509.05619v1 Announce Type: new \nAbstract: Graffiti has long documented the socio-cultural landscapes of urban spaces, yet increasing global regulations have constrained artists' creative freedom, prompting exploration of digital alternatives. Augmented Reality (AR) offers opportunities to extend graffiti into digital environments while retaining spatial and cultural significance, but prior research has largely centered on audience engagement rather than the embodied creative processes of graffiti artists. To address this, we developed GestoBrush, a mobile AR prototype that turns smartphones into virtual spray cans, enabling graffiti creation through embodied gestures. A co-design workshop underscored the role of embodiment-physical engagement with surroundings and body-driven creative processes-in digital workflows. We evaluated GestoBrush with six graffiti artists and findings suggested that embodied AR interactions supporting artists bypass real-world constraints and explore new artistic possibilities, whose AR artworks created enhanced senses of intuitiveness, immersion, and expressiveness. This work highlight how embodied AR tools can bridge the gap between physical graffiti practice and digital expression, suggesting pathways for designing immersive creative systems that respect the cultural ethos of street art while expanding its possibilities in virtual spaces.",
      "author": "Ruiqi Chen, Qingyang He, Hanxi Bao, Jung Choi, Xin Tong",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 179,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.877998+00:00",
      "updated_at": "2025-09-09T04:24:27.877999+00:00"
    },
    {
      "id": "8d2b8e8feb648816c3cc23bfe8306813",
      "url": "https://arxiv.org/abs/2509.05491",
      "title": "Hybrid User Interfaces: Past, Present, and Future of Complementary Cross-Device Interaction in Mixed Reality",
      "content": "arXiv:2509.05491v1 Announce Type: new \nAbstract: We investigate hybrid user interfaces (HUIs), aiming to establish a cohesive understanding and adopt consistent terminology for this nascent research area. HUIs combine heterogeneous devices in complementary roles, leveraging the distinct benefits of each. Our work focuses on cross-device interaction between 2D devices and mixed reality environments, which are particularly compelling, leveraging the familiarity of traditional 2D platforms while providing spatial awareness and immersion. Although such HUIs have been prominently explored in the context of mixed reality by prior work, we still lack a cohesive understanding of the unique design possibilities and challenges of such combinations, resulting in a fragmented research landscape. We conducted a systematic survey and present a taxonomy of HUIs that combine conventional display technology and mixed reality environments. Based on this, we discuss past and current challenges, the evolution of definitions, and prospective opportunities to tie together the past 30 years of research with our vision of future HUIs.",
      "author": "Sebastian Hubenschmid, Marc Satkowski, Johannes Zagermann, Juli\\'an M\\'endez, Niklas Elmqvist, Steven Feiner, Tiara Feuchtner, Jens Emil Gr{\\o}nb{\\ae}k, Benjamin Lee, Dieter Schmalstieg, Raimund Dachselt, Harald Reiterer",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.877963+00:00",
      "updated_at": "2025-09-09T04:24:27.877965+00:00"
    },
    {
      "id": "8f67ec0556038b53d6435af16a876b0d",
      "url": "https://arxiv.org/abs/2509.05298",
      "title": "Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression",
      "content": "arXiv:2509.05298v1 Announce Type: new \nAbstract: Loneliness and social isolation pose significant emotional and health challenges, prompting the development of technology-based solutions for companionship and emotional support. This paper introduces Livia, an emotion-aware augmented reality (AR) companion app designed to provide personalized emotional support by combining modular artificial intelligence (AI) agents, multimodal affective computing, progressive memory compression, and AR driven embodied interaction. Livia employs a modular AI architecture with specialized agents responsible for emotion analysis, dialogue generation, memory management, and behavioral orchestration, ensuring robust and adaptive interactions. Two novel algorithms-Temporal Binary Compression (TBC) and Dynamic Importance Memory Filter (DIMF)-effectively manage and prioritize long-term memory, significantly reducing storage requirements while retaining critical context. Our multimodal emotion detection approach achieves high accuracy, enhancing proactive and empathetic engagement. User evaluations demonstrated increased emotional bonds, improved satisfaction, and statistically significant reductions in loneliness. Users particularly valued Livia's adaptive personality evolution and realistic AR embodiment. Future research directions include expanding gesture and tactile interactions, supporting multi-user experiences, and exploring customized hardware implementations.",
      "author": "Rui Xi, Xianghan Wang",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 166,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:27.877925+00:00",
      "updated_at": "2025-09-09T04:24:27.877929+00:00"
    },
    {
      "id": "4c8533f55da668ea8435889d463a635d",
      "url": "https://arxiv.org/abs/2412.12134",
      "title": "Chaotic Dynamics and Fractal Geometry in Ring Lattice Systems of Nonchaotic Rulkov Neurons",
      "content": "arXiv:2412.12134v3 Announce Type: replace-cross \nAbstract: This paper investigates the complex dynamics and fractal attractors that arise in a 60-dimensional ring lattice system of electrically coupled nonchaotic Rulkov neurons. While networks of chaotic Rulkov neurons have been widely studied, systems of nonchaotic Rulkov neurons have not been extensively explored due to the piecewise complexity of the nonchaotic Rulkov map. Here, we find that rich dynamics emerge from the electrical coupling of regular-spiking Rulkov neurons, including chaotic spiking, synchronized chaotic bursting, and synchronized hyperchaos. By systematically varying the electrical coupling strength between neurons, we also uncover general trends in the maximal Lyapunov exponent across the system's dynamical regimes. By means of the Kaplan-Yorke conjecture, we examine the fractal geometry of the ring system's high-dimensional chaotic attractors and find that these attractors can occupy as many as 45 of the 60 dimensions of state space. We further explore how variations in chaotic behavior - quantified by the full Lyapunov spectra - correspond to changes in the attractors' fractal dimensions. This analysis advances our understanding of how complex collective behavior can emerge from the interaction of multiple simple neuron models and highlights the deep interplay between dynamics and geometry in high-dimensional systems.",
      "author": "Brandon B. Le",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 198,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769798+00:00",
      "updated_at": "2025-09-09T04:24:26.769800+00:00"
    },
    {
      "id": "b58e80f2e4356b7441d2a68a646bb38f",
      "url": "https://arxiv.org/abs/2402.07242",
      "title": "A Differentiable Model for Optimizing the Genetic Drivers of Synaptogenesis",
      "content": "arXiv:2402.07242v3 Announce Type: replace-cross \nAbstract: There is growing consensus among neuroscientists that neural circuits critical for survival are the result of genomic decompression processes. We introduce SynaptoGen, a novel computational framework--member of the Connectome Models family--bringing synthetic biological intelligence closer, facilitating neural biological agent development through precise genetic control of synaptogenesis. SynaptoGen is the first model of its kind offering mechanistic explanation of synaptic multiplicity based on genetic expression and protein interaction probabilities. The framework connects genetic factors through a differentiable function, working as a neural network where synaptic weights equal average numbers of synapses between neurons, multiplied by conductance, derived from genetic profiles. Differentiability enables gradient-based optimization, allowing generation of genetic expression patterns producing pre-wired biological agents for specific tasks. Validation in simulated synaptogenesis scenarios shows agents successfully solving four reinforcement learning benchmarks, consistently surpassing control baselines. Despite gaps in biological realism requiring mitigation, this framework has potential to accelerate synthetic biological intelligence research.",
      "author": "Tommaso Boccato, Matteo Ferrante, Nicola Toschi",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769767+00:00",
      "updated_at": "2025-09-09T04:24:26.769768+00:00"
    },
    {
      "id": "352f788152bf8687e3a966c2bc2dedc8",
      "url": "https://arxiv.org/abs/2508.01307",
      "title": "Patterns of imbalance states between sub-brain regimes during development in the resting state",
      "content": "arXiv:2508.01307v2 Announce Type: replace \nAbstract: The functional brain network emerges from the complex, coordinated activity of distinct yet connected regions, which underlie the diverse repertoire of human cognitive functions. Structural Balance Theory (SBT) has been successfully applied to model such nontrivial connections through the analysis of balance and unbalance triadic configurations. In this study, using SBT, we examine the network of imbalanced triads in the resting-state brain subnetworks, which undergo dynamic changes during development. We demonstrate that anticorrelation patterns evolve across the lifespan, reflecting a developmental trajectory from a locally modular organization in childhood to a flexible and reconfigurable architecture during adolescence and finally to a highly segregated and functionally specialized network system in adulthood. This developmental trajectory indicates that the spread of anticorrelations is not an inherent feature of brain organization. This mature organization facilitates a balance between self-referential, internally generated cognitive processes and externally oriented, goal-directed cognition, enabling efficient and adaptive cognitive control. This balance is underpinned by prominent anticorrelations between the Default Mode Network (DMN) and the Frontoparietal Network (FPN) in adulthood. This is while during adolescence these anticorrelations are substantially weaker, suggesting that the maturation of these network connections from adolescence to adulthood establishes a functional architecture that supports the segregation of internal and external cognitive processes. These findings elucidate how the dynamic evolution of anticorrelation patterns in brain networks supports cognitive development across the lifespan, offering new insights into the neural basis of adaptive cognitive control.",
      "author": "Fahimeh Ahmadi, Zahra Moradimanesh, Reza Khosrowabadi, G. Reza Jafari",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 241,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769738+00:00",
      "updated_at": "2025-09-09T04:24:26.769739+00:00"
    },
    {
      "id": "9bc85880f223441a3d1ef476dfe5b2d7",
      "url": "https://arxiv.org/abs/2504.06255",
      "title": "Diagrammatic expansion for the mutual-information rate in the realm of limited statistics",
      "content": "arXiv:2504.06255v3 Announce Type: replace \nAbstract: Neurons in sensory systems encode stimulus information into their stochastic spiking response. The mutual information has been extensively applied to these systems to quantify the neurons' capacity of transmitting such information. Yet, while for discrete stimuli, like flashed images or single tones, its computation is straightforward, for dynamical stimuli it is necessary to compute a (mutual) information rate (MIR), therefore integrating over the multiple temporal correlations which characterize sensory systems. Previous methods are based on extensive sampling of the neuronal response, require large amounts of data and are therefore prone to biases and inaccuracy. Here, we develop Moba-MIRA (moment-based mutual-information-rate approximation), a computational method to estimate the mutual information rate. To derive Moba-MIRA, we use Feynman diagrams to expand the mutual information to arbitrary order in the correlations around the corresponding value for the empirical spike count distributions of single bins. As a result, only the empirical estimation of the pairwise correlations between time bins and the single-bin entropies are required, without the need for the whole joint probability distributions. We tested Moba-MIRA on synthetic data generated with generalized linear models, and showed that it requires only a few tens of stimulus repetitions to provide an accurate estimate of the information rate. Finally, we applied it to ex-vivo electrophysiological recordings of rats retina, obtaining rates ranging between 5 to 20 bits per second, consistent with earlier estimates.",
      "author": "Tobias K\\\"uhn, Gabriel Mahuas, Ulisse Ferrari",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 232,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769703+00:00",
      "updated_at": "2025-09-09T04:24:26.769704+00:00"
    },
    {
      "id": "0023ab7af63b8c8ac171266b86b666f6",
      "url": "https://arxiv.org/abs/2410.13669",
      "title": "Theta and/or alpha? Neural oscillational substrates for dynamic inter-brain synchrony during mother-child cooperation",
      "content": "arXiv:2410.13669v3 Announce Type: replace \nAbstract: Mother-child interaction is a highly dynamic process neurally characterized by inter-brain synchrony (IBS) at {\\theta} and/or {\\alpha} rhythms. However, their establishment, dynamic changes, and roles in mother-child interactions remain unknown. Through dynamic analysis of dual-EEG from 40 mother-child dyads during turn-taking cooperation, we uncover that {\\theta}-IBS and {\\alpha}-IBS alternated with interactive behaviors, with EEG frequency-shift as a prerequisite for IBS transitions. When mothers attempt to track their children's attention and/or predict their intentions, they will adjust their EEG frequencies to align with their children's {\\theta} oscillations, leading to a higher occurrence of the {\\theta}-IBS state. Conversely, the {\\alpha}-IBS state, accompanied by the EEG frequency-shift to the {\\alpha} range, is more prominent during mother-led interactions. Further exploratory analysis reveals greater presence and stability of the {\\theta}-IBS state during cooperative than non-cooperative conditions, particularly in dyads with stronger emotional attachments and more frequent interactions in their daily lives. Our findings shed light on the neural oscillational substrates underlying the IBS dynamics during mother-child interactions.",
      "author": "Jiayang Xu, Yamin Li, Ruxin Su, Saishuang Wu, Chengcheng Wu, Haiwa Wang, Qi Zhu, Yue Fang, Fan Jiang, Shanbao Tong, Yunting Zhang, Xiaoli Guo",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769669+00:00",
      "updated_at": "2025-09-09T04:24:26.769670+00:00"
    },
    {
      "id": "1364fa3eae2974dfc3cf1c7b51e7c7ad",
      "url": "https://arxiv.org/abs/2409.18333",
      "title": "A Framework for Standardizing Similarity Measures in a Rapidly Evolving Field",
      "content": "arXiv:2409.18333v2 Announce Type: replace \nAbstract: Similarity measures are fundamental tools for quantifying the alignment between artificial and biological systems. However, the diversity of similarity measures and their varied naming and implementation conventions makes it challenging to compare across studies. To facilitate comparisons and make explicit the implementation choices underlying a given code package, we have created and are continuing to develop a Python repository that benchmarks and standardizes similarity measures. The goal of creating a consistent naming convention that uniquely and efficiently specifies a similarity measure is not trivial as, for example, even commonly used methods like Centered Kernel Alignment (CKA) have at least 12 different variations, and this number will likely continue to grow as the field evolves. For this reason, we do not advocate for a fixed, definitive naming convention. The landscape of similarity measures and best practices will continue to change and so we see our current repository, which incorporates approximately 100 different similarity measures from 14 packages, as providing a useful tool at this snapshot in time. To accommodate the evolution of the field we present a framework for developing, validating, and refining naming conventions with the goal of uniquely and efficiently specifying similarity measures, ultimately making it easier for the community to make comparisons across studies.",
      "author": "Nathan Cloos, Guangyu Robert Yang, Christopher J. Cueva",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 211,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769639+00:00",
      "updated_at": "2025-09-09T04:24:26.769640+00:00"
    },
    {
      "id": "8d818ce2cdd7eb67b67f08b101efe3f1",
      "url": "https://arxiv.org/abs/2102.08209",
      "title": "Modeling Visual Hallucination: A Generative Adversarial Network Framework",
      "content": "arXiv:2102.08209v2 Announce Type: replace \nAbstract: Visual hallucination refers to the perception of recognizable things that are not present. These phenomena are commonly linked to a range of neurological/psychiatric disorders. Despite ongoing research, the mechanisms through which the visual system generates hallucinations from real-world environments are still not well understood. Abnormal interactions between different regions of the brain responsible for perception are known to contribute to the occurrence of visual hallucinations. In this study, we propose and extend a generative neural network-based framework to address challenges within the visual system, aiming to create goal-driven models inspired by neurobiological mechanisms of visual hallucinations. We focus on the adversarial interactions between the visual system and the frontal lobe regions, proposing the Hallu-GAN model to suggest how these interactions can give rise to visual hallucinations. The architecture of the Hallu-GAN model is based on generative adversarial networks. Our simulation results indicate that disturbances in the ventral stream can lead to visual hallucinations. To further analyze the impact of other brain regions on the visual system, we extend the Hallu-GAN model by adding EEG data from individuals. This extended model, referred to as Hallu-GAN+, enables the examination of both hallucinating and non-hallucinating states. By training the Hallu-GAN+ model with EEG data from an individual with Charles Bonnet syndrome, we demonstrated its utility in analyzing the behavior of those experiencing hallucinations. Our simulation results confirmed the capability of the proposed model in resembling the visual system in both healthy and hallucinating states.",
      "author": "Masoumeh Zareh, Mohammad Hossein Manshaei, Sayed Jalal Zahabi, Marwan Krunz",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 246,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769605+00:00",
      "updated_at": "2025-09-09T04:24:26.769607+00:00"
    },
    {
      "id": "7f02bf86f51ffecf7e6b74d7cc41e6e9",
      "url": "https://arxiv.org/abs/2509.06810",
      "title": "Reward function compression facilitates goal-dependent reinforcement learning",
      "content": "arXiv:2509.06810v1 Announce Type: new \nAbstract: Reinforcement learning agents learn from rewards, but humans can uniquely assign value to novel, abstract outcomes in a goal-dependent manner. However, this flexibility is cognitively costly, making learning less efficient. Here, we propose that goal-dependent learning is initially supported by a capacity-limited working memory system. With consistent experience, learners create a \"compressed\" reward function (a simplified rule defining the goal) which is then transferred to long-term memory and applied automatically upon receiving feedback. This process frees up working memory resources, boosting learning efficiency. We test this theory across six experiments. Consistent with our predictions, our findings demonstrate that learning is parametrically impaired by the size of the goal space, but improves when the goal space structure allows for compression. We also find faster reward processing to correlate with better learning performance, supporting the idea that as goal valuation becomes more automatic, more resources are available for learning. We leverage computational modeling to support this interpretation. Our work suggests that efficient goal-directed learning relies on compressing complex goal information into a stable reward function, shedding light on the cognitive mechanisms of human motivation. These findings generate new insights into the neuroscience of intrinsic motivation and could help improve behavioral techniques that support people in achieving their goals.",
      "author": "Gaia Molinaro, Anne G. E. Collins",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 211,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769568+00:00",
      "updated_at": "2025-09-09T04:24:26.769569+00:00"
    },
    {
      "id": "f170ce41eebc2330ac5870cf9b3f40db",
      "url": "https://arxiv.org/abs/2509.06426",
      "title": "Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster",
      "content": "arXiv:2509.06426v1 Announce Type: new \nAbstract: Computational models are critical to advance our understanding of how neural, biomechanical, and physical systems interact to orchestrate animal behaviors. Despite the availability of near-complete reconstructions of the Drosophila melanogaster central nervous system, musculature, and exoskeleton, anatomically and physically grounded models of fly leg muscles are still missing. These models provide an indispensable bridge between motor neuron activity and joint movements. Here, we introduce the first 3D, data-driven musculoskeletal model of Drosophila legs, implemented in both OpenSim and MuJoCo simulation environments. Our model incorporates a Hill-type muscle representation based on high-resolution X-ray scans from multiple fixed specimens. We present a pipeline for constructing muscle models using morphological imaging data and for optimizing unknown muscle parameters specific to the fly. We then combine our musculoskeletal models with detailed 3D pose estimation data from behaving flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of muscle activity across diverse walking and grooming behaviors predict coordinated muscle synergies that can be tested experimentally. Furthermore, by training imitation learning policies in MuJoCo, we test the effect of different passive joint properties on learning speed and find that damping and stiffness facilitate learning. Overall, our model enables the investigation of motor control in an experimentally tractable model organism, providing insights into how biomechanics contribute to generation of complex limb movements. Moreover, our model can be used to control embodied artificial agents to generate naturalistic and compliant locomotion in simulated environments.",
      "author": "Pembe Gizem \\\"Ozdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 240,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769533+00:00",
      "updated_at": "2025-09-09T04:24:26.769535+00:00"
    },
    {
      "id": "c4cbc8f8447c272e7d52cafeae0fbdf2",
      "url": "https://arxiv.org/abs/2509.05305",
      "title": "Predicting Brain Morphogenesis via Physics-Transfer Learning",
      "content": "arXiv:2509.05305v1 Announce Type: new \nAbstract: Brain morphology is shaped by genetic and mechanical factors and is linked to biological development and diseases. Its fractal-like features, regional anisotropy, and complex curvature distributions hinder quantitative insights in medical inspections. Recognizing that the underlying elastic instability and bifurcation share the same physics as simple geometries such as spheres and ellipses, we developed a physics-transfer learning framework to address the geometrical complexity. To overcome the challenge of data scarcity, we constructed a digital library of high-fidelity continuum mechanics modeling that both describes and predicts the developmental processes of brain growth and disease. The physics of nonlinear elasticity from simple geometries is embedded into a neural network and applied to brain models. This physics-transfer approach demonstrates remarkable performance in feature characterization and morphogenesis prediction, highlighting the pivotal role of localized deformation in dominating over the background geometry. The data-driven framework also provides a library of reduced-dimensional evolutionary representations that capture the essential physics of the highly folded cerebral cortex. Validation through medical images and domain expertise underscores the deployment of digital-twin technology in comprehending the morphological complexity of the brain.",
      "author": "Yingjie Zhao, Yicheng Song, Fan Xu, Zhiping Xu",
      "published_date": "2025-09-09T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 185,
      "reading_time": 1,
      "created_at": "2025-09-09T04:24:26.769490+00:00",
      "updated_at": "2025-09-09T04:24:26.769494+00:00"
    }
  ]
}