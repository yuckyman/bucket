{
  "last_updated": "2025-11-17T05:22:34.089800+00:00",
  "count": 20,
  "articles": [
    {
      "id": "3c4665189157e0452e198093ceb16f0f",
      "url": "https://arxiv.org/abs/2511.10853",
      "title": "Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction",
      "content": "arXiv:2511.10853v1 Announce Type: cross \nAbstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.",
      "author": "Gerui Xu, Boyou Chen, Huizhong Guo, Dave LeBlanc, Ananna Ahmed, Zhaonan Sun, Shan Bao",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 222,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911724+00:00",
      "updated_at": "2025-11-17T05:22:13.911726+00:00"
    },
    {
      "id": "0a79a1a8e37b05f87e4aad50c7b544ca",
      "url": "https://arxiv.org/abs/2511.10693",
      "title": "Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate",
      "content": "arXiv:2511.10693v1 Announce Type: cross \nAbstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both \"polite and formal\" and \"casual and informal\" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.",
      "author": "Eyal Rabin, Zohar Elyoseph, Rotem Israel-Fishelson, Adi Dali, Ravit Nussinson",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911690+00:00",
      "updated_at": "2025-11-17T05:22:13.911692+00:00"
    },
    {
      "id": "a0c4fe593dcc35c983547cf7fc124cb5",
      "url": "https://arxiv.org/abs/2511.10652",
      "title": "Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI",
      "content": "arXiv:2511.10652v1 Announce Type: cross \nAbstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency",
      "author": "Rafael Arias Gonzalez, Steve DiPaola",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 178,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911661+00:00",
      "updated_at": "2025-11-17T05:22:13.911663+00:00"
    },
    {
      "id": "f13ab26436d42dbbf5f0c7a8f709ad7b",
      "url": "https://arxiv.org/abs/2511.11476",
      "title": "Context-aware Adaptive Visualizations for Critical Decision Making",
      "content": "arXiv:2511.11476v1 Announce Type: new \nAbstract: Effective decision-making often relies on timely insights from complex visual data. While Information Visualization (InfoVis) dashboards can support this process, they rarely adapt to users' cognitive state, and less so in real time. We present Symbiotik, an intelligent, context-aware adaptive visualization system that leverages neurophysiological signals to estimate mental workload (MWL) and dynamically adapt visual dashboards using reinforcement learning (RL). Through a user study with 120 participants and three visualization types, we demonstrate that our approach improves task performance and engagement. Symbiotik offers a scalable, real-time adaptation architecture, and a validated methodology for neuroadaptive user interfaces.",
      "author": "Angela Lopez-Cardona, Mireia Masias Bruns, Nuwan T. Attygalle, Sebastian Idesis, Matteo Salvatori, Konstantinos Raftopoulos, Konstantinos Oikonomou, Saravanakumar Duraisamy, Parvin Emami, Nacera Latreche, Alaa Eddine Anis Sahraoui, Michalis Vakallelis, Jean Vanderdonckt, Ioannis Arapakis, Luis A. Leiva",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 101,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911629+00:00",
      "updated_at": "2025-11-17T05:22:13.911631+00:00"
    },
    {
      "id": "2c3cbbe3e25bd2ee8067eef2b2830bbd",
      "url": "https://arxiv.org/abs/2511.11287",
      "title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction",
      "content": "arXiv:2511.11287v1 Announce Type: new \nAbstract: The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces  and  tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.",
      "author": "Sven Schultze, Meike Verena Kietzmann, Nils-Lucas Sch\\\"onfeld, Ruth Stock-Homburg",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 166,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911597+00:00",
      "updated_at": "2025-11-17T05:22:13.911598+00:00"
    },
    {
      "id": "1fd39c63070492fb665a46530718ee24",
      "url": "https://arxiv.org/abs/2511.11229",
      "title": "Devising Experiments with Interactive Environments",
      "content": "arXiv:2511.11229v1 Announce Type: new \nAbstract: This paper reports a practice-based investigation into authoring responsive light and sound in immersive performance without writing code. A modular system couples live gesture, position, and speech inputs to scenographic outputs through a visual logic layer that performers can operate in rehearsal. Across six workshops with eight professional performance-makers, we staged a progression from parallel ensemble and technical training to integrated dramaturgy, culminating in a single-spectator scratch immersive performance with interactive elements. This paper details the system's building blocks and the workshop arc. A reflexive reading of workshop video logs, post-workshop focus groups, and facilitator notes surfaced three ensemble-level strategies that made the technology workable in a hybrid devising/design practice: rotating roles between operator, performer, and mediator; embracing controlled imperfection as a creative resource; and using technology-describing metaphors to support creative practice.",
      "author": "Pavlos Panagiotidis, Jocelyn Spence, Nils Jager",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 137,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911567+00:00",
      "updated_at": "2025-11-17T05:22:13.911568+00:00"
    },
    {
      "id": "b610a498f5530c00d75f19fa956ed779",
      "url": "https://arxiv.org/abs/2511.11209",
      "title": "Towards Usable Privacy Management for IoT TAPs: Deriving Privacy Clusters and Preference Profiles",
      "content": "arXiv:2511.11209v1 Announce Type: new \nAbstract: IoT Trigger-Action Platforms (TAPs) typically offer coarse-grained permission controls. Even when fine-grained controls are available, users are likely overwhelmed by the complexity of setting privacy preferences. This paper contributes to usable privacy management for TAPs by deriving privacy clusters and profiles for different types of users that can be semi-automatically assigned or suggested to them. We developed and validated a questionnaire, based on users' privacy concerns regarding confidentiality and control and their requirements towards transparency in TAPs. In an online study (N=301), where participants were informed about potential privacy risks, we clustered users by their privacy concerns and requirements into Basic, Medium and High Privacy clusters. These clusters were then characterized by the users' data sharing preferences, based on a factorial vignette approach, considering the data categories, the data recipient types, and the purpose of data sharing. Our findings show three distinct privacy profiles, providing a foundation for more usable privacy controls in TAPs.",
      "author": "Piero Romare, Farzaneh Karegar, Simone Fischer-H\\\"ubner",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911538+00:00",
      "updated_at": "2025-11-17T05:22:13.911540+00:00"
    },
    {
      "id": "d170392eaa5c9e4d27cc53c486e832b9",
      "url": "https://arxiv.org/abs/2511.11187",
      "title": "ReTrace: Interactive Visualizations for Reasoning Traces of Large Reasoning Models",
      "content": "arXiv:2511.11187v1 Announce Type: new \nAbstract: Recent advances in Large Language Models have led to Large Reasoning Models, which produce step-by-step reasoning traces. These traces offer insight into how models think and their goals, improving explainability and helping users follow the logic, learn the process, and even debug errors. These traces, however, are often verbose and complex, making them cognitively demanding to comprehend. We address this challenge with ReTrace, an interactive system that structures and visualizes textual reasoning traces to support understanding. We use a validated reasoning taxonomy to produce structured reasoning data and investigate two types of interactive visualizations thereof. In a controlled user study, both visualizations enabled users to comprehend the model's reasoning more accurately and with less perceived effort than a raw text baseline. The results of this study could have design implications for making long and complex machine-generated reasoning processes more usable and transparent, an important step in AI explainability.",
      "author": "Ludwig Felder, Jacob Miller, Markus Wallinger, Stephen Kobourov, Chunyang Chen",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911491+00:00",
      "updated_at": "2025-11-17T05:22:13.911507+00:00"
    },
    {
      "id": "9d504785579ab9445bb0074841d1ad93",
      "url": "https://arxiv.org/abs/2511.11112",
      "title": "C2Views: Knowledge-based Colormap Design for Multiple-View Consistency",
      "content": "arXiv:2511.11112v1 Announce Type: new \nAbstract: Multiple-view (MV) visualization provides a comprehensive and integrated perspective on complex data, establishing itself as an effective method for visual communication and exploratory data analysis. While existing studies have predominantly focused on designing explicit visual linkages and coordinated interactions to facilitate the exploration of MV visualizations, these approaches often demand extra graphical and interactive effort, overlooking the potential of color as an effective channel for encoding data and relationships. Addressing this oversight, we introduce C2Views, a new framework for colormap design that implicitly shows the relation across views. We begin by structuring the components and their relationships within MVs into a knowledge-based graph specification, wherein colormaps, data, and views are denoted as entities, and the interactions among them are illustrated as relations. Building on this representation, we formulate the design criteria as an optimization problem and employ a genetic algorithm enhanced by Pareto optimality, generating colormaps that balance single-view effectiveness and multiple-view consistency. Our approach is further complemented with an interactive interface for user-intended refinement. We demonstrate the feasibility of C2Views through various colormap design examples for MVs, underscoring its adaptability to diverse data relationships and view layouts. Comparative user studies indicate that our method outperforms the existing approach in facilitating color distinction and enhancing multiple-view consistency, thereby simplifying data exploration processes.",
      "author": "Yihan Hou, Yilin Ye, Liangwei Wang, Huamin Qu, Wei Zeng",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911460+00:00",
      "updated_at": "2025-11-17T05:22:13.911462+00:00"
    },
    {
      "id": "d77b8883a966546de6f8e7f2c0e2f196",
      "url": "https://arxiv.org/abs/2511.10826",
      "title": "Surveillance and Disability in Online Proctored Exams: Student Perspectives and Design Implications",
      "content": "arXiv:2511.10826v1 Announce Type: new \nAbstract: Online proctoring systems (OPS) are technologies and services that are used to monitor students during an online exam to deter cheating. However, OPS often violates student privacy by implementing overly intrusive surveillance to which students cannot consent meaningfully. The technologies used in OPS have been shown to unfairly flag students with disabilities. Our reflexive thematic analysis of interviews with students who have first-hand experience with online invigilated exams and who have disability accommodations points to their anxiety about the interaction between surveillance and their disabilities, leading to fears about misrepresentation and increased cognitive load on the exam. Students describe the compromises they need to make with their privacy and accommodations to take remote tests and share their privacy values. We present the implications for the design of OPS to mitigate the issues faced by disabled students.",
      "author": "Monika Blue Kwapisz, Yoav Ackerman, Jennifer Nguyen, Prashanth Rajivan",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 141,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:13.911417+00:00",
      "updated_at": "2025-11-17T05:22:13.911421+00:00"
    },
    {
      "id": "783f12d737d787f62baa6b1acfd0999f",
      "url": "https://arxiv.org/abs/2509.04454",
      "title": "Mechanisms for anesthesia, unawareness, respiratory depression, memory replay and sleep: MHb > IPN > PAG + DRN + MRN > claustrum > cortical slow-waves",
      "content": "arXiv:2509.04454v2 Announce Type: replace \nAbstract: My findings show what causes loss of awareness, anesthesia, memory replay, opioid induced respiratory depression (OIRD), and slow-wave sleep (SWS). Opiates are fast pain relievers and anesthetics that can cause respiratory arrest. I found how mu-opioids and anesthetics by activating medial habenula (MHb) and/or interpeduncular nucleus (IPN) induce unawareness and slowdown respiration. MHb projects to IPN and both increase their glucose intake during anesthesia (Herkenham, 1981). The question is: What is the MHb-IPN circuit doing? I found that it promotes SWS, memory replay, sharp-wave ripples, spindles, hippocampo-cortical replay, synaptogenesis, rest and recovery, by activating median raphe (MRN) serotonin, and by inhibiting the theta state circuit, new memories encoding, awareness, arousal, alert wakefulness, and REM sleep. It causes also natural slowdown of respiration and heart rate, while it inhibits locomotion and arousal. This extended model adds role of the dentate gyrus>posterior septum>MHb>IPN>MRN>hippocampus + BF + claustrum>cortical slow-waves in memory replay, ripples, loss of awareness, SWS, and anesthesia. It proposes new neural mechanism for anesthetic ketamine, nitrous oxide, and phencyclidine effects: activation of the IPN>MRN>claustrum>cortical SWA circuit by the 5-HT2a receptors in the IPN and claustrum. My model shows why are ketamine and psychedelics anxiolytic and antidepressant. How they by activating the 5-HT2a receptors in vACC/infralimbic cortex increase safety, well-being signal, socializing, and cognitive flexibility, and attenuate fear, worries, anger, impulsivity, self-defence, and wanting. This model claims that mu-opioids, acetylcholine, nicotine, endocannabinoids, adenosine, GLP-1RA, and substance P activate the MHb-IPN-MRN circuit which promotes rest, recovery, repair, serotonin-BDNF-protein production, spines growth, and anti-inflammatory state.",
      "author": "Karin Vadovi\\v{c}ov\\'a",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 256,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:12.782615+00:00",
      "updated_at": "2025-11-17T05:22:12.782617+00:00"
    },
    {
      "id": "dd0fc049aee037639507a1a5994ad0d3",
      "url": "https://arxiv.org/abs/2511.10935",
      "title": "CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding",
      "content": "arXiv:2511.10935v1 Announce Type: cross \nAbstract: Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.",
      "author": "Yifan Zhuang, Calvin Huang, Zepeng Yu, Yongjie Zou, Jiawei Ju",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 238,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:12.782578+00:00",
      "updated_at": "2025-11-17T05:22:12.782579+00:00"
    },
    {
      "id": "7255dad355a9f8572363f6f223718af3",
      "url": "https://arxiv.org/abs/2511.10835",
      "title": "What the flock knows that the birds do not: exploring the emergence of joint agency in multi-agent active inference",
      "content": "arXiv:2511.10835v1 Announce Type: cross \nAbstract: Collective behavior pervades biological systems, from flocks of birds to neural assemblies and human societies. Yet, how such collectives acquire functional properties -- such as joint agency or knowledge -- that transcend those of their individual components remains an open question. Here, we combine active inference and information-theoretic analyses to explore how a minimal system of interacting agents can give rise to joint agency and collective knowledge. We model flocking dynamics using multiple active inference agents, each minimizing its own free energy while coupling reciprocally with its neighbors. We show that as agents self-organize, their interactions define higher-order statistical boundaries (Markov blankets) enclosing a ``flock'' that can be treated as an emergent agent with its own sensory, active, and internal states. When exposed to external perturbations (a ``predator''), the flock exhibits faster, coordinated responses than individual agents, reflecting collective sensitivity to environmental change. Crucially, analyses of synergistic information reveal that the flock encodes information about the predator's location that is not accessible to every individual bird, demonstrating implicit collective knowledge. Together, these results show how informational coupling among active inference agents can generate new levels of autonomy and inference, providing a framework for understanding the emergence of (implicit) collective knowledge and joint agency.",
      "author": "Domenico Maisto, Davide Nuzzi, Giovanni Pezzulo",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 208,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:12.782541+00:00",
      "updated_at": "2025-11-17T05:22:12.782543+00:00"
    },
    {
      "id": "6c4ca20adecae07e58120f4858674a10",
      "url": "https://arxiv.org/abs/2511.11480",
      "title": "Inferring response times of perceptual decisions with Poisson variational autoencoders",
      "content": "arXiv:2511.11480v1 Announce Type: new \nAbstract: Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.",
      "author": "Hayden R. Johnson, Anastasia N. Krouglova, Hadi Vafaii, Jacob L. Yates, Pedro J. Gon\\c{c}alves",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 166,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:12.782479+00:00",
      "updated_at": "2025-11-17T05:22:12.782481+00:00"
    },
    {
      "id": "b454779dddd9e5afee1dfb913c8cb074",
      "url": "https://arxiv.org/abs/2511.11463",
      "title": "A universal theorem of sensory information",
      "content": "arXiv:2511.11463v1 Announce Type: new \nAbstract: A universal theorem of sensory information, analogous to the second law of thermodynamics, is derived. Beginning from a minimal description of a sensory neuron, a state-space representation of firing rate emerges naturally from Shannon's measure of information. A special case of this formulation predicts a previously unknown inequality governing sensory adaptation, which was confirmed across different modalities, species, and experimental conditions. Further analysis shows that the firing rate behaves like a state function in thermodynamics, leading to an entropy production equation from which a general law follows: any closed cycle of stimulation yields a non-negative net gain of sensory information.",
      "author": "Willy Wong",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 105,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:12.782446+00:00",
      "updated_at": "2025-11-17T05:22:12.782448+00:00"
    },
    {
      "id": "571dfcf549c055c36c72c5f6b6e118c9",
      "url": "https://arxiv.org/abs/2511.10757",
      "title": "Habit learning is associated with efficiently controlled network dynamics in naive macaque monkeys",
      "content": "arXiv:2511.10757v1 Announce Type: new \nAbstract: Primates utilize distributed neural circuits to learn habits in uncertain environments, but the underlying mechanisms remain poorly understood. We propose a formal theory of network energetics explaining how brain states influence sequential behavior. We test our theory on multi-unit recordings from the caudate nucleus and cortical regions of macaques performing a motor habit task. The theory predicts the energy required to transition between brain states represented by trial-specific firing rates across channels, assuming activity spreads through effective connections. We hypothesized that habit formation would correlate with lower control energy. Consistent with this, we observed smaller energy requirements for transitions between similar saccade patterns and those of intermediate complexity, and sessions exploiting fewer patterns. Simulations ruled out confounds from neurons' directional tuning. Finally, virtual lesioning demonstrated robustness of observed relationships between control energy and behavior. This work paves the way for examining how behavior arises from changing activity in distributed circuitry.",
      "author": "Julia K. Brynildsen, Panagiotis Fotiadis, Karol P. Szymula, Jason Z. Kim, Fabio Pasqualetti, Ann M. Graybiel, Theresa M. Desrochers, Dani S. Bassett",
      "published_date": "2025-11-17T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-11-17T05:22:12.782415+00:00",
      "updated_at": "2025-11-17T05:22:12.782419+00:00"
    },
    {
      "id": "50bc1490b16353dccc56170c6bcb85c7",
      "url": "https://peterullrich.com/listen-to-database-changes-through-the-postgres-wal",
      "title": "Listen to Database Changes Through the Postgres WAL",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45885768\">Comments</a>",
      "author": "",
      "published_date": "2025-11-11T10:08:30+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-17T05:21:23.900910+00:00",
      "updated_at": "2025-11-17T05:21:23.900912+00:00"
    },
    {
      "id": "3253e34d22d1c5457279d59dddc23124",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306452225009868?dgcid=rss_sd_all",
      "title": "Comprehensive analysis of the prognostic value and immune infiltration of Uridine Monophosphate Synthetase (UMPS) in Pan-Glioma",
      "content": "<p>Publication date: 5 December 2025</p><p><b>Source:</b> Neuroscience, Volume 590</p><p>Author(s): Dong He, Xiaokun Jiang, Jinfeng Ma, Jinyan Chen, Yongfei Zhang, Xixi Dou, Qingwen Jia, Qian Liu, Ping Xie, Zhen Zhang</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 28,
      "reading_time": 1,
      "created_at": "2025-11-17T04:00:27.630458+00:00",
      "updated_at": "2025-11-17T04:21:52.051661+00:00",
      "metadata": {
        "processed_at": "2025-11-17T04:21:52.051670+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "4962c7307c1d0790faec32482dc3b2f2",
      "url": "https://www.reddit.com/r/Python/comments/1oz4x0f/ultrastrict_python_template_v2_uv_ruff/",
      "title": "Ultra-strict Python template v2 (uv + ruff + basedpyright)",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>Some time ago I shared a strict Python project setup. I\u2019ve since reworked and simplified it, and this is the <strong>new version</strong>.</p> <blockquote> <p><strong>pystrict-strict-python</strong> \u2013 an ultra-strict Python project template using <code>uv</code>, <code>ruff</code>, and <code>basedpyright</code>, inspired by TypeScript\u2019s <code>--strict</code> mode.</p> </blockquote> <p>Compared to my previous post, this version:</p> <ul> <li>focuses on a single <strong>pyproject.toml</strong> as the source of truth,</li> <li>switches to <code>basedpyright</code> with a clearer strict configuration,</li> <li>tightens the ruff rules and coverage settings,</li> <li>and is easier to drop into new or existing projects.</li> </ul> <p><strong>What it gives you</strong></p> <ul> <li><strong>Strict static typing</strong> with <code>basedpyright</code> (TS <code>--strict</code> style rules): <ul> <li>No implicit <code>Any</code></li> <li>Optional/<code>None</code> usage must be explicit</li> <li>Unused imports / variables / functions are treated as errors</li> </ul></li> <li><strong>Aggressive linting &amp; formatting</strong> with <code>ruff</code>: <ul> <li>pycodestyle, pyflakes, isort</li> <li>bugbear, security checks, performance, annotations, async, etc.</li> </ul></li> <li><strong>Testing &amp; coverage</strong>: <ul> <li><code>pytest</code> + <code>coverage</code> with 80% coverage enforced by default</li> </ul></li> <li><strong>Task runner via <code>poethepoet</code></strong>: <ul> <li><code>poe format</code> \u2192 format + lint + type check</li> <li><code>poe check</code> \u2192 lint + type check (no auto-fix)</li> <li><code>poe metrics</code> \u2192 dead code + complexity + maintainability</li> <li><code>poe quality</code> \u2192 full quality pipeline</li> </ul></li> <li><strong>Single-source config</strong>: everything is in <strong>pyproject.toml</strong></li> </ul> <p><strong>Use cases</strong></p> <ul> <li><p><strong>New projects</strong>:<br /> Copy the <strong>pyproject.toml</strong>, adjust the <code>[project]</code> metadata, create <code>src/your_package</code> + <code>tests/</code>, and install with:</p> <p>```bash uv venv .venv\\Scripts\\activate # Windows</p> <h1>or: source .venv/bin/activate</h1> <p>uv pip install -e &quot;.[dev]&quot; ```</p> <p>Then your daily loop is basically:</p> <p><code>bash uv run ruff format . uv run ruff check . --fix uv run basedpyright uv run pytest </code></p></li> <li><p><strong>Existing projects</strong>:<br /> You don\u2019t have to go \u201call in\u201d on day 1. You can cherry-pick:</p> <ul> <li>the <code>ruff</code> config,</li> <li>the <code>basedpyright</code> config,</li> <li>the pytest/coverage sections,</li> <li>and the dev dependencies,</li> </ul> <p>and progressively tighten things as you fix issues.</p></li> </ul> <p><strong>Why I built this v2</strong></p> <p>The first version worked, but it was a bit heavier and less focused. In this iteration I wanted:</p> <ul> <li>a cleaner, copy-pastable template,</li> <li>stricter typing rules by default,</li> <li>better defaults for dead code, complexity, and coverage,</li> <li>and a straightforward workflow that feels natural to run locally and in CI.</li> </ul> <p><strong>Repo</strong></p> <p>\ud83d\udc49 <a href=\"https://github.com/Ranteck/PyStrict-strict-python\">GitHub link here</a></p> <p>If you saw my previous post and tried that setup, I\u2019d love to hear how this version compares. Feedback very welcome:</p> <ul> <li>Rules that feel too strict or too lax?</li> <li>Basedpyright / ruff settings you\u2019d tweak?</li> <li>Ideas for a \u201cgradual adoption\u201d profile for large legacy codebases?</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ranteck\"> /u/Ranteck </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1oz4x0f/ultrastrict_python_template_v2_uv_ruff/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1oz4x0f/ultrastrict_python_template_v2_uv_ruff/\">[comments]</a></span>",
      "author": "/u/Ranteck",
      "published_date": "2025-11-17T02:46:14+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 426,
      "reading_time": 2,
      "created_at": "2025-11-17T03:59:43.472531+00:00",
      "updated_at": "2025-11-17T04:21:52.051674+00:00",
      "metadata": {
        "processed_at": "2025-11-17T04:21:52.051677+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "f987d9fd50052e48125e25bb3e4c804a",
      "url": "https://www.reddit.com/r/Python/comments/1oz3zqn/best_way_to_avoid_getting_rusty_with_python/",
      "title": "best way to avoid getting rusty with Python?",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>I don\u2019t code in Python daily, more like off and on for side projects or quick scripts. But every time I come back, it takes me a sec to get back in the groove. What do y\u2019all do to keep your Python skills fresh? Any favorite mini projects, sites, or habits that actually help?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Enlitenkanin\"> /u/Enlitenkanin </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1oz3zqn/best_way_to_avoid_getting_rusty_with_python/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1oz3zqn/best_way_to_avoid_getting_rusty_with_python/\">[comments]</a></span>",
      "author": "/u/Enlitenkanin",
      "published_date": "2025-11-17T02:02:50+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 75,
      "reading_time": 1,
      "created_at": "2025-11-17T03:59:43.472469+00:00",
      "updated_at": "2025-11-17T04:21:52.051679+00:00",
      "metadata": {
        "processed_at": "2025-11-17T04:21:52.051681+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}