{
  "last_updated": "2026-01-22T05:30:45.944590+00:00",
  "count": 20,
  "articles": [
    {
      "id": "e4ce644e22b3482ad1db9b723cdab07b",
      "url": "https://arxiv.org/abs/2601.14641",
      "title": "MIND: Empowering Mental Health Clinicians with Multimodal Data Insights through a Narrative Dashboard",
      "content": "arXiv:2601.14641v1 Announce Type: new \nAbstract: Advances in data collection enable the capture of rich patient-generated data: from passive sensing (e.g., wearables and smartphones) to active self-reports (e.g., cross-sectional surveys and ecological momentary assessments). Although prior research has demonstrated the utility of patient-generated data in mental healthcare, significant challenges remain in effectively presenting these data streams along with clinical data (e.g., clinical notes) for clinical decision-making. Through co-design sessions with five clinicians, we propose MIND, a large language model-powered dashboard designed to present clinically relevant multimodal data insights for mental healthcare. MIND presents multimodal insights through narrative text, complemented by charts communicating underlying data. Our user study (N=16) demonstrates that clinicians perceive MIND as a significant improvement over baseline methods, reporting improved performance to reveal hidden and clinically relevant data insights (p<.001) and support their decision-making (p=.004). Grounded in the study results, we discuss future research opportunities to integrate data narratives in broader clinical practices.",
      "author": "Ruishi Zou, Shiyu Xu, Margaret E Morris, Jihan Ryu, Timothy D. Becker, Nicholas Allen, Anne Marie Albano, Randy Auerbach, Dan Adler, Varun Mishra, Lace Padilla, Dakuo Wang, Ryan Sultan, Xuhai \"Orson\" Xu",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928916+00:00",
      "updated_at": "2026-01-22T05:30:26.928918+00:00"
    },
    {
      "id": "4a3f0de0627e8f33b9a4d320c444904f",
      "url": "https://arxiv.org/abs/2601.14639",
      "title": "DesignBridge: Bridging Designer Expertise and User Preferences through AI-Enhanced Co-Design for Fashion",
      "content": "arXiv:2601.14639v1 Announce Type: new \nAbstract: Effective collaboration between designers and users is important for fashion design, which can increase the user acceptance of fashion products and thereby create value. However, it remains an enduring challenge, as traditional designer-centric approaches restrict meaningful user participation, while user-driven methods demand design proficiency, often marginalizing professional creative judgment. Current co-design practices, including workshops and AI-assisted frameworks, struggle with low user engagement, inefficient preference collection, and difficulties in balancing user feedback with design considerations. To address these challenges, we conducted a formative study with designers and users experienced in co-design (N=7), identifying critical challenges for current collaboration between designers and users in the co-design process, and their requirements. Informed by these insights, we introduce DesignBridge, a multi-platform AI-enhanced interactive system that bridges designer expertise and user preferences through three stages: (1) Initial Design Framing, where designers define initial concepts. (2) Preference Expression Collection, where users intuitively articulate preferences via interactive tools. (3) Preference-Integrated Design, where designers use AI-assisted analytics to integrate feedback into cohesive designs. A user study demonstrates that DesignBridge significantly enhances user preference collection and analysis, enabling designers to integrate diverse preferences with professional expertise.",
      "author": "Yuheng Shao, Yuansong Xu, Yifan Jin, Shuhao Zhang, Wenxin Gu, Quan Li",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 192,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928886+00:00",
      "updated_at": "2026-01-22T05:30:26.928887+00:00"
    },
    {
      "id": "f007e278f0f625eb274c3c679822541b",
      "url": "https://arxiv.org/abs/2601.14611",
      "title": "Seeing to Think? How Source Transparency Design Shapes Interactive Information Seeking and Evaluation in Conversational AI",
      "content": "arXiv:2601.14611v1 Announce Type: new \nAbstract: Conversational AI systems increasingly function as primary interfaces for information seeking, yet how they present sources to support information evaluation remains under-explored. This paper investigates how source transparency design shapes interactive information seeking, trust, and critical engagement. We conducted a controlled between-subjects experiment (N=372) comparing four source presentation interfaces - Collapsible, Hover Card, Footer, and Aligned Sidebar - varying in visibility and accessibility. Using fine-grained behavioral analysis and automated critical thinking assessment, we found that interface design fundamentally alters exploration strategies and evidence integration. While the Hover Card interface facilitated seamless, on-demand verification during the task, the Aligned Sidebar uniquely mitigated the negative effects of information overload: as citation density increased, Sidebar users demonstrated significantly higher critical thinking and synthesis scores compared to other conditions. Our results highlight a trade-off between designs that support workflow fluency and those that enforce reflective verification, offering practical implications for designing adaptive and responsible conversational AI that fosters critical engagement with AI generated content.",
      "author": "Jiangen He, Jiqun Liu",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 165,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928851+00:00",
      "updated_at": "2026-01-22T05:30:26.928853+00:00"
    },
    {
      "id": "81e792f7cebcf07fca23773c31954407",
      "url": "https://arxiv.org/abs/2601.14589",
      "title": "Designing KRIYA: An AI Companion for Wellbeing Self-Reflection",
      "content": "arXiv:2601.14589v1 Announce Type: new \nAbstract: Most personal wellbeing apps present summative dashboards of health and physical activity metrics, yet many users struggle to translate this information into meaningful understanding. These apps commonly support engagement through goals, reminders, and structured targets, which can reinforce comparison, judgment, and performance anxiety. To explore a complementary approach that prioritizes self-reflection, we design KRIYA, an AI wellbeing companion that supports co-interpretive engagement with personal wellbeing data. KRIYA aims to collaborate with users to explore questions, explanations, and future scenarios through features such as Comfort Zone, Detective Mode, and What-If Planning. We conducted semi-structured interviews with 18 college students interacting with a KRIYA prototype using hypothetical data. Our findings show that through KRIYA interaction, users framed engaging with wellbeing data as interpretation rather than performance, experienced reflection as supportive or pressuring depending on emotional framing, and developed trust through transparency. We discuss design implications for AI companions that support curiosity, self-compassion, and reflective sensemaking of personal health data.",
      "author": "Shanshan Zhu, Wenxuan Song, Jiayue Melissa Shi, Dong Whi Yoo, Karthik S. Bhat, Koustuv Saha",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 162,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928806+00:00",
      "updated_at": "2026-01-22T05:30:26.928808+00:00"
    },
    {
      "id": "9b32107b41c6e05208a75cd19c7ff276",
      "url": "https://arxiv.org/abs/2601.14587",
      "title": "Explainable OOHRI: Communicating Robot Capabilities and Limitations as Augmented Reality Affordances",
      "content": "arXiv:2601.14587v1 Announce Type: new \nAbstract: Human interaction is essential for issuing personalized instructions and assisting robots when failure is likely. However, robots remain largely black boxes, offering users little insight into their evolving capabilities and limitations. To address this gap, we present explainable object-oriented HRI (X-OOHRI), an augmented reality (AR) interface that conveys robot action possibilities and constraints through visual signifiers, radial menus, color coding, and explanation tags. Our system encodes object properties and robot limits into object-oriented structures using a vision-language model, allowing explanation generation on the fly and direct manipulation of virtual twins spatially aligned within a simulated environment. We integrate the end-to-end pipeline with a physical robot and showcase diverse use cases ranging from low-level pick-and-place to high-level instructions. Finally, we evaluate X-OOHRI through a user study and find that participants effectively issue object-oriented commands, develop accurate mental models of robot limitations, and engage in mixed-initiative resolution.",
      "author": "Lauren W. Wang, Mohamed Kari, Parastoo Abtahi",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928774+00:00",
      "updated_at": "2026-01-22T05:30:26.928775+00:00"
    },
    {
      "id": "29b0ccc788906fa8395d25f9c453d048",
      "url": "https://arxiv.org/abs/2601.14566",
      "title": "SCSimulator: An Exploratory Visual Analytics Framework for Partner Selection in Supply Chains through LLM-driven Multi-Agent Simulation",
      "content": "arXiv:2601.14566v1 Announce Type: new \nAbstract: Supply chains (SCs), complex networks spanning from raw material acquisition to product delivery, with enterprises as interconnected nodes, play a pivotal role in organizational success. However, optimizing SCs remains challenging, particularly in partner selection, a key bottleneck shaped by competitive and cooperative dynamics. This challenge constitutes a multi-objective dynamic game requiring a synergistic integration of Multi-Criteria Decision-Making and Game Theory. Traditional approaches, grounded in mathematical simplifications and managerial heuristics, fail to capture real-world intricacies and risk introducing subjective biases. Multi-agent simulation offers promise, but prior research has largely relied on fixed, uniform agent logic, limiting practical applicability. Recent advances in LLMs create opportunities to represent complex SC requirements and hybrid game logic. However, challenges persist in modeling dynamic SC relationships, ensuring interpretability, and balancing agent autonomy with expert control. We present SCSimulator, a visual analytics framework that integrates LLM-driven MAS with human-in-the-loop collaboration for SC partner selection. It simulates SC evolution via adaptive network structures and enterprise behaviors, which are visualized via interpretable interfaces. By combining CoT reasoning with XAI techniques, it generates multi-faceted, transparent explanations of decision trade-offs. Users can iteratively adjust simulation settings to explore outcomes aligned with their expectations and strategic priorities. Developed through iterative co-design with SC experts and industry managers, SCSimulator serves as a proof-of-concept, offering methodological contributions and practical insights for future research on SC decision-making and interactive AI-driven analytics. Usage scenarios and a user study demonstrate the system's effectiveness and usability.",
      "author": "Shenghan Gao, Junye Wang, Junjie Xiong, Yun Jiang, Yun Fang, Qifan Hu, Baolong Liu, Quan Li",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 243,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928735+00:00",
      "updated_at": "2026-01-22T05:30:26.928736+00:00"
    },
    {
      "id": "0316fe6ab494a200ff4d262162cb9e4c",
      "url": "https://arxiv.org/abs/2601.14561",
      "title": "Evaluating Preattentive Features for Detecting Changes in Virtual Environments",
      "content": "arXiv:2601.14561v1 Announce Type: new \nAbstract: Visual perception plays a critical role in detecting changes within immersive Virtual Reality (VR) environments. However, as visual complexity increases, perceptual performance declines, making it more difficult to detect changes quickly and accurately. This study examines how visual features, known for facilitating preattentive processing, impact a change detection task in immersive 3D environments, with a focus on visual complexity, object attributes, and spatial proximity. Our results demonstrate that preattentive processing enhances change detection, particularly when the altered object is spatially isolated and not perceptually grouped with similar surrounding objects. Changes to isolated objects were detected more reliably, suggesting that perceptual isolation reduces cognitive load and draws more attention. Conversely, when a changed object was surrounded by visually similar elements, participants were less likely to detect the change, indicating that perceptual grouping hinders individual object recognition in complex scenes. These results provide guidelines for designing VR applications that strategically utilize spatial isolation and visual features to improve the user experience.",
      "author": "DongHoon Kim, Isaac Cho",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 164,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928690+00:00",
      "updated_at": "2026-01-22T05:30:26.928692+00:00"
    },
    {
      "id": "d7ae69e65249c0dfb9d9e9ff9a82d3ca",
      "url": "https://arxiv.org/abs/2601.14435",
      "title": "SPIRIT: A Design Framework To Support Technology Interventions for Spiritual Care Within and Beyond the Clinic",
      "content": "arXiv:2601.14435v1 Announce Type: new \nAbstract: Despite its importance for well-being, spiritual care remains under-explored in HCI, while the adoption of technology in clinical spiritual care lags behind other healthcare fields. Prior work derived a definition of \"spiritual support\" through co-design workshops with stakeholders in online health communities. This paper contributes: (1) a revision of that definition through member checking with professional spiritual care providers (SCPs); (2) a novel design framework -- SPIRIT -- which can help to expand models of delivery for spiritual care using digital technologies. Through re-analysis of previous data and new interviews with SCPs, we identify three prerequisites for meaningful spiritual care: openness to care, safe space, and the ability to discern and articulate spiritual needs. We also propose six design dimensions: loving presence, meaning-making, appropriate degree of technology use, location, degree of relational closeness, and temporality. We discuss how SPIRIT offers guidance for designing impactful digital spiritual care intervention systems within and beyond clinical settings.",
      "author": "C. Estelle Smith, Alemitu Bezabih, Shadi Nourriz, Jesan Ahammed Ovi",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928652+00:00",
      "updated_at": "2026-01-22T05:30:26.928654+00:00"
    },
    {
      "id": "d033ba328fe702823801d43d0a6b6132",
      "url": "https://arxiv.org/abs/2601.14423",
      "title": "Loss Aversion Online: Emotional Responses to Financial Booms and Crashes",
      "content": "arXiv:2601.14423v1 Announce Type: new \nAbstract: Financial events negatively affect emotional well-being, but large-scale studies examining their impact on online emotional expression using real-time social media data remain limited. To address this gap, we propose analyzing Reddit communities (financial and non-financial) across two case studies: a financial crash and a boom. We investigate how emotional and psycholinguistic responses differ between financial and non-financial communities, and the extent to which the type of financial event affects user behavior during the two case study periods. To examine the effect of these events on expressed language, we analyze daily sentiment, emotion, and LIWC counts using quasi-experimental methods: Difference-in-Differences (DiD) and Causal Impact analyses during a financial boom and a financial crash. Overall, we find coherent, negative shifts in emotional responses during financial crashes, but weaker, mixed responses during booms, consistent with loss aversion. By exploring emotional and psycholinguistic expressions during financial events, we identify future implications for understanding online users' mental health and building connected, healthy communities.",
      "author": "Aryan Ramchandra Kapadia, Niharika Bhattacharjee, Mung Yao Jia, Ishq Gupta, Dong Wang, Koustuv Saha",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 163,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928619+00:00",
      "updated_at": "2026-01-22T05:30:26.928621+00:00"
    },
    {
      "id": "28e82a9d13970ab85fff8b5aa37b2b8e",
      "url": "https://arxiv.org/abs/2601.14324",
      "title": "When Generative AI Is Intimate, Sexy, and Violent: Examining Not-Safe-For-Work (NSFW) Chatbots on FlowGPT",
      "content": "arXiv:2601.14324v1 Announce Type: new \nAbstract: User-created chatbots powered by generative AI offer new ways to share and interact with Not-Safe-For-Work (NSFW) content. However, little is known about the characteristics of these GenAI-based chatbots and their user interactions. Drawing on the functional theory of NSFW on social media, this study analyzes 376 NSFW chatbots and 307 public conversation sessions on FlowGPT. Findings identify four chatbot types: roleplay characters, story generators, image generators, and do-anything-now bots. AI Characters portraying fantasy personas and enabling hangout-style interactions are most common, often using explicit avatar images to invite engagement. Sexual, violent, and insulting content appears in both user prompts and chatbot outputs, with some chatbots generating explicit material even when users do not create erotic prompts. In sum, the NSFW experience on FlowGPT can be understood as a combination of virtual intimacy, sexual delusion, violent thought expression, and unsafe content acquisition. We conclude with implications for chatbot design, creator support, user safety, and content moderation.",
      "author": "Xian Li, Yuanning Han, Di Liu, Pengcheng An, Shuo Niu",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 160,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928577+00:00",
      "updated_at": "2026-01-22T05:30:26.928583+00:00"
    },
    {
      "id": "52a03d63a84a1c913071e1c67126117a",
      "url": "https://arxiv.org/abs/2510.24709",
      "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?",
      "content": "arXiv:2510.24709v2 Announce Type: replace-cross \nAbstract: Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a quadratic similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in DINO, CLIP, and ImageNet-supervised ViTs, but is markedly weaker in MAE, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of \"which parts belong together\" emerges naturally in a connectionist system.",
      "author": "Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 255,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.852120+00:00",
      "updated_at": "2026-01-22T05:30:25.852122+00:00"
    },
    {
      "id": "54b09707d6bca606acbbb662dcdb5cb5",
      "url": "https://arxiv.org/abs/2601.10397",
      "title": "Reshaping Neural Representation via Associative, Presynaptic Short-Term Plasticity",
      "content": "arXiv:2601.10397v2 Announce Type: replace \nAbstract: Short-term synaptic plasticity (STP) is often regarded as a presynaptic filter of spikes, independent of postsynaptic activity. Recent experiments, however, indicate an associative STP that depends on pre- and postsynaptic coactivation. We develop a normative, information-theoretic theory of associative STP. Extending Fisher-information-based learning to Tsodyks-Markram synapses, we derive learning rules for baseline weight and release probability that maximize stimulus information under resource constraints. The rules split into a postsynaptic term tracking local firing and a presynaptic, phase-advanced term that selectively detects stimulus onset. For slowly varying inputs, this onset sensitivity favors anti-causal connectivity and enhances response offset during drive and reverse replay after drive removal in recurrent circuits. Linear-response analysis shows that STP yields frequency-dependent phase selectivity and that release-probability constraints tune temporal asymmetry. These results identify release-probability plasticity as a principled substrate for rapidly reconfigurable temporal coding.",
      "author": "Genki Shimizu, Taro Toyoizumi",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 143,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.852003+00:00",
      "updated_at": "2026-01-22T05:30:25.852005+00:00"
    },
    {
      "id": "430d8f5d80c14c8f42645426060f7226",
      "url": "https://arxiv.org/abs/2601.10221",
      "title": "A Unified Dynamical Field Theory of Learning, Inference, and Emergence",
      "content": "arXiv:2601.10221v2 Announce Type: replace \nAbstract: Learning, inference, and emergence in biological and artificial systems are often studied within disparate theoretical frameworks, ranging from energy-based models to recurrent and attention-based architectures. Here we develop a unified dynamical field theory in which learning and inference are governed by a minimal stochastic dynamical equation admitting a Martin--Siggia--Rose--Janssen--de Dominicis formulation. Within this framework, inference corresponds to saddle-point trajectories of the associated action, while fluctuation-induced loop corrections render collective modes dynamically emergent and generate nontrivial dynamical time scales. A central result of this work is that cognitive function is controlled not by microscopic units or precise activity patterns, but by the collective organization of dynamical time scales. We introduce the \\emph{time-scale density of states} (TDOS) as a compact diagnostic of the distribution of collective relaxation modes governing inference dynamics. Learning and homeostatic regulation are naturally interpreted as processes that reshape both the effective potential and the underlying state-space geometry, thereby reorganizing the TDOS and selectively stabilizing slow collective modes that support stable inference, memory, and context-dependent computation despite stochasticity and structural irregularity. This framework unifies energy-based models, recurrent neural networks, transformer architectures, and biologically motivated homeostatic dynamics within a single physical description, and provides a principled route toward understanding cognition as an emergent dynamical phenomenon.",
      "author": "Byung Gyu Chae",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 210,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.851974+00:00",
      "updated_at": "2026-01-22T05:30:25.851976+00:00"
    },
    {
      "id": "8f851f6a4084b24846f850f03eff2050",
      "url": "https://arxiv.org/abs/2601.15091",
      "title": "Circadian Modulation of Semantic Exploration in Social Media Language",
      "content": "arXiv:2601.15091v1 Announce Type: cross \nAbstract: Human cognition exhibits strong circadian modulation, yet its influence on high-dimensional semantic behavior remains poorly understood. Using large-scale Reddit data, we quantify time-of-day variation in language use by embedding text into a pretrained transformer model and measuring semantic entropy as an index of linguistic exploration-exploitation, for which we show a robust circadian rhythmicity that could be entrained by seasonal light cues. Distinguishing between local and global semantic entropy reveals a systematic temporal dissociation: local semantic exploration peaks in the morning, reflecting broader exploration of semantic space, whereas global semantic diversity peaks later in the day as submissions accumulate around already established topics, consistent with \"rich-get-richer\" dynamics. These patterns are not explained by sentiment or affective valence, indicating that semantic exploration captures a cognitive dimension distinct from mood. The observed temporal structure aligns with known diurnal patterns in neuromodulatory systems, suggesting that biological circadian rhythms extend to the semantic domain.",
      "author": "Vuong Hung Truong, Mariana Gabrielle Cangco Reyes, Masatoshi Koizumi, Jihwan Myung",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.851940+00:00",
      "updated_at": "2026-01-22T05:30:25.851941+00:00"
    },
    {
      "id": "e482332494be4be7e128f9325b787e61",
      "url": "https://arxiv.org/abs/2601.14514",
      "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning",
      "content": "arXiv:2601.14514v1 Announce Type: cross \nAbstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.",
      "author": "Tony Chen, Sam Cheyette, Kelsey Allen, Joshua Tenenbaum, Kevin Smith",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 180,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.851910+00:00",
      "updated_at": "2026-01-22T05:30:25.851912+00:00"
    },
    {
      "id": "6ea1cdbfdf5ba2c6019dfd6ffd42bb1e",
      "url": "https://arxiv.org/abs/2601.15032",
      "title": "Single-Node Wilson--Cowan Model Accounts for Speech-Evoked $\\gamma$-Band Deficits in Schizophrenia",
      "content": "arXiv:2601.15032v1 Announce Type: new \nAbstract: Cortical gamma ($\\gamma$)-band activity reflects local excitation-inhibition (E/I) balance. In schizophrenia (SCZ), reduced task-evoked gamma suggests altered E/I dynamics, but it is unclear whether differences stem from input properties or systematic shifts in E/I operating point and gain. We coupled a cochlear-inspired speech front end to a Wilson-Cowan E/I model to simulate gamma responses across three conditions: Healthy, SCZ-speech, and SCZ-semantics. Metrics included event-related spectral perturbation (ERSP$_\\gamma$) and threshold-time fraction ($\\gamma%$). A stable hierarchy emerged: Healthy(speech/semantics) $>$ SCZ(speech) $>$ SCZ(semantics), robust under equal-energy control and gain perturbations. Network dynamics coincided with single-node solutions, supporting interpretability. Pharmacological analogs showed bidirectional effects: reduced inhibition lowered $\\gamma$, while reduced excitation increased $\\gamma$, with no self-sustained oscillations. Findings indicate SCZ gamma deficits align more with shifts in E/I operating point and gain than input differences. This pipeline provides a testable, reusable mechanistic framework for speech-evoked gamma and a baseline for cross-population studies.",
      "author": "Zhengdi Zhang, Yan Xu, Wenjun Xia",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.851877+00:00",
      "updated_at": "2026-01-22T05:30:25.851879+00:00"
    },
    {
      "id": "032fce0d474bc4ecb05ed2c23ed2ac98",
      "url": "https://arxiv.org/abs/2601.14961",
      "title": "Power-Law Scaling in the Classification Performance of Small-Scale Spiking Neural Networks",
      "content": "arXiv:2601.14961v1 Announce Type: new \nAbstract: This paper investigates the classification capability of small-scale spiking neural networks based on the Leaky Integrate-and-Fire (LIF) neuron model. We analyze the relationship between classification accuracy and three factors: the number of neurons, the number of stimulus nodes, and the number of classification categories. Notably, we employ a large language model (LLM) to assist in discovering the underlying functional relationships among these variables, and compare its performance against traditional methods such as linear and polynomial fitting. Experimental results show that classification accuracy follows a power-law scaling primarily with the number of categories, while the effects of neuron count and stimulus nodes are relatively minor. A key advantage of the LLM-based approach is its ability to propose plausible functional forms beyond pre-defined equation templates, often leading to more concise or accurate mathematical descriptions of the observed scaling laws. This finding has important implications for understanding efficient computation in biological neural systems and for pioneering new paradigms in AI-aided scientific discovery.",
      "author": "Zhengdi Zhang, Cong Han, Wenjun Xia",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 164,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.851829+00:00",
      "updated_at": "2026-01-22T05:30:25.851831+00:00"
    },
    {
      "id": "2325a72f08fefcb262efd31909a63230",
      "url": "https://arxiv.org/abs/2601.14313",
      "title": "Investigating cerebral anomalies in preterm infants and associated risk factors with magnetic resonance imaging at term-equivalent age",
      "content": "arXiv:2601.14313v1 Announce Type: new \nAbstract: Background: Being born very or extreme preterm is a major source of cerebral anomalies and neurodevelopmental disorders, whose occurrence depends on many perinatal factors. A better understanding of these factors could be provided by cerebral Magnetic Resonance Imaging (MRI) at term-equivalent age (TEA). Objective: To investigate, through cerebral TEA-MRIs, the relationship between the main perinatal factors, the occurrence of cerebral anomalies, and cerebral volumetry. Methods: We assembled a cohort of preterm babies born before 32 weeks of gestation who underwent a cerebral TEA-MRI. We assessed cerebral anomalies using a radiological scoring system -- the Kidokoro scoring -- and performed cerebral volumetry. We investigated the relationships between 9 clinical factors (birth characteristics, resuscitation treatments{\\ldots}), Kidokoro scores, and brain volumes. Results: Among 110 preterms who underwent a cerebral MRI at TEA, only 6% suffered moderate-to-severe brain anomalies. We identified mechanical ventilation as a risk factor for cerebral anomalies (adjusted Odds-Ratio aOR = 4.6, 95% Confidence Interval CI [1.7-12.8]) and prolonged parenteral nutrition as a protective factor for white matter anomalies (aOR = 0.2, 95%CI [0.1-0.8]). Mechanical ventilation (p = 0.01) and being born small for gestational age (p < 0.001) were risk factors for the reduction of cerebral volumes. An increase in brain lesion severity was associated with decreased cerebral volumes (p = 0.016). Conclusion: Our study highlights the importance of treatment-related perinatal factors on the occurrence of cerebral anomalies in very and extreme preterms, and the interest in using both qualitative (Kidokoro scoring) and quantitative (volumetry) MRI-tools.",
      "author": "Nicolas Elbaz (UNIACT, NeuroDiderot), Val\\'erie Biran (UNIACT, NeuroDiderot), Chlo\\'e Ghozland (UNIACT, NeuroDiderot), Laurie Devisscher (UNIACT, NeuroDiderot), Aline Gonzalez Carpinteiro (UNIACT, NeuroDiderot), Aur\\'elie Bourmaud (UNIACT, NeuroDiderot), Monique Elmaleh-Berg\\`es (UNIACT, NeuroDiderot), Lucie Hertz-Pannier (UNIACT, NeuroDiderot), Yann Leprince (UNIACT, NeuroDiderot), Alice Fr\\'erot (CEA, INSERM), Alice H\\'eneau (CEA, INSERM), Jessica Dubois (CEA, INSERM), Marianne Alison",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 251,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:25.851791+00:00",
      "updated_at": "2026-01-22T05:30:25.851795+00:00"
    },
    {
      "id": "aa486ace6ac1a8516dcc6c075fb945f5",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.18.700184v1?rss=1",
      "title": "Visual Context Influences Lateral Balance While Walking on Winding Paths",
      "content": "Effectively navigating daily environments necessitates achieving adaptability while maintaining stability. This requires integrating sensory feedback, primarily from the visual system, to maintain balance and maneuverability as we walk. This study examined the impact of visual information salience on lateral balance and stepping. Twenty-eight healthy adults (16F/12M; 26.16{+/-}4.23 years) participated. Participants walked along each of two virtual paths (Straight vs. Winding), having each of two path color contrasts (High vs. Low), in each of two environments with differing visual richness (Rich vs. Sparse). We quantified stepping errors as the percentage of steps landing outside designated path boundaries. We computed means () and standard deviations ({sigma}) of the minimum mediolateral margins of stability (MoSL), and we computed lateral Probability of Instability (PoIL) to assess participants' risk of taking unstable (MoSL < 0) steps. On Straight paths, participants made more stepping errors on Low (vs. High) contrast paths for both environments, and exhibited decreased{sigma} (MoSL) in Sparse (vs. Rich) environments on paths of both visual contrasts. On Winding paths, participants made the most stepping errors on Low Contrast paths in Sparse environments. They walked with smaller (MoSL) and exhibited higher PoIL on Low Contrast paths in both environments, and they exhibited higher PoIL in Sparse environments on paths of both visual contrasts. The effects of diminished visual information were far more pronounced on Winding paths (vs. Straight), hindering performance and balance maintenance, as these conditions challenged both mechanical and sensory mechanisms underlying balance control.",
      "author": "Render, A. C., Singh, T., Cusumano, J. P., Dingwell, J. B.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 241,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:16.824675+00:00",
      "updated_at": "2026-01-22T05:30:16.824677+00:00"
    },
    {
      "id": "ebd2d149946b042df6a39a9978dfcc2e",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.18.700191v1?rss=1",
      "title": "Computational neural dynamics of goal-directed visual attention in macaques",
      "content": "Visual search requires flexible coordination of attention, memory, and category representations across distributed cortical networks. Using large-scale recordings from V4, IT, OFC, and LPFC, we identified distinct neural populations selective for attention and category. Population dynamics robustly represented visual categories during cue presentation, sustained cue information across delays, and differentiated categories and attentional states during search. Cue-related activity predicted subsequent search efficiency, linking pre-search processing to behavioral performance. The orthogonal subspace provided a crucial latent representational structure for encoding and maintaining task-relevant information across search stages. Foveal attention enhanced peripheral representations by both increasing pattern separation and reshaping representational geometry in a non-linear, context-dependent manner. Search dynamics further reflected fixation history and target detection, which modulated both response strength and representational structure. Finally, V4 and IT encoded the spatial geometry of the search array, preserving its layout. Together, these findings highlight population-level dynamics as critical mechanisms supporting goal-directed visual search.",
      "author": "Zhang, J., Rungratsameetaweemana, N., Wang, S.",
      "published_date": "2026-01-21T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:16.824611+00:00",
      "updated_at": "2026-01-22T05:30:16.824617+00:00"
    }
  ]
}