{
  "last_updated": "2026-01-27T05:29:22.552243+00:00",
  "count": 20,
  "articles": [
    {
      "id": "1a63aec824aead827de5c329bbc83d2a",
      "url": "https://arxiv.org/abs/2601.17373",
      "title": "\"Privacy across the boundary\": Examining Perceived Privacy Risk Across Data Transmission and Sharing Ranges of Smart Home Personal Assistants",
      "content": "arXiv:2601.17373v1 Announce Type: new \nAbstract: As Smart Home Personal Assistants (SPAs) evolve into social agents, understanding user privacy necessitates interpersonal communication frameworks, such as Privacy Boundary Theory (PBT). To ground our investigation, our three-phase preliminary study (1) identified transmission and sharing ranges as key boundary-related risk factors, (2) categorized relevant SPA functions and data types, and (3) analyzed commercial practices, revealing widespread data sharing and non-transparent safeguards. A subsequent mixed-methods study (N=412 survey, N=40 interviews among the survey participants) assessed users' perceived privacy risks across data types, transmission ranges and sharing ranges. Results demonstrate a significant, non-linear escalation in perceived risk when data crosses two critical boundaries: the `public network' (transmission) and `third parties' (sharing). This boundary effect holds robustly across data types and demographics. Furthermore, risk perception is modulated by data attributes (e.g., social relational data), and contextual privacy calculus. Conversely, anonymization safeguards show limited efficacy especially for third-party sharing, a finding attributed to user distrust. These findings empirically ground PBT in the SPA context and inform design of boundary-aware privacy protection.",
      "author": "Shuning Zhang, Shixuan Li, Haobin Xing, Jiarui Liu, Yan Kong, Xin Yi, Hewu Li",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 173,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466676+00:00",
      "updated_at": "2026-01-27T05:29:04.466678+00:00"
    },
    {
      "id": "ea47d1eea69abb1ed4b47c6ca22f44c2",
      "url": "https://arxiv.org/abs/2601.17371",
      "title": "Collab: Fostering Critical Identification of Deepfake Videos on Social Media via Synergistic Annotation",
      "content": "arXiv:2601.17371v1 Announce Type: new \nAbstract: Identifying deepfake videos on social media platforms is challenged by dynamic spatio-temporal artifacts and inadequate user tools. This hinders both critical viewing by users and scalable moderation on platforms. Here, we present Collab, a web plugin enabling users to collaboratively annotate deepfake videos. Collab integrates three key components: (i) an intuitive interface for spatio-temporal labeling where users provide confidence scores and rationales, facilitating detailed input even from non-experts, (ii) a novel confidence-weighted spatio-temporal Intersection-over-Union (IoU) algorithm to aggregate diverse user annotations into accurate aggregations, and (iii) a hierarchical demonstration strategy presenting aggregated results to guide attention toward contentious regions and foster critical evaluation. A seven-day online study (N=90), where participants annotated suspicious videos when viewing an online experimental platforms, compared Collab against two conditions without aggregation or demonstration respectively. Collab significantly improved identification accuracy and enhanced reflection compared to non-demonstration condition, while outperforming non-aggregation condition for its novelty and effectiveness.",
      "author": "Shuning Zhang, Linzhi Wang, Shixuan Li, Yuanyuan Wu, Yuwei Chuai, Luoxi Chen, Xin Yi, Hewu Li",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466646+00:00",
      "updated_at": "2026-01-27T05:29:04.466648+00:00"
    },
    {
      "id": "e813512963658d2de1de21b06d6463ed",
      "url": "https://arxiv.org/abs/2601.17368",
      "title": "A Scoping Review and Guidelines on Privacy Policy's Visualization from an HCI Perspective",
      "content": "arXiv:2601.17368v1 Announce Type: new \nAbstract: Privacy Policies are a cornerstone of informed consent, yet a persistent gap exists between their legal intent and practical efficacy. Despite decades of Human-Computer Interaction (HCI) research proposing various visualizations, user comprehension remains low, and designs rarely see widespread adoption. To understand this landscape and chart a path forward, we synthesized 65 top-tier papers using a framework adapted from the user-centered design lifecycle. Our analysis presented findings of the field's evolution across four dimensions: (1) the trade-off between information load and decision efficacy, which demonstrates a shift from augmenting disclosures to prioritizing information condensation and cognitive load management to counter the inefficacy of comprehensive texts, (2) the co-evolutionary dynamic of design and automation, revealing that complex design ambitions such as context-awareness drove the need for advanced NLP, while recent LLM breakthroughs are enabling the semantic interpretation required to realize those designs, (3) the tension between generality and specificity, highlighting the divergence between standardized, cross-platform solutions and the increasing necessity for specialized, context-aware interaction patterns in IoT and immersive environments, and (4) balancing stakeholder opinions, which shows that visualization efficacy is constrained by the complex interplay of regulatory mandates, developer capabilities and provider incentives. We conclude by outlining four critical challenges for future research.",
      "author": "Shuning Zhang, Eve He, Sixing Tao, Yuting Yang, Ying Ma, Ailei Wang, Xin Yi, Hewu Li",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 208,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466615+00:00",
      "updated_at": "2026-01-27T05:29:04.466617+00:00"
    },
    {
      "id": "e9079e73d3232df39e1a1515b64411b9",
      "url": "https://arxiv.org/abs/2601.17351",
      "title": "AI-RP: The AI Relationship Process Framework",
      "content": "arXiv:2601.17351v1 Announce Type: new \nAbstract: For a growing number of people, AI chatbots have become close personal companions. Despite rising scholarly attention, theoretical accounts of how such relationships develop remain fragmented. Existing frameworks address important aspects of the phenomenon, but they rarely treat human-chatbot communication as the central behavior that builds relationships. To address this gap, we propose the AI relationship process (AI-RP) framework. The AI-RP outlines relationship formation as a sequential process. (a) Chatbot characteristics shape users' (b) social perceptions. These perceptions guide (c) communication, and communication produces (d) relational outcomes such as attachment and companionship. The AI-RP introduces a six-features profile characterizing chatbots, a dual-route approach of social perception, a behavioral conceptualization of communication and discusses the foundation and types of artificial relationships. By foregrounding observable communicative behavior, the AI-RP provides a foundation for theory building and empirical research on the social and ethical implications of AI companionship.",
      "author": "Nadja Rupprechter, Tobias Dienlin, Tilo Hartmann",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466581+00:00",
      "updated_at": "2026-01-27T05:29:04.466583+00:00"
    },
    {
      "id": "62500f3f412616773a3bee19d1d814ca",
      "url": "https://arxiv.org/abs/2601.17240",
      "title": "Exploring Needs and Design Opportunities for Proactive Information Support in In-Person Small-Group Conversations",
      "content": "arXiv:2601.17240v1 Announce Type: new \nAbstract: In-person small-group conversations play a crucial role in everyday life; however, facilitating effective group interaction can be challenging, as the real-time nature demands full attention, offers no opportunity for revision, and requires interpreting non-verbal cues. Using Mixed Reality to provide proactive information support shows promise in helping individuals engage in and contribute to group conversations. We present a preliminary participatory design and qualitative study (N = 10) using focus groups and two technology probes to explore the opportunities of designing proactive information support in in-person small-group conversations. We reveal key design opportunities concerning how to maximize the benefits of proactive information support and how to effectively design such supporting information. Our study is crucial for paving the way toward designing future proactive AI agents to enable the paradigm of augmented in-person small-group conversation experience.",
      "author": "Shaoze Zhou, Diana Nelly Rivera Rodriguez, Pedro Remior, Joaquin Frangi, Lingyao Li, Renkai Ma, Janet G. Johnson, Christine Lisetti, Chen Chen",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 139,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466552+00:00",
      "updated_at": "2026-01-27T05:29:04.466554+00:00"
    },
    {
      "id": "7346a7db91cabdead7419819d3649ba1",
      "url": "https://arxiv.org/abs/2601.17238",
      "title": "Studying Mobile Spatial Collaboration across Video Calls and Augmented Reality",
      "content": "arXiv:2601.17238v1 Announce Type: new \nAbstract: Mobile video calls are widely used to share information about real-world objects and environments with remote collaborators. While these calls provide valuable visual context in real time, the experience of interacting with people and moving around a space is significantly reduced when compared to co-located conversations. Recent work has demonstrated the potential of Mobile Augmented Reality applications to enable more spatial forms of collaboration across distance. To better understand the dynamics of mobile AR collaboration and how this medium compares against the status quo, we conducted a comparative structured observation study to analyze people's perception of space and interaction with remote collaborators across mobile video calls and AR-based calls. Fourteen pairs of participants completed a spatial collaboration task using each medium. Through a mixed-methods analysis of session videos, transcripts, motion logs, post-task exercises, and interviews, we highlight how the choice of medium influences the roles and responsibilities that collaborators take on and the construction of a shared language for coordination. We discuss the importance of spatial reasoning with one's body, how video calls help participants \"be on the same page\" more directly, and how AR calls enable both onsite and remote collaborators to engage with the space and each other in ways that resemble in-person interaction. Our study offers a nuanced view of the benefits and limitations of both mediums, and we conclude with a discussion of design implications for future systems that integrate mobile video and AR to better support spatial collaboration in its many forms.",
      "author": "Rishi Vanukuru, Krithik Ranjan, Ada Yi Zhao, David Lindero, Gunilla H. Berndtsson, Gregoire Phillips, Amy Bani\\'c, Mark D. Gross, Ellen Yi-Luen Do",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 252,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466523+00:00",
      "updated_at": "2026-01-27T05:29:04.466525+00:00"
    },
    {
      "id": "b52d4bc4a4e55a12e925b390868e835b",
      "url": "https://arxiv.org/abs/2601.17149",
      "title": "Exploring EEG-driven brain-heart coupling across sleep stages in individuals with sleep disorders",
      "content": "arXiv:2601.17149v1 Announce Type: new \nAbstract: The interactions between the brain and heart during sleep are responsible for regulating autonomic function. While brain-heart coupling has been studied in healthy populations, the relationships between neural and cardiac activity across sleep stages in the presence of sleep disorders are not clear. This study examines the influence of brain-driven cardiac activity across sleep stages for individuals with sleep disorders. Overnight recordings of C3 and C4 electroencephalogram (EEG) channels and electrocardiogram (ECG) signals from 146 individuals were preprocessed and analyzed in the frequency domain through a linear mixed-effect model. Our results show that parasympathetic activity is sensitive to changes in delta and beta powers during later stages of non-rapid eye movement (NREM) sleep, as both band powers exhibited strong negative effects on high-frequency heart rate variability (HF-HRV) power. These findings show that neural activity can drive vagal tone across sleep stages, suggesting that treatments on key EEG bands during NREM and REM stages may help restore regular cardiac behaviour.",
      "author": "Jathushan Kaetheeswaran, Jenny Wei",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 164,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466478+00:00",
      "updated_at": "2026-01-27T05:29:04.466479+00:00"
    },
    {
      "id": "489c32a79f9ae3e36f73904f5ea0e722",
      "url": "https://arxiv.org/abs/2601.17134",
      "title": "Deconstructing Taste: Toward a Human-Centered AI Framework for Modeling Consumer Aesthetic Perceptions",
      "content": "arXiv:2601.17134v1 Announce Type: new \nAbstract: Understanding and modeling consumers' stylistic taste such as \"sporty\" is crucial for creating designs that truly connect with target audiences. However, capturing taste during the design process remains challenging because taste is abstract and subjective, and preference data alone provides limited guidance for concrete design decisions. This paper proposes an integrated human-centered computational framework that links subjective evaluations (e.g., perceived luxury of car wheels) with domain-specific features (e.g., spoke configuration) and computer vision-based measures (e.g., texture). By jointly modeling human-derived (consumer and designer) and machine-extracted features, our framework advances aesthetic assessment by explicitly linking model outcomes to interpretable design features. In particular, it demonstrates how perceptual features, domain-specific design patterns, and consumers' own interpretations of style contribute to aesthetic evaluations. This framework will enable product teams to better understand, communicate, and critique aesthetic decisions, supporting improved anticipation of consumer taste and more informed exploration of design alternatives at design time.",
      "author": "Matthew K. Hong, Joey Li, Alexandre Filipowicz, Monica Van, Kalani Murakami, Yan-Ying Chen, Shiwali Mohan, Shabnam Hakimi, Matthew Klenk",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466446+00:00",
      "updated_at": "2026-01-27T05:29:04.466448+00:00"
    },
    {
      "id": "cedfd1f062d6d441a6653371447a21db",
      "url": "https://arxiv.org/abs/2601.17123",
      "title": "Acoustic Field Video for Multimodal Scene Understanding",
      "content": "arXiv:2601.17123v1 Announce Type: new \nAbstract: We introduce and explore a new multimodal input representation for vision-language models: acoustic field video. Unlike conventional video (RGB with stereo/mono audio), our video stream provides a spatially grounded visualization of sound intensity across a scene, offering a new and powerful dimension of perceptual understanding. Our real-time pipeline uses low-cost beamforming microphone arrays that are already common in smart speakers and increasingly present in robotics and XR headsets, yet this sensing capability remains unutilized for scene understanding. To assess the value of spatial acoustic information, we constructed an evaluation set of 402 question-answer scenes, comparing a state-of-the-art VLM given conventional video with and without paired acoustic field video. Results show a clear and consistent improvement when incorporating spatial acoustic data; the VLM we test improves from 38.3% correct to 67.4%. Our findings highlight that many everyday scene understanding tasks remain underconstrained when relying solely on visual and audio input, and that acoustic field data provides a promising and practical direction for multimodal reasoning. A video demo is available at https://daehwakim.com/seeingsound",
      "author": "Daehwa Kim, Chris Harrison",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 175,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466414+00:00",
      "updated_at": "2026-01-27T05:29:04.466416+00:00"
    },
    {
      "id": "b207d1fcbfdbee475eac47fa37c1e046",
      "url": "https://arxiv.org/abs/2601.17087",
      "title": "Lost in Simulation: LLM-Simulated Users are Unreliable Proxies for Human Users in Agentic Evaluations",
      "content": "arXiv:2601.17087v1 Announce Type: new \nAbstract: Agentic benchmarks increasingly rely on LLM-simulated users to scalably evaluate agent performance, yet the robustness, validity, and fairness of this approach remain unexamined. Through a user study with participants across the United States, India, Kenya, and Nigeria, we investigate whether LLM-simulated users serve as reliable proxies for real human users in evaluating agents on {\\tau}-Bench retail tasks. We find that user simulation lacks robustness, with agent success rates varying up to 9 percentage points across different user LLMs. Furthermore, evaluations using simulated users exhibit systematic miscalibration, underestimating agent performance on challenging tasks and overestimating it on moderately difficult ones. African American Vernacular English (AAVE) speakers experience consistently worse success rates and calibration errors than Standard American English (SAE) speakers, with disparities compounding significantly with age. We also find simulated users to be a differentially effective proxy for different populations, performing worst for AAVE and Indian English speakers. Additionally, simulated users introduce conversational artifacts and surface different failure patterns than human users. These findings demonstrate that current evaluation practices risk misrepresenting agent capabilities across diverse user populations and may obscure real-world deployment challenges.",
      "author": "Preethi Seshadri, Samuel Cahyawijaya, Ayomide Odumakinde, Sameer Singh, Seraphina Goldfarb-Tarrant",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 187,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:04.466376+00:00",
      "updated_at": "2026-01-27T05:29:04.466380+00:00"
    },
    {
      "id": "12be74aafcc94954afe8b0f949d77676",
      "url": "https://arxiv.org/abs/2510.22860",
      "title": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement",
      "content": "arXiv:2510.22860v3 Announce Type: replace-cross \nAbstract: Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly \"entangled,\" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing.",
      "author": "Linyang He, Tianjun Zhong, Richard Antonello, Gavin Mischler, Micah Goldblum, Nima Mesgarani",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.357224+00:00",
      "updated_at": "2026-01-27T05:29:03.357225+00:00"
    },
    {
      "id": "2c43eb32fdce7278f4dd2fa6b40ceb70",
      "url": "https://arxiv.org/abs/2510.19764",
      "title": "A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks",
      "content": "arXiv:2510.19764v2 Announce Type: replace-cross \nAbstract: The majority of research in both training Artificial Neural Networks (ANNs) and modeling learning in biological brains focuses on synaptic plasticity, where learning equates to changing the strength of existing connections. However, in biological brains, structural plasticity - where new connections are created and others removed - is also vital, not only for effective learning but also for recovery from damage and optimal resource usage. Inspired by structural plasticity, pruning is often used in machine learning to remove weak connections from trained models to reduce the computational requirements of inference. However, the machine learning frameworks typically used for backpropagation-based training of both ANNs and Spiking Neural Networks (SNNs) are optimized for dense connectivity, meaning that pruning does not help reduce the training costs of ever-larger models. The GeNN simulator already supports efficient GPU-accelerated simulation of sparse SNNs for computational neuroscience and machine learning. Here, we present a new flexible framework for implementing GPU-accelerated structural plasticity rules and demonstrate this first using the e-prop supervised learning rule and DEEP R to train efficient, sparse SNN classifiers and then, in an unsupervised learning context, to learn topographic maps. Compared to baseline dense models, our sparse classifiers reduce training time by up to 10x while the DEEP R rewiring enables them to perform as well as the original models. We demonstrate topographic map formation in faster-than-realtime simulations, provide insights into the connectivity evolution, and measure simulation speed versus network size. The proposed framework will enable further research into achieving and maintaining sparsity in network structure and neural communication, as well as exploring the computational benefits of sparsity in a range of neuromorphic applications.",
      "author": "James C. Knight, Johanna Senk, Thomas Nowotny",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 275,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.357189+00:00",
      "updated_at": "2026-01-27T05:29:03.357191+00:00"
    },
    {
      "id": "7e3bf6874b268cf51708f237924d1c83",
      "url": "https://arxiv.org/abs/2505.14125",
      "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning",
      "content": "arXiv:2505.14125v3 Announce Type: replace-cross \nAbstract: Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.",
      "author": "Viet Anh Khoa Tran, Emre Neftci, Willem A. M. Wybo",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.357153+00:00",
      "updated_at": "2026-01-27T05:29:03.357154+00:00"
    },
    {
      "id": "2be45b0047c680e7fb2e2399c53bcedb",
      "url": "https://arxiv.org/abs/2601.10912",
      "title": "Graph Neural Network Reveals the Local Cortical Morphology of Brain Aging in Normal Cognition and Alzheimers Disease",
      "content": "arXiv:2601.10912v4 Announce Type: replace \nAbstract: Estimating brain age (BA) from T1-weighted magnetic resonance images (MRIs) provides a useful approach to map the anatomic features of brain senescence. Whereas global BA (GBA) summarizes overall brain health, local BA (LBA) can reveal spatially localized patterns of aging. Although previous studies have examined anatomical contributors to GBA, no framework has been established to compute LBA using cortical morphology. To address this gap, we introduce a novel graph neural network (GNN) that uses morphometric features (cortical thickness, curvature, surface area, gray/white matter intensity ratio and sulcal depth) to estimate LBA across the cortical surface at high spatial resolution (mean inter-vertex distance = 1.37 mm). Trained on cortical surface meshes extracted from the MRIs of cognitively normal adults (N = 14,250), our GNN identifies prefrontal and parietal association cortices as early sites of morphometric aging, in concordance with biological theories of brain aging. Feature comparison using integrated gradients reveals that morphological aging is driven primarily by changes in surface area (gyral crowns and highly folded regions) and cortical thickness (occipital lobes), with additional contributions from gray/white matter intensity ratio (frontal lobes and sulcal troughs) and curvature (sulcal troughs). In Alzheimers disease (AD), as expected, the model identifies widespread, excessive morphological aging in parahippocampal gyri and related temporal structures. Significant associations are found between regional LBA gaps and neuropsychological measures descriptive of AD-related cognitive impairment, suggesting an intimate relationship between morphological cortical aging and cognitive decline. These results highlight the ability of GNN-derived gero-morphometry to provide insights into local brain aging.",
      "author": "Samuel D. Anderson, Nikhil N. Chaudhari, Nahian F. Chowdhury, Jordan Jomsky, Xiaoyu Rayne Zheng, Andrei Irimia, Alzheimers Disease Neuroimaging Initiative",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 255,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.357120+00:00",
      "updated_at": "2026-01-27T05:29:03.357122+00:00"
    },
    {
      "id": "ced5396dd74b43fcf3ea8fea5a42681f",
      "url": "https://arxiv.org/abs/2509.13481",
      "title": "Complex-valued Phase Synchrony Reveals Directional Coupling in FMRI and Tracks Medication Effects",
      "content": "arXiv:2509.13481v2 Announce Type: replace \nAbstract: Understanding interactions in complex systems requires capturing the relative timing of coupling, not only its strength. Phase synchronization captures this timing, yet most methods either reduce the phase to its cosine or collapse it into scalar indices such as the phase-locking value, discarding relative timing. We propose a complex-valued phase synchrony (CVPS) framework that estimates phase with an adaptive Gabor wavelet and preserves both cosine and sine components. Simulations confirm that CVPS recovers true phase offsets and tracks non-stationary dynamics more faithfully than Hilbert-based methods. Because antipsychotics are known to modulate the timing of cortical interactions, they provide a rigorous context to evaluate whether CVPS can capture such pharmacological effects. CVPS further reveals cortical neuro-hemodynamic drivers, with occipital-to-parietal and prefrontal-to-striatal lead--lag flows consistent with known receptor targets, confirming its ability to capture pharmacological timing. CVPS, therefore, offers a robust, generalizable framework for detecting relative timing in complex systems such as the brain.",
      "author": "Sir-Lord Wiafe, Najme Soleimani, Masoud Seraji, Bradley Baker, Robyn Miller, Ashkan Faghiri, Vince D. Calhoun",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 157,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.357084+00:00",
      "updated_at": "2026-01-27T05:29:03.357086+00:00"
    },
    {
      "id": "7484e5582163523db0b00c03a647f384",
      "url": "https://arxiv.org/abs/2508.16414",
      "title": "NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents",
      "content": "arXiv:2508.16414v2 Announce Type: replace \nAbstract: Understanding how prenatal exposure to psychoactive substances such as cannabis shapes adolescent brain organization remains a critical challenge, complicated by the complexity of multimodal neuroimaging data and the limitations of conventional analytic methods. Existing approaches often fail to fully capture the complementary features embedded within structural and functional connectomes, constraining both biological insight and predictive performance. To address this, we introduced NeuroKoop, a novel graph neural network-based framework that integrates structural and functional brain networks utilizing neural Koopman operator-driven latent space fusion. By leveraging Koopman theory, NeuroKoop unifies node embeddings derived from source-based morphometry (SBM) and functional network connectivity (FNC) based brain graphs, resulting in enhanced representation learning and more robust classification of prenatal drug exposure (PDE) status. Applied to a large adolescent cohort from the ABCD dataset, NeuroKoop outperformed relevant baselines and revealed salient structural-functional connections, advancing our understanding of the neurodevelopmental impact of PDE.",
      "author": "Badhan Mazumder, Aline Kotoski, Vince D. Calhoun, Dong Hye Ye",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.357033+00:00",
      "updated_at": "2026-01-27T05:29:03.357035+00:00"
    },
    {
      "id": "8ac42c99e63863411a421ec9c9763a9f",
      "url": "https://arxiv.org/abs/2601.17528",
      "title": "Sampling in the Euclidean Motion Group and a Problem from Brain's Primary Visual Cortex",
      "content": "arXiv:2601.17528v1 Announce Type: cross \nAbstract: We study a sampling problem for the abstract wavelet transform associated with the quasiregular representation of the $SE(2)$ group, for a modulated gaussian mother wavelet. This problem is motivated by the behavior of brain's primary visual cortex. We provide a characterization in terms of a dual Gramian matrix, and study numerically the relationships among the parameters defining the sampling and the mother wavelet.",
      "author": "Davide Barbieri",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 68,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.357003+00:00",
      "updated_at": "2026-01-27T05:29:03.357005+00:00"
    },
    {
      "id": "2e5af7c5f4bdcd30ec60e8a0fa72ff83",
      "url": "https://arxiv.org/abs/2601.18286",
      "title": "Closed Eyes and Coil Size -- Effects on Motor Threshold and Intracortical Inhibition, measured with TMS",
      "content": "arXiv:2601.18286v1 Announce Type: new \nAbstract: Rationale: Transcranial magnetic stimulation (TMS)-based measures such as resting motor threshold (RMT) and short interval intracortical inhibition (SICI) are widely employed to study motor cortical and corticospinal tract function, and effects of diseases and drug therapies thereon. However, the effect of key experimental factors, including as eye state (open or closed) or stimulating coil size, remain unclear. As such, it is unknown whether these factors must be kept consistent across multi-center studies, and whether differences in such factors may underpin contradictory findings in existing literature.\n  Materials and Methods: Threshold tracking TMS was employed to measure RMT and SICI (3ms interstimulus interval, conditioning at 70% of RMT) in 21 alert and awake, healthy controls. Motor evoked potentials were recorded from abductor pollicis brevis. Both RMT and SICI were measured under 6 conditions, while eyes were open or closed, using 3 figure-of-eight coils of differing winding diameter. Mixed effects modelling was employed to investigate effects of eye state and coil size on each measure.\n  Results: RMT was found to be significantly higher for the smallest (30BFT) coil compared to both larger (50BFT and 70BF) coils. No difference in SICI was identified across coil sizes. Eye state was not found to affect either RMT or SICI measurements.\n  Conclusions: Measurements of RMT and SICI can be considered comparable if recorded with eyes open or closed, provided the individual is awake and alert. Measurements of SICI recorded with figure-of-eight coils of different size can be considered comparable.",
      "author": "Meher Sabharwal, Narin Suleyman, Gabriel R. Palma, Roisin McMackin",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 247,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.356976+00:00",
      "updated_at": "2026-01-27T05:29:03.356977+00:00"
    },
    {
      "id": "22bb758f001e5b16706a159eb578702e",
      "url": "https://arxiv.org/abs/2601.17796",
      "title": "AI and World Models",
      "content": "arXiv:2601.17796v1 Announce Type: new \nAbstract: While large neural nets perform impressively on specific tasks, they are unreliable and unsafe, as is shown by the persistent hallucinations of large language models. This paper shows that large neural nets are intrinsically unreliable, because it is not possible to make or validate a tractable theory of how a neural net works. There is no reliable way to extrapolate its performance from a limited number of test cases to an unlimited set of use cases. To have confidence in the performance of a neural net, it is necessary to enclose it in a guardrail which is provably safe, so that whatever the neural net does, there cannot be harmful consequences. World models have been proposed as a way to do this. This paper discusses the scope and architecture required of world models. World models are often conceived as models of the physical and natural world, using established theories of natural science, or learned regularities, to predict the physical consequences of AI actions. However, unforeseen consequences of AI actions impact the human social world as much as the physical world. To predict and control the consequences of AI, a world model needs to include a model of the human social world. I explore the challenges that this entails. Human language is based on a Common Ground of mutual understanding of the world, shared by the people conversing. The common ground is an overlapping subset of each persons world model, including their models of the physical, social and mental worlds. LLMs have no stable representation of a common ground. To be reliable, AI systems will need to represent a common ground with their users, including physical, mental and social domains.",
      "author": "Robert Worden",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 284,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.356938+00:00",
      "updated_at": "2026-01-27T05:29:03.356940+00:00"
    },
    {
      "id": "58f0ba103f74994e2f09dba733a788b1",
      "url": "https://arxiv.org/abs/2601.17523",
      "title": "Unsupervised sleep-like intra- and inter-layer plasticity categorizes and improves energy efficiency in a multilayer spiking network",
      "content": "arXiv:2601.17523v1 Announce Type: new \nAbstract: Sleep is thought to support memory consolidation and the recovery of optimal energetic regime by reorganizing synaptic connectivity, yet how plasticity across hierarchical brain circuits contributes to abstraction and energy efficiency remains unclear. Here we study a spiking multi-layer network alternating wake-like and deep-sleep-like states, with state-dependent dendritic integration and synaptic plasticity in a biologically inspired thalamo-cortical framework. During wakefulness, the model learns from few perceived examples, while during deep sleep it undergoes spontaneous replay driven by slow oscillations. Plasticity enabled not only within intra-layer connections, but also in inter-layer pathways, is critical for memory consolidation and energetic downshift. Compared to restricted plasticity, full inter-layer plasticity yields higher post-sleep visual classification accuracy and promotes the emergence of sharper class-specific associations. Furthermore, we introduce a biophysically grounded estimator of metabolic power expressing network energy consumption in ATP units, partitioned into baseline, synaptic maintenance, action potential, and transmission costs. We find that inter-layer plasticity in sleep leads to a larger reduction in firing rates, synaptic strength and synaptic activity, corresponding to a substantially larger decrease in power consumption. This work suggests promising elements to be integrated in neuromorphic/energy-efficient AI learning systems, supported by brain state-specific apical mechanisms.",
      "author": "Leonardo Tonielli, Cosimo Lupo, Elena Pastorelli, Giulia De Bonis, Francesco Simula, Alessandro Lonardo, Pier Stanislao Paolucci",
      "published_date": "2026-01-27T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 200,
      "reading_time": 1,
      "created_at": "2026-01-27T05:29:03.356890+00:00",
      "updated_at": "2026-01-27T05:29:03.356894+00:00"
    }
  ]
}