{
  "last_updated": "2026-01-29T05:30:18.797392+00:00",
  "count": 20,
  "articles": [
    {
      "id": "698a3967478d4ba61f5711c9117ea7e6",
      "url": "https://www.embs.org/uncategorized/call-for-applications-ieee-tmrb-editor-in-chief-search/",
      "title": "Call for Applications: IEEE T-MRB Editor in Chief Search",
      "content": "<p>The post <a href=\"https://www.embs.org/uncategorized/call-for-applications-ieee-tmrb-editor-in-chief-search/\">Call for Applications: IEEE T-MRB Editor in Chief Search</a> appeared first on <a href=\"https://www.embs.org\">IEEE EMBS</a>.</p>",
      "author": "Deidre Artis",
      "published_date": "2025-04-03T14:16:16+00:00",
      "source": "Embs",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 18,
      "reading_time": 1,
      "created_at": "2026-01-29T05:30:01.428550+00:00",
      "updated_at": "2026-01-29T05:30:01.428552+00:00"
    },
    {
      "id": "6180193191d50495d0300d3fd016ab1d",
      "url": "https://arxiv.org/abs/2601.20437",
      "title": "Remember Me, Not Save Me: A Collective Memory System for Evolving Virtual Identities in Augmented Reality",
      "content": "arXiv:2601.20437v1 Announce Type: new \nAbstract: This paper presents \"Remember Me, Not Save Me,\" an AR & AI system enabling virtual citizens to develop personality through collective dialogue. Core innovations include: Dynamic Collective Memory (DCM) model with narrative tension mechanisms for handling contradictory memories; State-Reflective Avatar for ambient explainability; and Geo-Cultural Context Anchoring for local identity. Deployed at the 2024 Jinan Biennale, the system demonstrated stable personality emergence (ISTP type via Apply Magic Sauce analysis) from over 2,500 public interactions. We provide a framework for designing evolving digital entities that transform collective memory into coherent identity.",
      "author": "Tongzhou Yu, Han Lin",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 95,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.737183+00:00",
      "updated_at": "2026-01-29T05:29:59.737185+00:00"
    },
    {
      "id": "fb4d929277c2838353d4042260dd1014",
      "url": "https://arxiv.org/abs/2601.20402",
      "title": "GuideAI: A Real-time Personalized Learning Solution with Adaptive Interventions",
      "content": "arXiv:2601.20402v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have emerged as powerful learning tools, but they lack awareness of learners' cognitive and physiological states, limiting their adaptability to the user's learning style. Contemporary learning techniques primarily focus on structured learning paths, knowledge tracing, and generic adaptive testing but fail to address real-time learning challenges driven by cognitive load, attention fluctuations, and engagement levels. Building on findings from a formative user study (N=66), we introduce GuideAI, a multi-modal framework that enhances LLM-driven learning by integrating real-time biosensory feedback including eye gaze tracking, heart rate variability, posture detection, and digital note-taking behavior. GuideAI dynamically adapts learning content and pacing through cognitive optimizations (adjusting complexity based on learning progress markers), physiological interventions (breathing guidance and posture correction), and attention-aware strategies (redirecting focus using gaze analysis). Additionally, GuideAI supports diverse learning modalities, including text-based, image-based, audio-based, and video-based instruction, across varied knowledge domains. A preliminary study (N = 25) assessed GuideAI's impact on knowledge retention and cognitive load through standardized assessments. The results show statistically significant improvements in both problem-solving capability and recall-based knowledge assessments. Participants also experienced notable reductions in key NASA-TLX measures including mental demand, frustration levels, and effort, while simultaneously reporting enhanced perceived performance. These findings demonstrate GuideAI's potential to bridge the gap between current LLM-based learning systems and individualized learner needs, paving the way for adaptive, cognition-aware education at scale.",
      "author": "Ananya Shukla, Chaitanya Modi, Satvik Bajpai, Siddharth Siddharth",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 231,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.737157+00:00",
      "updated_at": "2026-01-29T05:29:59.737159+00:00"
    },
    {
      "id": "0e1d7543db212c9352df4608d8275258",
      "url": "https://arxiv.org/abs/2601.20311",
      "title": "DiagLink: A Dual-User Diagnostic Assistance System by Synergizing Experts with LLMs and Knowledge Graphs",
      "content": "arXiv:2601.20311v1 Announce Type: new \nAbstract: The global shortage and uneven distribution of medical expertise continue to hinder equitable access to accurate diagnostic care. While existing intelligent diagnostic system have shown promise, most struggle with dual-user interaction, and dynamic knowledge integration -- limiting their real-world applicability. In this study, we present DiagLink, a dual-user diagnostic assistance system that synergizes large language models (LLMs), knowledge graphs (KGs), and medical experts to support both patients and physicians. DiagLink uses guided dialogues to elicit patient histories, leverages LLMs and KGs for collaborative reasoning, and incorporates physician oversight for continuous knowledge validation and evolution. The system provides a role-adaptive interface, dynamically visualized history, and unified multi-source evidence to improve both trust and usability. We evaluate DiagLink through user study, use cases and expert interviews, demonstrating its effectiveness in improving user satisfaction and diagnostic efficiency, while offering insights for the design of future AI-assisted diagnostic systems.",
      "author": "Zihan Zhou, Yinan Liu, Yuyang Xie, Bin Wang, Xiaochun Yang, Zezheng Feng",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.737118+00:00",
      "updated_at": "2026-01-29T05:29:59.737119+00:00"
    },
    {
      "id": "44064db91b264ca87ac6cb864ac5139b",
      "url": "https://arxiv.org/abs/2601.20194",
      "title": "An Autonomous Agent Framework for Feature-Label Extraction from Device Dialogues and Automatic Multi-Dimensional Device Hosting Planning Based on Large Language Models",
      "content": "arXiv:2601.20194v1 Announce Type: new \nAbstract: With the deep integration of artificial intelligence and smart home technologies, the intelligent transformation of traditional household appliances has become an inevitable trend. This paper presents AirAgent--an LLM-driven autonomous agent framework designed for home air systems. Leveraging a voice-based dialogue interface, AirAgent autonomously and personally manages indoor air quality through comprehensive perception, reasoning, and control. The framework innovatively adopts a two-layer cooperative architecture: Memory-Based Tag Extraction and Reasoning-Driven Planning. First, a dynamic memory tag extraction module continuously updates personalized user profiles. Second, a reasoning-planning model integrates real-time environmental sensor data, user states, and domain-specific prior knowledge (e.g., public health guidelines) to generate context-aware decisions. To support both interpretability and execution, we design a semi-streaming output mechanism that uses special tokens to segment the model's output stream in real time, simultaneously producing human-readable Chain-of-Thought explanations and structured, device-executable control commands. The system handles planning across 25 distinct complex dimensions while satisfying more than 20 customized constraints. As a result, AirAgent endows home air systems with proactive perception, service, and orchestration capabilities, enabling seamless, precise, and personalized air management responsive to dynamic indoor and outdoor conditions. Experimental results demonstrate up to 94.9 percent accuracy and more than 20 percent improvement in user experience metrics compared to competing commercial solutions.",
      "author": "Huichao Men, Yizhen Hu, Yu Gao, Xiaofeng Mou, Yi Xu, Xinhua Xiao",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 212,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.737088+00:00",
      "updated_at": "2026-01-29T05:29:59.737090+00:00"
    },
    {
      "id": "eba703a43f6d49fd92907ad0aee9508a",
      "url": "https://arxiv.org/abs/2601.20161",
      "title": "Supporting Informed Self-Disclosure: Design Recommendations for Presenting AI-Estimates of Privacy Risks to Users",
      "content": "arXiv:2601.20161v1 Announce Type: new \nAbstract: People candidly discuss sensitive topics online under the perceived safety of anonymity; yet, for many, this perceived safety is tenuous, as miscalibrated risk perceptions can lead to over-disclosure. Recent advances in Natural Language Processing (NLP) afford an unprecedented opportunity to present users with quantified disclosure-based re-identification risk (i.e., \"population risk estimates\", PREs). How can PREs be presented to users in a way that promotes informed decision-making, mitigating risk without encouraging unnecessary self-censorship? Using design fictions and comic-boarding, we story-boarded five design concepts for presenting PREs to users and evaluated them through an online survey with N = 44 Reddit users. We found participants had detailed conceptions of how PREs may impact risk awareness and motivation, but envisioned needing additional context and support to effectively interpret and act on risks. We distill our findings into four key design recommendations for how best to present users with quantified privacy risks to support informed disclosure decision-making.",
      "author": "Isadora Krsek, Meryl Ye, Wei Xu, Alan Ritter, Laura Dabbish, Sauvik Das",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.737054+00:00",
      "updated_at": "2026-01-29T05:29:59.737056+00:00"
    },
    {
      "id": "d85f6b40aec9d5cacede17b73e63166e",
      "url": "https://arxiv.org/abs/2601.20100",
      "title": "Taming Toxic Talk: Using chatbots to intervene with users posting toxic comments",
      "content": "arXiv:2601.20100v1 Announce Type: new \nAbstract: Generative AI chatbots have proven surprisingly effective at persuading people to change their beliefs and attitudes in lab settings. However, the practical implications of these findings are not yet clear. In this work, we explore the impact of rehabilitative conversations with generative AI chatbots on users who share toxic content online. Toxic behaviors -- like insults or threats of violence, are widespread in online communities. Strategies to deal with toxic behavior are typically punitive, such as removing content or banning users. Rehabilitative approaches are rarely attempted, in part due to the emotional and psychological cost of engaging with aggressive users. In collaboration with seven large Reddit communities, we conducted a large-scale field experiment (N=893) to invite people who had recently posted toxic content to participate in conversations with AI chatbots. A qualitative analysis of the conversations shows that many participants engaged in good faith and even expressed remorse or a desire to change. However, we did not observe a significant change in toxic behavior in the following month compared to a control group. We discuss possible explanations for our findings, as well as theoretical and practical implications based on our results.",
      "author": "Jeremy Foote, Deepak Kumar, Bedadyuti Jha, Ryan Funkhouser, Loizos Bitsikokos, Hitesh Goel, Hsuen-Chi Chiu",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 196,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.737024+00:00",
      "updated_at": "2026-01-29T05:29:59.737025+00:00"
    },
    {
      "id": "997f9672b5e967541bd5879df383c81d",
      "url": "https://arxiv.org/abs/2601.20086",
      "title": "Evaluating Actionability in Explainable AI",
      "content": "arXiv:2601.20086v1 Announce Type: new \nAbstract: A core assumption of Explainable AI (XAI) is that explanations are useful to users -- that is, users will do something with the explanations. Prior work, however, does not clearly connect the information provided in explanations to user actions to evaluate effectiveness. In this paper, we articulate this connection. We conducted a formative study through 14 interviews with end users in education and medicine. We contribute a catalog of information and associated actions. Our catalog maps 12 categories of information that participants described relying on to take 60 different actions. We show how AI Creators can use the catalog's specificity and breadth to articulate how they expect information in their explanations to lead to user actions and test their assumptions. We use an exemplar XAI system to illustrate this approach. We conclude by discussing how our catalog expands the design space for XAI systems to support actionability.",
      "author": "Gennie Mansi, Julia Kim, Mark Riedl",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 152,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.736991+00:00",
      "updated_at": "2026-01-29T05:29:59.736993+00:00"
    },
    {
      "id": "24c8145368d8335073baff360aa38482",
      "url": "https://arxiv.org/abs/2601.20085",
      "title": "Editrail: Understanding AI Usage by Visualizing Student-AI Interaction in Code",
      "content": "arXiv:2601.20085v1 Announce Type: new \nAbstract: Programming instructors have diverse philosophies about integrating generative AI into their classes. Some encourage students to use AI, while others restrict or forbid it. Regardless of their approach, all instructors benefit from understanding how their students actually use AI while writing code. Such insight helps instructors assess whether AI use aligns with their pedagogical goals, enables timely intervention when they find unproductive usage patterns, and establishes effective policies for AI use. However, our survey with programming instructors found that many instructors lack visibility into how students use AI in their code-writing processes. To address this challenge, we introduce Editrail, an interactive system that enables instructors to track students' AI usage, create personalized assessments, and provide timely interventions, all within the workflow of monitoring coding histories. We found that Editrail enables instructors to detect AI use that conflicts with pedagogical goals accurately and to determine when and which students require intervention.",
      "author": "Ashley Ge Zhang, Yan-Ru Jhou, Yinuo Yang, Shamita Rao, Maryam Arab, Yan Chen, Steve Oney",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.736958+00:00",
      "updated_at": "2026-01-29T05:29:59.736960+00:00"
    },
    {
      "id": "ed53e05f9c30e40812d7dd1e5ea04e47",
      "url": "https://arxiv.org/abs/2601.20080",
      "title": "Locatability and Locatability Robustness of Visual Variables in Single Target Localization",
      "content": "arXiv:2601.20080v1 Announce Type: new \nAbstract: Finding a particular object in a display is important for viewers in many visualizations, for example, when reacting to brushing or to a highlighted object. This can be enabled by making the target object different in one of the visual variables that determine the object's appearance; for example, by changing its color or size. Certain interpretations of the visual search literature have promoted the view that using visual variables such as hue-often labeled as preattentive-would make the target object automatically \"popout,\" implying that an object can be located almost instantly, regardless of the number of objects in the display. In this paper we present a study that serves as a bridge between the extensive visual search literature and visualization, establishing empirical base measurements for the localization task. By testing displays with up to hundreds of objects, we are able to show that none of the common visual variables is immune to the increase in the number of objects. We also provide the first empirically informed comparisons between visual variables for this task in the context of visualization, and show how different visual variables have varying robustness with respect to two additional dimensions: the location of the target and the overall visual arrangement (layout). A free copy of this paper and all supplemental materials are available on our online repository: https://osf.io/z68ak/overview.",
      "author": "Wei Wei, Miguel A. Nacenta, Michelle F. Miranda, Charles Perin",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 225,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.736906+00:00",
      "updated_at": "2026-01-29T05:29:59.736909+00:00"
    },
    {
      "id": "9968c6d968394a674b0c0eb059645ea9",
      "url": "https://arxiv.org/abs/2601.20010",
      "title": "Treating symptoms or root causes: How does information about causal mechanisms affect interventions?",
      "content": "arXiv:2601.20010v1 Announce Type: new \nAbstract: When deciding how to solve complex problems, it seems important not only to know whether an intervention is helpful but also to understand why. Therefore, the present study investigated whether explicit information about causal mechanisms enables people to distinguish between multiple interventions. It was hypothesised that mechanism information helps them appreciate indirect interventions that treat the root causes of a problem instead of just fixing its symptoms. This was investigated in an experimental hoof trimming scenario in which participants evaluated various interventions. To do so, they received causal diagrams with different types of causal information and levels of mechanistic detail. While detailed mechanism information and its embedding in the context of other influences made participants less sceptical towards indirect interventions, the effects were quite small. Moreover, it did not mitigate participants' robust preference for interventions that only fix a problem's symptoms. Taken together, the findings suggest that in order to help people choose sustainable interventions, it is not sufficient to make information about causal mechanisms available.",
      "author": "Romy M\\\"uller",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:59.736861+00:00",
      "updated_at": "2026-01-29T05:29:59.736865+00:00"
    },
    {
      "id": "447bcdf02499d9edc897e2500d144ae4",
      "url": "https://arxiv.org/abs/2601.20480",
      "title": "An explainable framework for the relationship between dementia and glucose metabolism patterns",
      "content": "arXiv:2601.20480v1 Announce Type: cross \nAbstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.",
      "author": "C. V\\'azquez-Garc\\'ia, F. J. Mart\\'inez-Murcia, F. Segovia Rom\\'an, A. Forte, J. Ram\\'irez, I. Ill\\'an, A. Hern\\'andez-Segura, C. Jim\\'enez-Mesa, Juan M. G\\'orriz",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 177,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:58.636157+00:00",
      "updated_at": "2026-01-29T05:29:58.636159+00:00"
    },
    {
      "id": "a7850001dd81b0560a8b2d76452c8270",
      "url": "https://arxiv.org/abs/2601.20236",
      "title": "Implications of temporal sampling in voltage imaging microscopy",
      "content": "arXiv:2601.20236v1 Announce Type: cross \nAbstract: Significance: Voltage imaging microscopy has emerged as a powerful tool to investigate neural activity both in vivo and in vitro. Various imaging approaches have been developed, including point-scanning, line-scanning and wide-field microscopes, however the effects of their different temporal sampling methods on signal fidelity have not yet been fully investigated. Aim: To provide an analysis of the inherent advantages and disadvantages of temporal sampling in scanning and wide-field microscopes and their effect on the fidelity of voltage spike detection. Approach: We develop a mathematical framework based on a mixture of analytical modeling and computer simulations with Monte-Carlo approaches. Results: Scanning microscopes outperform wide-field microscopes in low signal-to-noise conditions and when only a small subset of spikes needs to be detected. Wide-field microscopes outperform scanning microscopes when the measurement is temporally undersampled and a large fraction of the spikes needs to be detected. Both modalities converge in performance as sampling increases and the frame rate reaches the decay rate of the voltage indicator. Conclusions: Our work provides guidance for the selection of optimal temporal sampling parameters for voltage imaging. Most importantly it advises against using scanning voltage imaging microscopes at frame rates below 500 Hz.",
      "author": "Jakub Czuchnowski, Jerome Mertz",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 199,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:58.636123+00:00",
      "updated_at": "2026-01-29T05:29:58.636125+00:00"
    },
    {
      "id": "2f9ea4909179e03057524dabd8c9628b",
      "url": "https://arxiv.org/abs/2601.20447",
      "title": "Assembling the Mind's Mosaic: Towards EEG Semantic Intent Decoding",
      "content": "arXiv:2601.20447v1 Announce Type: new \nAbstract: Enabling natural communication through brain-computer interfaces (BCIs) remains one of the most profound challenges in neuroscience and neurotechnology. While existing frameworks offer partial solutions, they are constrained by oversimplified semantic representations and a lack of interpretability. To overcome these limitations, we introduce Semantic Intent Decoding (SID), a novel framework that translates neural activity into natural language by modeling meaning as a flexible set of compositional semantic units. SID is built on three core principles: semantic compositionality, continuity and expandability of semantic space, and fidelity in reconstruction. We present BrainMosaic, a deep learning architecture implementing SID. BrainMosaic decodes multiple semantic units from EEG/SEEG signals using set matching and then reconstructs coherent sentences through semantic-guided reconstruction. This approach moves beyond traditional pipelines that rely on fixed-class classification or unconstrained generation, enabling a more interpretable and expressive communication paradigm. Extensive experiments on multilingual EEG and clinical SEEG datasets demonstrate that SID and BrainMosaic offer substantial advantages over existing frameworks, paving the way for natural and effective BCI-mediated communication.",
      "author": "Jiahe Li, Junru Chen, Fanqi Shen, Jialan Yang, Jada Li, Zhizhang Yuan, Baowen Cheng, Meng Li, Yang Yang",
      "published_date": "2026-01-29T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 170,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:58.636084+00:00",
      "updated_at": "2026-01-29T05:29:58.636088+00:00"
    },
    {
      "id": "55f0157b4e9cc009560ff6cfd78fc9dd",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.26.700084v1?rss=1",
      "title": "Protein folding stress shapes microglial phenotype in progressive supranuclear palsy",
      "content": "Microglia play a pivotal role in neurodegeneration, yet their response to tau pathology remains incompletely understood. Through single-nucleus RNA sequencing of Amyloid beta plaque-free frontal cortex from 4R tauopathy progressive supranuclear palsy (PSP) brains, we reveal a distinct transcriptional reprogramming of homeostatic microglia. PSP microglia exhibit pronounced protein-folding and ER-stress signatures, elevated homeostatic and MHC class II gene expression, and attenuated cytoskeletal, motility, and interferon-related programs, accompanied by reduced inferred intercellular communication. Neuropathological analysis corroborates a shift toward highly ramified morphologies and increased MHC class II positivity in PSP cortex. In HMC3 cells, pharmacological induction of protein-folding stress recapitulates this phenotype, driving homeostatic gene upregulation and morphological transition to process-bearing forms. We define this protein-folding-stress-associated microglial (PSAM) state as a PSP-linked phenotype mechanistically distinct from disease-associated microglia (DAM), highlighting ER stress as a key driver of microglial remodeling in tauopathies.",
      "author": "Tanikawa, S., Kuwabara, T., Tanaka, S., Kovacs, G. G.",
      "published_date": "2026-01-28T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 140,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:49.740867+00:00",
      "updated_at": "2026-01-29T05:29:49.740869+00:00"
    },
    {
      "id": "d1c66b3d44cef9b93c5b1677cb712e3f",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.26.700683v1?rss=1",
      "title": "Cell-Type Specific Molecular and Functional Consequences of TDP-43 Loss-of-Function in Human Induced Neurons",
      "content": "Amyotrophic Lateral Sclerosis (ALS) is a devastating neurodegenerative disorder characterized by the cytoplasmic aggregation and nuclear depletion of the TDP-43 protein. The latter impairs TDP-43 function as an RNA-binding protein and compromises the repression of cryptic splicing events, affecting both glutamatergic upper motor neurons and cholinergic lower motor neurons. This study systematically investigated the molecular and functional consequences of TDP-43 knockdown in human induced pluripotent stem cell (hiPSC)-derived glutamatergic neurons (iGNs) and cholinergic motor neurons (iMNs) using antisense oligonucleotides. The results demonstrated that TDP-43 loss elicits widespread, cell-type-specific changes in gene expression and alternative splicing. Notably, a shared subset of ALS-associated targets, including STMN2 and UNC13A, were consistently downregulated and mis-spliced across both neuronal subtypes. Functionally, Microelectrode Array (MEA) electrophysiology recordings revealed that TDP-43 knockdown induces a hyperexcitable phenotype in both neuronal populations, though they exhibited distinct network patterns: iGNs displayed synchronized bursting and significant shifts in overall electrophysiological profiles, while iMNs showed asynchronous firing. Furthermore, the inclusion of astrocytes in co-culture models expanded the repertoire of detectable cryptic splicing, including an event in HDGFL2 previously identified in patient cerebrospinal fluid. Despite these profound molecular and functional deficits, TDP-43 depletion did not impact neuronal viability or increase susceptibility to glutamate-induced excitotoxicity. These findings validate hiPSC-derived iGNs and iMNs as relevant models for ALS and highlight the critical necessity of considering cell-type specificity when elucidating pathogenesis and developing targeted therapies.",
      "author": "Filippa, V. G., Bach, K., Kolodyazhniy, V., Joenson, L., Costa, M. R.",
      "published_date": "2026-01-28T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 229,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:49.740835+00:00",
      "updated_at": "2026-01-29T05:29:49.740837+00:00"
    },
    {
      "id": "b901a05081ba24178edecc4ba4d153f1",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.26.701887v1?rss=1",
      "title": "Flexible perception of face attributes under naturalistic visual constraints",
      "content": "How does human brain adapt face perception to naturalistic visual constraints? While faces are typically perceived under dynamic and uncertain conditions, most studies use static presentation and enough temporal exposure, leaving the neural mechanisms of adaptive face processing poorly understood. Here, we employed steady-state evoked potential electroencephalography (EEG) to track how perception of multiple face attributes (age, emotion, gender, and race) adjusts to two ecologically relevant scenarios: (1) faces gradually sharpening from blur to clear (mimicking approaching from a distance), and (2) faces presented with incrementally increasing exposure times (mimicking brief, flashed encounters). By progressively increasing sensory input (via spatial frequency content in Experiment 1 and exposure time in Experiment 2) in a stepwise manner, we show that emotion and race categorization emerge early, even under high blur (e.g., 4.89 cycles/image) or brief exposures (e.g., 41.7-50 ms), reflecting coarse visual processing. In contrast, age discrimination requires higher clarity (e.g., 7.31 cycles/image) but shorter exposure (e.g., 41.7 ms) if clear images are presented. In both scenarios, gender processing exhibits the strongest dependence on clarity (e.g., 10.94 cycles/image) and time (e.g., 66.7 ms). Representational similarity analyses further show that reliable response patterns for emotion and race emerge earlier in the posterior brain region relative to those for gender in both experiments. Together, these results identify a flexible temporal order for face perception that adapts to naturalistic visual constraints, bridging the gap between controlled laboratory paradigms and naturalistic social vision.",
      "author": "Li, S., Wu, Y., Yan, X.",
      "published_date": "2026-01-28T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 237,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:49.740788+00:00",
      "updated_at": "2026-01-29T05:29:49.740793+00:00"
    },
    {
      "id": "ef1d360d604223946bde97ad8fab7bd1",
      "url": "https://www.frontiersin.org/articles/10.3389/fnbot.2025.1574044",
      "title": "Path planning of industrial robots based on the adaptive field cooperative sampling algorithm",
      "content": "For the low efficiency and poor generalization ability of path planning algorithm of industrial robots, this work proposes an adaptive field co-sampling algorithm (AFCS). Firstly, the environment complexity function is proposed to make full use of environment information and improve its generalization ability of the traditional rapidly random search tree algorithm (RRT) algorithm. Then an optimal sampling strategy is proposed to make the improvement of the efficiency and optimal direction of RRT algorithm. Finally, this article designs a collaborative extension strategy, which introduces the improved artificial potential field algorithm (APF) into the traditional RRT algorithm to determine the new nodes, so as to improve the orientation and expansion efficiency of the algorithm. The proposed AFCS algorithm completes simulation experiments in two environments with different complexity. Compared with the traditional RRT, RRT* and tRRT algorithm, the results show that the AFCS algorithm has achieved great improvement in environmental adaptability, stability and efficiency. At last, ROKAE industrial robot is taken as the object to build a simulation environment for the path planning, which further verifies the practicability of the algorithm.",
      "author": "Lv Wei",
      "published_date": "2025-11-13T00:00:00+00:00",
      "source": "Frontiers Neurorobotics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 178,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:38.261573+00:00",
      "updated_at": "2026-01-29T05:29:38.261575+00:00"
    },
    {
      "id": "9f4f593edeb4e4729540f7d732972f8b",
      "url": "https://www.reddit.com/r/Python/comments/1qps4e7/spectrograms_a_highperformance_toolkit_for_audio/",
      "title": "Spectrograms: A high-performance toolkit for audio and image analysis",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve released <a href=\"https://github.com/jmg049/Spectrograms\">Spectrograms</a>, a library designed to provide an all-in-one pipeline for spectral analysis. It was originally built to handle the spectrogram logic for my <a href=\"https://github.com/jmg049/audio_samples\">audio_samples</a> project and was abstracted into its own toolkit to provide a more complete set of features than what is currently available in the Python ecosystem.</p> <h3>What My Project Does</h3> <p><strong>Spectrograms</strong> provides a high-performance pipeline for computing spectrograms and performing FFT-based operations on 1D signals (audio) and 2D signals (images). It supports various frequency scales (Linear, Mel, ERB, LogHz) and amplitude scales (Power, Magnitude, Decibels), alongside general-purpose 2D FFT operations for image processing like spatial filtering and convolution.</p> <h3>Target Audience</h3> <p>This library is designed for developers and researchers requiring production-ready DSP tools. It is particularly useful for those needing batch processing efficiency, low-latency streaming support, or a Python API where metadata (like frequency/time axes) remains unified with the computation.</p> <h3>Comparison</h3> <p>Unlike standard alternatives such as SciPy or Librosa which return raw <code>ndarrays</code>, <strong>Spectrograms</strong> returns context-aware objects that bundle metadata with the data. It uses a plan-based architecture implemented in Rust that releases the GIL, offering significant performance advantages in batch processing and parallel execution compared to naive NumPy-based implementations.</p> <hr /> <h3>Key Features:</h3> <ul> <li><strong>Integrated Metadata</strong>: Results are returned as <code>Spectrogram</code> objects rather than raw <code>ndarrays</code>. This ensures the frequency and time axes are always bundled with the data. The object maintains the parameters used for its creation and provides direct access to its <code>duration()</code>, <code>frequencies</code>, and <code>times</code>. These objects can act as drop-in replacements for <code>ndarrays</code> in most scenarios since they implement the <code>__array__</code> interface.</li> <li><strong>Unified API</strong>: The library handles the full process from raw samples to scaled results. It supports <code>Linear</code>, <code>Mel</code>, <code>ERB</code>, and <code>LogHz</code> frequency scales, with amplitude scaling in <code>Power</code>, <code>Magnitude</code>, or <code>Decibels</code>. It also includes support for chromagrams, MFCCs, and general-purpose 1D and 2D FFT functions.</li> <li><strong>Performance via Plan Reuse</strong>: For batch processing, the <code>SpectrogramPlanner</code> caches FFT plans and pre-computes filterbanks to avoid re-calculating constants in a loop. <strong>Benchmarks included in the repository show this approach to be faster across tested configurations compared to standard SciPy or Librosa implementations.</strong> The repo includes detailed benchmarks for various configurations.</li> <li><strong>GIL-free Execution</strong>: The core compute is implemented in Rust and releases the Python Global Interpreter Lock (GIL). This allows for actual parallel processing of audio batches using standard Python threading.</li> <li><strong>2D FFT Support</strong>: The library includes support for 2D signals and spatial filtering for image processing using the same design philosophy as the audio tools.</li> </ul> <h3>Quick Example: Linear Spectrogram</h3> <p>```python import numpy as np import spectrograms as sg</p> <h1>Generate a 440 Hz test signal</h1> <p>sr = 16000 t = np.linspace(0, 1.0, sr) samples = np.sin(2 * np.pi * 440.0 * t)</p> <h1>Configure parameters</h1> <p>stft = sg.StftParams(n_fft=512, hop_size=256, window=&quot;hanning&quot;) params = sg.SpectrogramParams(stft, sample_rate=sr)</p> <h1>Compute linear power spectrogram</h1> <p>spec = sg.compute_linear_power_spectrogram(samples, params)</p> <p>print(f&quot;Frequency range: {spec.frequency_range()} Hz&quot;) print(f&quot;Total duration: {spec.duration():.3f} s&quot;) print(f&quot;Data shape: {spec.data.shape}&quot;)</p> <p>```</p> <h3>Batch Processing with Plan Reuse</h3> <p>```python planner = sg.SpectrogramPlanner()</p> <h1>Pre-computes filterbanks and FFT plans once</h1> <p>plan = planner.mel_db_plan(params, mel_params, db_params)</p> <h1>Process signals efficiently</h1> <p>results = [plan.compute(s) for s in signal_batch]</p> <p>```</p> <h3>Benchmark Overview</h3> <p>The following table summarizes average execution times for various spectrogram operators using the Spectrograms library in Rust compared to NumPy and SciPy implementations.Comparisons to librosa are contained in the repo benchmarks since they target mel spectrograms specifically.</p> <table><thead> <tr> <th>Operator</th> <th>Rust (ms)</th> <th>Rust Std</th> <th>Numpy (ms)</th> <th>Numpy Std</th> <th>Scipy (ms)</th> <th>Scipy Std</th> <th>Avg Speedup vs NumPy</th> <th>Avg Speedup vs SciPy</th> </tr> </thead><tbody> <tr> <td>db</td> <td>0.257</td> <td>0.165</td> <td>0.350</td> <td>0.251</td> <td>0.451</td> <td>0.366</td> <td>1.363</td> <td>1.755</td> </tr> <tr> <td>erb</td> <td>0.601</td> <td>0.437</td> <td>3.713</td> <td>2.703</td> <td>3.714</td> <td>2.723</td> <td>6.178</td> <td>6.181</td> </tr> <tr> <td>loghz</td> <td>0.178</td> <td>0.149</td> <td>0.547</td> <td>0.998</td> <td>0.534</td> <td>0.965</td> <td>3.068</td> <td>2.996</td> </tr> <tr> <td>magnitude</td> <td>0.140</td> <td>0.089</td> <td>0.198</td> <td>0.133</td> <td>0.319</td> <td>0.277</td> <td>1.419</td> <td>2.287</td> </tr> <tr> <td>mel</td> <td>0.180</td> <td>0.139</td> <td>0.630</td> <td>0.851</td> <td>0.612</td> <td>0.801</td> <td>3.506</td> <td>3.406</td> </tr> <tr> <td>power</td> <td>0.126</td> <td>0.082</td> <td>0.205</td> <td>0.141</td> <td>0.327</td> <td>0.288</td> <td>1.630</td> <td>2.603</td> </tr> </tbody></table> <hr /> <p>Want to learn more about computational audio and image analysis? Check out my write up for the crate on the repo, <a href=\"https://github.com/jmg049/Spectrograms/blob/main/manual/Computational%20Audio%20and%20Image%20Analysis%20with%20the%20Spectrograms%20Library.pdf\">Computational Audio and Image Analysis with the Spectrograms Library</a></p> <hr /> <p><strong>PyPI</strong>: <a href=\"https://pypi.org/project/spectrograms/\">https://pypi.org/project/spectrograms/</a> <strong>GitHub</strong>: <a href=\"https://github.com/jmg049/Spectrograms\">https://github.com/jmg049/Spectrograms</a> <strong>Documentation</strong>: <a href=\"https://jmg049.github.io/Spectrograms/\">https://jmg049.github.io/Spectrograms/</a></p> <p><strong>Rust Crate</strong>: For those interested in the Rust implementation, the core library is also available as a Rust crate: <a href=\"https://crates.io/crates/spectrograms\">https://crates.io/crates/spectrograms</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JackG049\"> /u/JackG049 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qps4e7/spectrograms_a_highperformance_toolkit_for_audio/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qps4e7/spectrograms_a_highperformance_toolkit_for_audio/\">[comments]</a></span>",
      "author": "/u/JackG049",
      "published_date": "2026-01-28T23:44:56+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 730,
      "reading_time": 3,
      "created_at": "2026-01-29T05:29:13.645195+00:00",
      "updated_at": "2026-01-29T05:29:13.645197+00:00"
    },
    {
      "id": "5c0d3f7dcd6daaa5064b33f8c1f68b42",
      "url": "https://www.science.org/content/article/scienceshot-illustrated-guide-hippo-castration",
      "title": "An Illustrated Guide to Hippo Castration",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46746971\">Comments</a>",
      "author": "",
      "published_date": "2026-01-24T19:47:41+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-29T05:29:12.411390+00:00",
      "updated_at": "2026-01-29T05:29:12.411392+00:00"
    }
  ]
}