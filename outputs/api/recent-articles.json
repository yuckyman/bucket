{
  "last_updated": "2026-01-30T06:41:02.128654+00:00",
  "count": 20,
  "articles": [
    {
      "id": "138dbc20af95ce55414e7d62214f9607",
      "url": "https://www.embs.org/press/embc-eic-sunghoon-ivan-lee/#new_tab",
      "title": "Ivan Lee, Appointed Editor-in-Chief of EMBC Proceedings",
      "content": "<p>&#160;</p>\n<p>The post <a href=\"https://www.embs.org/press/embc-eic-sunghoon-ivan-lee/#new_tab\">Ivan Lee, Appointed Editor-in-Chief of EMBC Proceedings</a> appeared first on <a href=\"https://www.embs.org\">IEEE EMBS</a>.</p>",
      "author": "Nancy Zimmerman",
      "published_date": "2025-09-08T16:27:03+00:00",
      "source": "Embs",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 17,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:57.363176+00:00",
      "updated_at": "2026-01-30T06:41:02.019248+00:00",
      "metadata": {
        "processed_at": "2026-01-30T06:41:02.019257+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "8567468fc0edb90603eff96e86450dca",
      "url": "https://arxiv.org/abs/2601.21492",
      "title": "Organizational Practices and Socio-Technical Design of Human-Centered AI",
      "content": "arXiv:2601.21492v1 Announce Type: new \nAbstract: This contribution explores how the integration of Artificial Intelligence (AI) into organizational practices can be effectively framed through a socio-technical perspective to comply with the requirements of Human-centered AI (HCAI). Instead of viewing AI merely as a technical tool, the analysis emphasizes the importance of embedding AI into communication, collaboration, and decision-making processes within organizations from a human-centered perspective. Ten case-based patterns illustrate how AI support of predictive maintenance can be organized to address quality assurance and continuous improvement and to provide different types of sup-port for HCAI. The analysis shows that AI adoption often requires and enables new forms of organizational learning, where specialists jointly interpret AI output, adapt workflows, and refine rules for system improve-ment. Different dimensions and levels of socio-technical integration of AI are considered to reflect the effort and benefits of keeping the organization in the loop.",
      "author": "Thomas Herrmann",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 146,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784628+00:00",
      "updated_at": "2026-01-30T06:41:02.019262+00:00",
      "metadata": {
        "processed_at": "2026-01-30T06:41:02.019264+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b09f6477f033c865c25881245299cde3",
      "url": "https://arxiv.org/abs/2601.21490",
      "title": "Are they just delegating? Cross-Sample Predictions on University Students' & Teachers' Use of AI",
      "content": "arXiv:2601.21490v1 Announce Type: new \nAbstract: Mutual trust between teachers and students is a prerequisite for effective teaching, learning, and assessment in higher education. Accurate predictions about the other group's use of generative artificial intelligence (AI) are fundamental for such trust. However, the disruptive rise of AI has transformed academic work practices, raising important questions about how teachers and students use these tools and how well they can estimate each other's usage. While the frequency of use is well studied, little is known about how AI is used, and comparisons with similar practices are rare. This study surveyed German university teachers (N = 113) and students (N = 123) on the frequency of AI use and the degree of delegation across six identical academic tasks. Participants also provided incentivized cross-sample predictions of the other group's AI use to assess the accuracy of their predictions. We find that students reported higher use of AI and greater delegation than teachers. Both groups significantly overestimated the other group's use, with teachers predicting very frequent use and high delegation by students, and students assuming teachers use AI similarly to themselves. These findings reveal a perception gap between teachers' and students' expectations and actual AI use. Such gaps may hinder trust and effective collaboration, underscoring the need for open dialogue about AI practices in academia and for policies that support the equitable and transparent integration of AI tools in higher education.",
      "author": "Fabian Albers, Sebastian Strau{\\ss}, Nikol Rummel, Nils K\\\"obis",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 235,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784598+00:00",
      "updated_at": "2026-01-30T06:41:02.019267+00:00",
      "metadata": {
        "processed_at": "2026-01-30T06:41:02.019268+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "06545fa5612f1773e4eb9b29758a65e4",
      "url": "https://arxiv.org/abs/2601.21460",
      "title": "Tell Me What I Missed: Tell Me What I Missed: Interacting with GPT during Recalling of One-Time Witnessed Events",
      "content": "arXiv:2601.21460v1 Announce Type: new \nAbstract: LLM-assisted technologies are increasingly used to support cognitive processing and information interpretation, yet their role in aiding memory recall, and how people choose to engage with them, remains underexplored. We studied participants who watched a short robbery video (approximating a one-time eyewitness scenario) and composed recall statements using either a default GPT or a guided GPT prompted with a standardized eyewitness protocol. Results show that, in the default condition, participants who believed they had a clearer understanding of the event were more likely to trust GPT's output, whereas in the guided condition, participants showed stronger alignment between subjective clarity and actual recall. Additionally, participants evaluated the legitimacy of the individuals in the incident differently across conditions. Interaction analysis further revealed that default-GPT users spontaneously developed diverse strategies, including building on existing recollections, requesting potentially missing details, and treating GPT as a recall coach. This work shows how GPT-user interplay can subconsciously shape beliefs and perceptions of remembered events.",
      "author": "Suifang Zhou, Qi Gong, Ximing Shen, RAY LC",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 163,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784564+00:00",
      "updated_at": "2026-01-30T06:41:02.019271+00:00",
      "metadata": {
        "processed_at": "2026-01-30T06:41:02.019272+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "16c2be9cbb0cd2d3fccb892a4be0df73",
      "url": "https://arxiv.org/abs/2601.21271",
      "title": "Envisioning Audio Augmented Reality in Everyday Life",
      "content": "arXiv:2601.21271v1 Announce Type: new \nAbstract: While visual augmentation dominates the augmented reality landscape, devices like Meta Ray-Ban audio smart glasses signal growing industry movement toward audio augmented reality (AAR). Hearing is a primary channel for sensing context, anticipating change, and navigating social space, yet AAR's everyday potential remains underexplored. We address this gap through a collaborative autoethnography (N=5, authoring) and an online survey (N=74). We identify ten roles for AAR, grouped into three categories: task- and utility-oriented, emotional and social, and perceptual collaborator. These roles are further layered with a rhythmic and embodied collaborator framing, mapping them onto micro-, meso-, and macro-rhythms of everyday life. Our analysis surfaces nuanced tensions, such as blocking distractions without erasing social presence, highlighting the need for context-aware design. This paper contributes a foundational and forward-looking framework for AAR in everyday life, providing design groundwork for systems attuned to daily routines, sensory engagement, and social expectations.",
      "author": "Tram Thi Minh Tran, Soojeong Yoo, Oliver Weidlich, Yidan Cao, Xinyan Yu, Xin Cheng, Yin Ye, Natalia Gulbransen-Diaz, Callum Parker",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784533+00:00",
      "updated_at": "2026-01-30T06:41:02.019275+00:00",
      "metadata": {
        "processed_at": "2026-01-30T06:41:02.019276+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "228a74134a3b5c44debd1470d4172d13",
      "url": "https://arxiv.org/abs/2601.21264",
      "title": "Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR",
      "content": "arXiv:2601.21264v1 Announce Type: new \nAbstract: In time-critical eXtended reality (XR) scenarios where users must rapidly reorient their attention to hazards, alerts, or instructions while engaged in a primary task, spatial audio can provide an immediate directional cue without occupying visual bandwidth. However, such scenarios can afford only a brief auditory exposure, requiring users to interpret sound direction quickly and without extended listening or head-driven refinement. This paper reports a controlled exploratory study of rapid spatial-audio localization in XR. Using HRTF-rendered broadband stimuli presented from a semi-dense set of directions around the listener, we quantify how accurately users can infer coarse direction from brief audio alone. We further examine the effects of short-term visuo-auditory feedback training as a lightweight calibration mechanism. Our findings show that brief spatial cues can convey coarse directional information, and that even short calibration can improve users' perception of aural signals. While these results highlight the potential of spatial audio for rapid attention guidance, they also show that auditory cues alone may not provide sufficient precision for complex or high-stakes tasks, and that spatial audio may be most effective when complemented by other sensory modalities or visual cues, without relying on head-driven refinement. We leverage this study on spatial audio as a preliminary investigation into a first-stage attention-guidance channel for wearable XR (e.g., VR head-mounted displays and AR smart glasses), and provide design insights on stimulus selection and calibration for time-critical use.",
      "author": "Yoonsang Kim, Swapnil Dey, Arie Kaufman",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 235,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784504+00:00",
      "updated_at": "2026-01-30T05:32:55.784505+00:00"
    },
    {
      "id": "094d5e3c8a846b8a62678d5693bfc1b7",
      "url": "https://arxiv.org/abs/2601.21141",
      "title": "Optimization and Mobile Deployment for Anthropocene Neural Style Transfer",
      "content": "arXiv:2601.21141v1 Announce Type: new \nAbstract: This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer.\n  To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language.\n  Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes.\n  By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene.",
      "author": "Po-Hsun Chen, Ivan C. H. Liu",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 231,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784467+00:00",
      "updated_at": "2026-01-30T05:32:55.784469+00:00"
    },
    {
      "id": "b217c2c670391d765e0ba70ec796afee",
      "url": "https://arxiv.org/abs/2601.21057",
      "title": "Privatization of Synthetic Gaze: Attenuating State Signatures in Diffusion-Generated Eye Movements",
      "content": "arXiv:2601.21057v1 Announce Type: new \nAbstract: The recent success of deep learning (DL) has enabled the generation of high-quality synthetic gaze data. However, such data also raises privacy concerns because gaze sequences can encode subjects' internal states, like fatigue, emotional load, or stress. Ideally, synthetic gaze should preserve the signal quality of real recordings and remove or attenuate state-related, privacy-sensitive attributes. Many recent DL-based generative models focus on replicating real gaze trajectories and do not explicitly consider subjective reports or the privatization of internal states. However, in this work, we consider a recent diffusion-based gaze synthesis approach and examine correlations between synthetic gaze features and subjective reports (e.g., fatigue and related self-reported states). Our result shows that these correlations are trivial, which suggests the generative approach suppresses state-related features. Moreover, synthetic gaze preserves necessary signal characteristics similar to those of real data, which supports its use for privacy-preserving gaze-based applications.",
      "author": "Kamrul Hasan, Oleg V. Komogortsev",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 149,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784425+00:00",
      "updated_at": "2026-01-30T05:32:55.784427+00:00"
    },
    {
      "id": "9c09b5810ce545ec2e92c005ff978205",
      "url": "https://arxiv.org/abs/2601.21045",
      "title": "Eye Feel You: A DenseNet-driven User State Prediction Approach",
      "content": "arXiv:2601.21045v1 Announce Type: new \nAbstract: Subjective self-reports, collected with eye-tracking data, reveal perceived states like fatigue, effort, and task difficulty. However, these reports are costly to collect and challenging to interpret consistently in longitudinal studies. In this work, we focus on determining whether objective gaze dynamics can reliably predict subjective reports across repeated recording rounds in the eye-tracking dataset. We formulate subjective-report prediction as a supervised regression problem and propose a DenseNet-based deep learning regressor that learns predictive representations from gaze velocity signals. We conduct two complementary experiments to clarify our aims. First, the cross-round generalization experiment tests whether models trained on earlier rounds transfer to later rounds, evaluating the models' ability to capture longitudinal changes. Second, cross-subject generalization tests models' robustness by predicting subjective outcomes for new individuals. These experiments aim to reduce reliance on hand-crafted feature designs and clarify which states of subjective experience systematically appear in oculomotor behavior over time.",
      "author": "Kamrul Hasan, Oleg V. Komogortsev",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 153,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784395+00:00",
      "updated_at": "2026-01-30T05:32:55.784397+00:00"
    },
    {
      "id": "68b042cc462770bccc9bd90aa99d3b9d",
      "url": "https://arxiv.org/abs/2601.21043",
      "title": "Log2Motion: Biomechanical Motion Synthesis from Touch Logs",
      "content": "arXiv:2601.21043v1 Announce Type: new \nAbstract: Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.",
      "author": "Micha{\\l} Patryk Miazga, Hannah Bussmann, Antti Oulasvirta, Patrick Ebel",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 165,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784363+00:00",
      "updated_at": "2026-01-30T05:32:55.784365+00:00"
    },
    {
      "id": "c6af44dce922dccb605a9dd0a78d2822",
      "url": "https://arxiv.org/abs/2601.21001",
      "title": "Designing the Interactive Memory Archive (IMA): A Socio-Technical Framework for AI-Mediated Reminiscence and Cultural Memory Preservation",
      "content": "arXiv:2601.21001v1 Announce Type: new \nAbstract: This paper introduces the Interactive Memory Archive (IMA), a conceptual framework for AI-mediated reminiscence designed to support cognitive en-gagement among older adults experiencing memory loss. IMA integrates multimodal sensing, natural language conversational scaffolding, and cloud-based archiving within the familiar form of a large format historical picture book. The model theorizes reminiscence as a guided, context-aware interaction eliciting autobiographical memories and preserving them as cul-tural artifacts. The paper positions IMA as a theoretical contribution, articu-lates testable propositions, and outlines a research agenda for future empiri-cal, technical, and ethical inquiry.",
      "author": "Ron Fulbright",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 93,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:55.784322+00:00",
      "updated_at": "2026-01-30T05:32:55.784327+00:00"
    },
    {
      "id": "7255dad355a9f8572363f6f223718af3",
      "url": "https://arxiv.org/abs/2511.10835",
      "title": "What the flock knows that the birds do not: exploring the emergence of joint agency in multi-agent active inference",
      "content": "arXiv:2511.10835v2 Announce Type: replace-cross \nAbstract: Collective behavior pervades biological systems, from flocks of birds to neural assemblies and human societies. Yet, how such collectives acquire functional properties -- such as joint agency or knowledge -- that transcend those of their individual components remains an open question. Here, we combine active inference and information-theoretic analyses to explore how a minimal system of interacting agents can give rise to joint agency and collective knowledge. We model flocking dynamics using multiple active inference agents, each minimizing its own free energy while coupling reciprocally with its neighbors. We show that as agents self-organize, their interactions define higher-order statistical boundaries (Markov blankets) enclosing a ``flock'' that can be treated as an emergent agent with its own sensory, active, and internal states. When exposed to external perturbations (a ``predator''), the flock exhibits faster, coordinated responses than individual agents, reflecting collective sensitivity to environmental change. Crucially, analyses of synergistic information reveal that the flock encodes information about the predator's location that is not accessible to every individual bird, demonstrating implicit collective knowledge. Together, these results show how informational coupling among active inference agents can generate new levels of autonomy and inference, providing a framework for understanding the emergence of (implicit) collective knowledge and joint agency.",
      "author": "Domenico Maisto, Davide Nuzzi, Giovanni Pezzulo",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 208,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:54.473756+00:00",
      "updated_at": "2026-01-30T05:32:54.473758+00:00"
    },
    {
      "id": "17ae4de26b585daa5ec226f54aeb244b",
      "url": "https://arxiv.org/abs/2512.21768",
      "title": "Numerical Twin with Two Dimensional Ornstein--Uhlenbeck Processes of Transient Oscillations in EEG signal",
      "content": "arXiv:2512.21768v2 Announce Type: replace \nAbstract: Stochastic burst-like oscillations are common in physiological signals, yet there are few compact generative models that capture their transient structure. We propose a numerical-twin framework that represents transient narrowband activity as a two-dimensional Ornstein-Uhlenbeck (OU) process with three interpretable parameters: decay rate, mean frequency, and noise amplitude. We develop two complementary estimation strategies. The first fits the power spectral density, amplitude distribution, and autocorrelation to recover OU-parameters. The second segments burst events and performs a statistical match between empirical spindle statistics (duration, amplitude, inter-event interval) and simulated OU output via grid search, resolving parameter degeneracies by including event counts. We extend the framework to multiple frequency bands and piecewise-stationary dynamics to track slow parameter drifts. Applied to electroencephalography (EEG) recorded during general anesthesia, the method identifies OU models that reproduce alpha-spindle (8-12 Hz) morphology and band-limited spectra with low residual error, enabling real-time tracking of state changes that are not apparent from band power alone. This decomposition yields a sparse, interpretable representation of transient oscillations and provides interpretable metrics for brain monitoring.",
      "author": "P. O. Michel, C. Sun, S. Jaffard, D. Longrois, D. Holcman",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 177,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:54.473723+00:00",
      "updated_at": "2026-01-30T05:32:54.473725+00:00"
    },
    {
      "id": "599d9582cef13fb736ce8672f3a51cf1",
      "url": "https://arxiv.org/abs/2511.09506",
      "title": "A thermoinformational formulation for the description of neuropsychological systems",
      "content": "arXiv:2511.09506v3 Announce Type: replace \nAbstract: Complex systems produce high-dimensional signals that lack macroscopic variables analogous to entropy, temperature, or free energy. This work introduces a thermoinformational formulation that derives entropy, internal energy, temperature, and Helmholtz free energy directly from empirical microstate distributions of arbitrary datasets. The approach provides a data-driven description of how a system reorganizes, exchanges information, and moves between stable and unstable states. Applied to dual-EEG recordings from mother-infant dyads performing the A-not-B task, the formulation captures increases in informational heat during switches and errors, and reveals that correct choices arise from more stable, low-temperature states. In an independent optogenetic dam-pup experiment, the same variables separate stimulation conditions and trace coherent trajectories in thermodynamic state space. Across both human and rodent systems, this thermoinformational formulation yields compact and physically interpretable macroscopic variables that generalize across species, modalities, and experimental paradigms.",
      "author": "George-Rafael Domenikos, Victoria Leong",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 142,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:54.473692+00:00",
      "updated_at": "2026-01-30T05:32:54.473693+00:00"
    },
    {
      "id": "e9ab4b4e2ac57d0881e8d0b148ac3c61",
      "url": "https://arxiv.org/abs/2505.01098",
      "title": "Models of attractor dynamics in the brain",
      "content": "arXiv:2505.01098v2 Announce Type: replace \nAbstract: Attractor dynamics are a fundamental computational motif in neural circuits, supporting diverse cognitive functions through stable, self-sustaining patterns of neural activity. In these lecture notes, we review four key examples that demonstrate how autoassociative neural network models can elucidate the computational mechanisms underlying attractor-based information processing in biological neural systems performing cognitive functions. Drawing on empirical evidence, we explore hippocampal spatial representations, visual classification in the inferotemporal cortex, perceptual adaptation and priming, and working-memory biases shaped by sensory history. Across these domains, attractor network models reveal common computational principles and provide analytical insights into how experience shapes neural activity and behavior. Our synthesis underscores the value of attractor models as powerful tools for probing the neural basis of cognition and behavior.",
      "author": "Tala Fakhoury, Elia Turner, Sushrut Thorat, Athena Akrami",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 126,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:54.473663+00:00",
      "updated_at": "2026-01-30T05:32:54.473665+00:00"
    },
    {
      "id": "5cb26bcb09d2668dfb0ddd72a18e6959",
      "url": "https://arxiv.org/abs/2601.21407",
      "title": "BrainFuse: a unified infrastructure integrating realistic biological modeling and core AI methodology",
      "content": "arXiv:2601.21407v1 Announce Type: cross \nAbstract: Neuroscience and artificial intelligence represent distinct yet complementary pathways to general intelligence. However, amid the ongoing boom in AI research and applications, the translational synergy between these two fields has grown increasingly elusive-hampered by a widening infrastructural incompatibility: modern AI frameworks lack native support for biophysical realism, while neural simulation tools are poorly suited for gradient-based optimization and neuromorphic hardware deployment. To bridge this gap, we introduce BrainFuse, a unified infrastructure that provides comprehensive support for biophysical neural simulation and gradient-based learning. By addressing algorithmic, computational, and deployment challenges, BrainFuse exhibits three core capabilities: (1) algorithmic integration of detailed neuronal dynamics into a differentiable learning framework; (2) system-level optimization that accelerates customizable ion-channel dynamics by up to 3,000x on GPUs; and (3) scalable computation with highly compatible pipelines for neuromorphic hardware deployment. We demonstrate this full-stack design through both AI and neuroscience tasks, from foundational neuron simulation and functional cylinder modeling to real-world deployment and application scenarios. For neuroscience, BrainFuse supports multiscale biological modeling, enabling the deployment of approximately 38,000 Hodgkin-Huxley neurons with 100 million synapses on a single neuromorphic chip while consuming as low as 1.98 W. For AI, BrainFuse facilitates the synergistic application of realistic biological neuron models, demonstrating enhanced robustness to input noise and improved temporal processing endowed by complex HH dynamics. BrainFuse therefore serves as a foundational engine to facilitate cross-disciplinary research and accelerate the development of next-generation bio-inspired intelligent systems.",
      "author": "Baiyu Chen, Yujie Wu, Siyuan Xu, Peng Qu, Dehua Wu, Xu Chu, Haodong Bian, Shuo Zhang, Bo Xu, Youhui Zhang, Zhengyu Ma, Guoqi Li",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 240,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:54.473635+00:00",
      "updated_at": "2026-01-30T05:32:54.473637+00:00"
    },
    {
      "id": "a291f45f7501f458418412fb7c7a5897",
      "url": "https://arxiv.org/abs/2601.21508",
      "title": "How 'Neural' is a Neural Foundation Model?",
      "content": "arXiv:2601.21508v1 Announce Type: new \nAbstract: Foundation models have shown remarkable success in fitting biological visual systems; however, their black-box nature inherently limits their utility for understanding brain function. Here, we peek inside a SOTA foundation model of neural activity (Wang et al., 2025) as a physiologist might, characterizing each 'neuron' based on its temporal response properties to parametric stimuli. We analyze how different stimuli are represented in neural activity space by building decoding manifolds, and we analyze how different neurons are represented in stimulus-response space by building neural encoding manifolds. We find that the different processing stages of the model (i.e., the feedforward encoder, recurrent, and readout modules) each exhibit qualitatively different representational structures in these manifolds. The recurrent module shows a jump in capabilities over the encoder module by 'pushing apart' the representations of different temporal stimulus patterns. Our 'tubularity' metric quantifies this stimulus-dependent development of neural activity as biologically plausible. The readout module achieves high fidelity by using numerous specialized feature maps rather than biologically plausible mechanisms. Overall, this study provides a window into the inner workings of a prominent neural foundation model, gaining insights into the biological relevance of its internals through the novel analysis of its neurons' joint temporal response patterns. Our findings suggest design changes that could bring neural foundation models into closer alignment with biological systems: introducing recurrence in early encoder stages, and constraining features in the readout module.",
      "author": "Johannes Bertram, Luciano Dyballa, Anderson Keller, Savik Kinger, Steven W. Zucker",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 235,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:54.473596+00:00",
      "updated_at": "2026-01-30T05:32:54.473598+00:00"
    },
    {
      "id": "c80500df0cd4fa6301a3fe26585cb51d",
      "url": "https://arxiv.org/abs/2601.21478",
      "title": "Differential Dynamic Causal Nets: Model Construction, Identification and Group Comparisons",
      "content": "arXiv:2601.21478v1 Announce Type: new \nAbstract: Pathophysiolpgical modelling of brain systems from microscale to macroscale remains difficult in group comparisons partly because of the infeasibility of modelling the interactions of thousands of neurons at the scales involved. Here, to address the challenge, we present a novel approach to construct differential causal networks directly from electroencephalogram (EEG) data. The proposed network is based on conditionally coupled neuronal circuits which describe the average behaviour of interacting neuron populations that contribute to observed EEG data. In the network, each node represents a parameterised local neural system while directed edges stand for node-wise connections with transmission parameters. The network is hierarchically structured in the sense that node and edge parameters are varying in subjects but follow a mixed-effects model. A novel evolutionary optimisation algorithm for parameter inference in the proposed method is developed using a loss function derived from Chen-Fliess expansions of stochastic differential equations. The method is demonstrated by application to the fitting of coupled Jansen-Rit local models. The performance of the proposed method is evaluated on both synthetic and real EEG data. In the real EEG data analysis, we track changes in the parameters that characterise dynamic causality within brains that demonstrate epileptic activity. We show evidence of network functional disruptions, due to imbalance of excitatory-inhibitory interneurons and altered epileptic brain connectivity, before and during seizure periods.",
      "author": "Kang You, Gary Green, Jian Zhang",
      "published_date": "2026-01-30T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 224,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:54.473554+00:00",
      "updated_at": "2026-01-30T05:32:54.473559+00:00"
    },
    {
      "id": "98f1c52476c6630480367243d74c05fd",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.27.701765v1?rss=1",
      "title": "Prior Scene Context Shapes the Neural Dynamics of Face Detection",
      "content": "Detecting faces in our surroundings typically only takes a fraction of a second. How can such a rapid perceptual process still be influenced by prior expectations? Here, we used electroencephalography (EEG) to investigate how prior scene context modulates the temporal dynamics of neural face representations. Participants viewed natural scenes containing a single face (left or right), each preceded by either a faceless version of the same scene (preview condition) or a gray screen (no-preview condition), while performing a face detection task. Using multivariate decoding, we found that face location could be decoded shortly after target onset. Critically, decoding accuracy was higher in the preview condition at early stages, whereas the no-preview condition showed higher decoding at later time points, suggesting rapid facilitation by prior context followed by compensatory processing when contextual information was absent. In contrast, time-frequency decoding revealed a sustained preview advantage across alpha, beta, and gamma bands, even during time periods when evoked responses favored the no-preview condition. This dissociation between evoked and induced neural signals indicates that prior scene context engages distinct neural processes: early evoked activity reflects rapid contextual facilitation of sensory representations, while induced oscillatory activity may support prolonged context-dependent modulation. Together, these results show how prior scene context and sensory-driven processing jointly shape rapid face perception through temporally and spectrally distinct neural dynamics.",
      "author": "Tasliyurt-Celebi, S., Kaiser, D., Dobs, K.",
      "published_date": "2026-01-29T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 219,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:44.721822+00:00",
      "updated_at": "2026-01-30T05:32:44.721824+00:00"
    },
    {
      "id": "db387ffb496999d909252bdd0a53e1a4",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.27.701964v1?rss=1",
      "title": "The activation of CT fibers or affective touch does not affect pain sensitization by secondary hyperalgesia",
      "content": "Interpersonal affective touch, that preferentially engages C-tactile (CT) afferents, has been shown to produce analgesic effects, yet its role in pain sensitization remains poorly understood. This study explores whether touch delivered at CT-optimal parameters, either artificial or interpersonal, modulates secondary hyperalgesia (SH) induced by high-frequency stimulation (HFS). Two experimental studies were conducted. In Study 1, 46 participants underwent SH induction during two conditions: robot stroking at CT-optimal velocities and vibrotactile stimulation. In Study 2, 64 participants (32 couples) experienced SH in two separate sessions: alone or accompanied by their partner delivering affective touch. Pain reports, electroencephalographic activity (N-P complex and time-frequency activity), electrocardiographic (ECG) and electrodermal activity data (EDA) were collected. HFS successfully established SH in both studies; however, no significant differences were found in the SH area between CT-stimulation and control conditions. In Study 2, partner-delivered affective touch significantly reduced reported acute pain during HFS compared to the alone condition, whereas no such effect was observed in Study 1 (robotic vs. vibrotactile). At the neural level, no condition effects were observed in the N-P complex, EEG time-frequency data and ECG indexes in either study. Tonic EDA after HFS was higher when participants received stroking from their romantic partner compared to being alone. No sex differences were observed. Overall, while affective touch from a romantic partner may reduce acute pain during nociceptive stimulation -an effect not observed with robotic CT-stimulation- neither robotic nor interpersonal affective touch appeared to modulate the development of secondary hyperalgesia.",
      "author": "da-Silva, M., Ribeiro-Carreira, A., Oliveira, M., Sampaio, A., Coutinho, J., Gonzalez-Villar, A. J.",
      "published_date": "2026-01-29T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 244,
      "reading_time": 1,
      "created_at": "2026-01-30T05:32:44.721778+00:00",
      "updated_at": "2026-01-30T05:32:44.721783+00:00"
    }
  ]
}