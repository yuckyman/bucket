{
  "last_updated": "2025-09-30T08:29:12.383373+00:00",
  "count": 20,
  "articles": [
    {
      "id": "a07bb58009d0c700eba383662b3fed57",
      "url": "http://daniellakens.blogspot.com/2025/09/type-s-and-m-errors-as-rhetorical-tool.html",
      "title": "Type S and M errors as a \u201crhetorical tool\u201d",
      "content": "<p><i>Update 30/09/2025: I have added a reply by Andrew Gelman below my original blog post.</i>&nbsp;</p><p>We recently\nposted a preprint criticizing the idea of Type S and M errors (<a href=\"https://osf.io/2phzb_v1\">https://osf.io/2phzb_v1</a>). From our abstract:\n\u201cWhile these concepts have been proposed to be useful both when designing a\nstudy (prospective) and when evaluating results (retroactive), we argue that\nthese statistics do not facilitate the proper design of studies, nor the\nmeaningful interpretation of results.\u201d</p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">In a recent\n<a href=\"https://statmodeling.stat.columbia.edu/2025/09/25/on-the-poor-statistical-properties-of-the-p-curve-meta-analytic-procedure/#comment-2403979\" target=\"_blank\">blog post</a> that is mainly on p-curve analysis, Gelman writes briefly about Type\nS and M errors, stating that he does not see them as tools that should be used\nregularly, but that they mainly function as a \u2018rhetorical tool\u2019:</span></p>\n\n<p class=\"MsoNormal\"><i><span lang=\"EN-US\">I offer\na three well-known examples of statistical ideas arising in the field of\nscience criticism, three methods whose main value is rhetorical:</span></i></p>\n\n<p class=\"MsoNormal\"><i><span lang=\"EN-US\">[\u2026]</span></i></p>\n\n<p class=\"MsoNormal\"><i><span lang=\"EN-US\">2. The\nconcepts of Type M and Type S errors, which I developed with Francis Tuerlinckx\nin 2000 and John Carlin in 2014. This has been an influential idea\u2013ok, not as\ninfluential as Ioannidis\u2019s paper!\u2013and I like it a lot, but it doesn\u2019t\ncorrespond to a method that I will typically use in practice. To me, the value\nof the concepts of Type M and Type S errors is they help us understand certain\nexisting statistical procedures, such as selection on statistical significance,\nthat have serious problems. There\u2019s mathematical content here for sure, but I\nfundamentally think of these error calculations as having rhetorical value for\nthe design of studies and interpretation of reported results.</span></i></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">The main\nsentence of interest here is that Gelman says this is not a method he would use\nin practice. I was surprised, because in their article Gelman and Carlin (2014)\nrecommend the calculation of Type S and M errors more forcefully: \u201cWe suggest\nthat design calculations be performed after as well as before data collection\nand analysis.\u201d Throughout their article, they compare design calculations where\nType S and M errors are calculated to power analyses, which are widely seen as\na requirement before data collection of any hypothesis testing study. For\nexample, in the abstract they write \u201cpower analysis is flawed in that a narrow\nemphasis on statistical significance is placed as the primary focus of study\ndesign. In noisy, small-sample settings, statistically significant results can\noften be misleading. To help researchers address this problem in the context of\ntheir own studies, we recommend design calculations\u201d.</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">They also say\ndesign calculations are useful when interpreting results, and that they add\nsomething to p-values and effect sizes, which again seems to suggest they can complement\nordinary data analysis: \u201cOur retrospective analysis provided useful insight, beyond\nwhat was revealed by the estimate, confidence interval, and p value that came\nfrom the original data summary.\u201d (Gelman &amp; Carlin, 2014, p. 646). In\ngeneral, they seem to suggest design analyses are done before or after data analysis:\n\u201cFirst, it is indeed preferable to do a design analysis ahead of time, but a\nresearcher can analyze data in many different ways\u2014indeed, an important part of\ndata analysis is the discovery of unanticipated patterns (Tukey, 1977) so that it\nis unreasonable to suppose that all potential analyses could have been\ndetermined ahead of time. The second reason for performing postdata design\ncalculations is that they can be a useful way to interpret the results from a\ndata analysis, as we next demonstrate in two examples.\u201d (Gelman &amp; Carlin,\n2014, p. 643).</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">One the other\nhand, in a single sentence in the discussion, they also write: \u201cOur goal in\ndeveloping this software is not so much to provide a tool for routine use but\nrather to demonstrate that such calculations are possible and to allow\nresearchers to play around and get a sense of the sizes of Type S errors and Type\nM errors in realistic data settings.\u201d</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Maybe I\nhave always misinterpreted Gelman and Carlin, 2014, in that I took it as a\npaper that recommended the regular use of Type S and M errors, and I should\nhave understood that the sentence in the discussion made it clear that this was\nnever their intention. If the idea is to replace Type 1 and 2 errors, and\nhence, replace power analysis and the interpretation of data, design analysis\nshould be part of every hypothesis testing study. Sentences such as \u201cthe\nrequirement of design analysis can stimulate engagement with the existing literature\nin the subject-matter field\u201d seemed to suggest to me that design analyses could\nbe a requirement for all studies. But maybe I was wrong. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Or maybe I\nwasn\u2019t. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">In this <a href=\"https://statmodeling.stat.columbia.edu/2016/11/13/more-on-my-paper-with-john-carlin-on-type-m-and-type-s-errors/\">blog\npost</a>, Gelman writes: \u201cNow, one odd thing about my paper with Carlin is that\nit gives some tools that I recommend others use when designing and evaluating\ntheir research, but I would not typically use these tools directly myself!\nBecause I am not wanting to summarize inference by statistical significance.\u201d So,\nhere there seems to be the idea that others routinely use Type S and M errors. And\nin a very early version of the paper with Carlin, available <a href=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2016/11/retropower.pdf\">here</a>,\nthe opening sentence also suggests routine use: \u201cThe present article proposes\nan ideal that every statistical analysis be followed up with a power\ncalculation to better understand the inference from the data. As the quotations\nabove illustrate, however, our suggestion contradicts the advice of many\nrespected statisticians. Our resolution of this apparent disagreement is that\nwe perform retrospective power analysis in a different way and for a different\npurpose than is typically recommended in the literature.\u201d</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Of course,\none good thing about science is that people change their beliefs about things.\nMaybe Gelman one time thought Type S and M errors should be part of \u2018every\nstatistical analysis\u2019 but now sees the tool mainly as a \u2018rhetorical device\u2019. And\nthat is perfectly fine. It is also good to know, because I regular see people\nwho suggest that Type S and M error should routinely be used in practice. I\nguess I can now point them to a blog post where Gelman himself disagrees with\nthat suggestion.</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">As we explain\nin our preprint, the idea of Type S errors is conceptually incoherent, and any\nprobabilities calculated will be identical to the Type 1 error in directional\ntests, or the false discovery rate, as all that Type S errors do is remove the\npossibility of an effect being 0 from the distribution, but this probability is\nitself 0. We also explain how other tools are better to educate researchers\nabout effect size inflation in studies selected for significance (for which\nGelman would recommend Type M errors), and we actually recommend p-uniform for\nthis, or just teaching people about critical effect sizes.</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Personally,\nI don\u2019t like rhetorical tools. Although in our preprint we agree that teaching\nthe idea of Type S and M errors can be useful in education, there are also conceptually\ncoherent and practically useful statistical ideas that we can teach instead to\nachieve the same understanding. Rhetorical tools might be useful to convince\npeople who do not think logically about a topic, but I prefer to have a\nslightly higher bar for the scientists that I aim to educate about good\nresearch practices, and I think they are able to understand the problem of low\nstatistical power and selection bias without rhetorical tools. </span></p><p class=\"MsoNormal\"><span lang=\"EN-US\"><br /></span></p><div>--Reply by Andrew Gelman--</div><p class=\"MsoNormal\"><span lang=\"EN-US\"></span></p><div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">Hi, Daniel. &nbsp;Thanks for your comments. &nbsp;It's always good to see that people are reading our articles and blog posts. &nbsp;I think you are a little bit confused about what we wrote, but ultimately that's our fault for not being clear, so I appreciate the opportunity to clarify.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">So you don't need to consider this comment as a \"rebuttal\" to your post. &nbsp;For convenience I'll go through several of your statements one by one, but my goal is to clarify.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">First, I guess I should've avoided the word \"rhetorical.\" &nbsp;In my post, I characterized Ioannidis's 2005 claim, type M and S errors, and multiverse analysis as \"rhetorical tools\" that have been been useful in the field of science criticism but which I would not use in my own analyses. &nbsp;I could've added to this many other statistical methods including p-values and Bayes factors.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">When I describe a statistical method as \"rhetorical\" in this context, I'm not saying it's mathematically invalid or that it's conceptually incoherent (to use your term), nor am I saying these methods should not be used! &nbsp;All these tools can be useful; they just rely on very strong assumptions. &nbsp;P-values and Bayes factors are measures of evidence relative to a null hypothesis (not just an assumption that a particular parameter equals zero, but an entire set of assumptions about the data-generating process) that is irrelevant in the science and decision problems I've seen--but these methods are clearly defined and theoretically justified, and many practitioners get a lot out of them. &nbsp;I very rarely would use p-values or Bayes factors in my work because I'm very rarely interested in this sort of discrepancy from a null hypothesis.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">A related point comes up in my paper with Hill and Yajima, \"Why we (usually) don't have to worry about multiple comparisons\" (https://sites.stat.columbia.edu/gelman/research/published/multiple2f.pdf). &nbsp;Multiple comparisons corrections can be important, indeed I've criticized some published work for misinterpreting evidence by not accounting for multiple comparisons or multiple potential comparisons--but it doesn't come up so much in the context of multilevel modeling.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">Ioannidis (2005) is a provocative paper that I think has a lot of value--but you have to be really careful to try to directly apply such an analysis to real data. &nbsp; He's making some really strong assumptions! &nbsp;The logic of his paper is clear, though. &nbsp;O'Rourke and I discuss the challenges of moving from that sort of model to larger conclusions in our 2013 paper (https://sites.stat.columbia.edu/gelman/research/published/GelmanORourkeBiostatistics.pdf).</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">The multiverse is a cool idea, and researchers have found it to be useful. &nbsp;The sociologists Cristobal Young and Erin Cumberworth recently published a book on it (https://www.cambridge.org/core/books/multiverse-analysis/D53C3AB449F6747B4A319174E5C95FA1). &nbsp;I don't think I'd apply the method in my own applied research, though, because the whole idea of the multiverse is to consider all the possible analyses you might have done on a dataset, and if I get to that point I'm more inclined to fit a multilevel model that subsumes all these analyses. &nbsp;I have found multiverse analysis to be useful in understanding research published by others, and maybe it would be useful for my own work too, given that my final published analyses never really include all the possibilities of what I might have done. &nbsp;The point is that this is yet another useful method that can have conceptual value even if I might not apply it to my own work. &nbsp;Again, the term \"rhetorical\" might be misleading, as these are real methods that, like all statistical methods, are appropriate in some settings and not in others.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">So please don't let your personal dislike of the term \"rhetorical tools\" to dissuade you from taking seriously the tools that I happen to have characterized as \"rhetorical,\" as these include p-values, multiple comparisons corrections, Bayesian analysis with point priors, and all sorts of other methods that are rigorously defined and can be useful in many applied settings, including some of yours!</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">OK, now on to Type M and Type S errors. &nbsp;You seem to imply that at some time I thought that these \"should be part of \u2018every statistical analysis,'\" but I can assure you that I have never believed or written such a thing. &nbsp;You put the phrase \"every statistical analysis,\" but this is your phrase, not mine.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">One very obvious way to see that I never thought Type M and Type S errors \"should be part of \u2018every statistical analysis'\" is that, since the appearance of that article in 2014, I've published dozens of applied papers, and in only very few of these did I look at Type M and Type S errors.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">What is that? &nbsp;Why is it that my colleagues and I came up with this idea that has been influential, and which I indeed think can be very useful and which I do think should often be used by practitioners, but I only use it myself?</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">The reason is that the focus of our work on Type M and Type S errors has been to understand selection on statistical significance (as in that notorious estimate that early childhood intervention increases adult earnings by 42% on average, but with that being the result of an inferential procedure that, under any reasonable assumptions, greatly overestimates the magnitude of any real effect; that is, Type M error). &nbsp;In my applied work it's very rare that I condition on statistical significance, and so this sort of use of Type M and S errors is not so relevant. &nbsp;So it's perfectly coherent for me to say that Type M and S error analysis is valuable in a wide range of settings that that I think these tools should be applied very widely, without believing that they should be part of \"every statistical analysis\" or that I should necessarily use them for my own analyses.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">That said, more recently I've been thinking that Type M and S errors are a useful approach to understanding statistical estimates more generally, not just for estimates that are conditioned on statistical significance. &nbsp;I'm working with Erik van Zwet and Witold Wi\u0119cek on applying these ideas to Bayesian inferences as well. &nbsp;So I'm actually finding these methods to be more, not less, valuable for statistical understanding, and not just for \"people who do not think logically about a topic\" (in your phrasing). &nbsp;Our papers on these topics are published in real journals and of course they're intended for people who &lt;em&gt;do&lt;/em&gt; think logically about the topic! &nbsp;And, just to be clear, I believe that you're thinking logically in your post too; I just think you've been misled by my terminology (again, I accept the blame for that), and also you work on different sorts of problems than I do, so it makes sense that a method that I find useful might not be so helpful to you. &nbsp;There are many ways to Rome, which is another point I was making in that blog post.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">Finally, a few things in your post that I did not address above:</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">1. &nbsp;You quote from my blog post, where I wrote, \u201cNow, one odd thing about my paper with Carlin is that it gives some tools that I recommend others use when designing and evaluating their research, but I would not typically use these tools directly myself! Because I am not wanting to summarize inference by statistical significance.\u201d &nbsp;That's exactly my point above! &nbsp;You had it right there.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">2. &nbsp;You wrote, \"Maybe I have always misinterpreted Gelman and Carlin, 2014, in that I took it as a paper that recommended the regular use of Type S and M errors, and I should have understood that the sentence in the discussion made it clear that this was never their intention.\" &nbsp;So, just to clarify, yes in our paper we recommended the regular use of Type M and S errors, and we still recommend that!</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">3. &nbsp;You write that our \"sentences such as 'the requirement of design analysis can stimulate engagement with the existing literature in the subject-matter field' seemed to suggest to me that design analyses could be a requirement for all studies.\" &nbsp;That's right--I actually do think that design analysis should be done for all studies!</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">OK, nothing is done all the time. &nbsp;I guess that some studies are so cheap that there's no need for a design analysis--or maybe we could say that in such studies the design analysis is implicit. &nbsp;For example, if I'm doing A/B testing in a company, and they've done lots of A/B tests before, and I think the new effect will be comparable to previous things being studied, then maybe I just go with the same design as in previous experiments, without performing a formal design analysis. &nbsp;But one could argue that this corresponds to some implicit calculation.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">In any case, yeah, in general I think that a design analysis should come before any study. &nbsp;Indeed, that is what I tell students and colleagues: &nbsp;never collect data before doing a simulation study first. &nbsp;Often we do fake-data simulation after the data come in, to validate our model-fitting strategies, but for a while I've been thinking it's best to do it before.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">This is not controversial advice in statistics, to recommend a design analysis before gathering data! &nbsp;Indeed, in medical research it's basically a requirement. &nbsp;In our paper, Carlin and I argue--and I still believe--that a design analysis using Type M and S errors is more valuable than the traditional Type 1 and 2 errors. &nbsp;But in any case I consider \"design analysis\" to be the general term, with \"power analysis\" being a special case (design analysis looking at the probability of attaining statistical significance). &nbsp;I don't think traditional power analysis is useless--one way you can see this is that we demonstrate power calculations in chapter 16 of Regression and Other Stories, a book that came out several years after my paper with Carlin--; I just think it can be misleading, especially if it is done without consideration of Type M and S errors.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">Thanks again for your comments. &nbsp;It's good to have an opportunity to clarify my thinking, and these are important issues in statistics.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">P.S. &nbsp;If you see something on our blog that you disagree with, feel free to comment there directly, as that way you can also reach readers of the original post.</div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\"><br /></div><div style=\"border: 0px; color: inherit; font: inherit; margin: 0px; padding: 0px; vertical-align: baseline;\">--</div></div><p class=\"MsoNormal\"><span lang=\"EN-US\">References:&nbsp;</span></p><p class=\"MsoNormal\"><span lang=\"EN-US\"></span></p>Lakens, D., Cristian, Xavier-Quintais, G., Rasti, S., Toffalini, E., &amp; Alto\u00e8, G. (2025). Rethinking Type S and M Errors. OSF. <a href=\"https://doi.org/10.31234/osf.io/2phzb_v1\">https://doi.org/10.31234/osf.io/2phzb_v1</a><br /> <br />Gelman, A., &amp; Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641\u2013651. <a href=\"https://doi.org/10.1177/1745691614551642\">https://doi.org/10.1177/1745691614551642</a>",
      "author": "noreply@blogger.com (Daniel Lakens)",
      "published_date": "2025-09-28T05:19:00+00:00",
      "source": "Twenty Percent Statistician",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 3572,
      "reading_time": 17,
      "created_at": "2025-09-30T08:29:06.803584+00:00",
      "updated_at": "2025-09-30T08:29:06.803590+00:00"
    },
    {
      "id": "3de9e2508b224b40b03dc041f64ceb44",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925004720?dgcid=rss_sd_all",
      "title": "Neural mechanisms of emotion-focused interventions: A meta-analytic review of fMRI studies",
      "content": "<p>Publication date: 15 October 2025</p><p><b>Source:</b> NeuroImage, Volume 320</p><p>Author(s): Yanlin Li, Geng Li, Yang Liu, Chengzhen Liu, Antao Chen</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 18,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:43.327599+00:00",
      "updated_at": "2025-09-30T08:28:43.327600+00:00"
    },
    {
      "id": "385714185a4fcfb034800342db351fdb",
      "url": "https://www.biorxiv.org/content/10.1101/2025.09.29.679390v1?rss=1",
      "title": "Single-Neuron Encoding of Learnability in the Dorsal Anterior Cingulate Cortex",
      "content": "In natural environments, associations that indicate true learnable regularities are intermixed with those that arise from random and ultimately unlearnable relationships between events. To efficiently allocate cognitive resources and avoid inferring spurious patterns, organisms must distinguish learnable from unlearnable associations, but the mechanisms underlying this ability are not understood. We recently showed that monkeys performing a transitive inference task, while discovering the true hidden order in learnable image sets, also behaved to varying degrees as if they inferred subjective order in objectively random (unlearnable) image sets. Here, we show that the ability to detect learnability is encoded by neurons in the dorsal anterior cingulate cortex (dACC, area 24c). dACC neurons responded strongly after a decision outcome as reported in previous studies and, additionally, signaled whether a trial was from a learnable vs unlearnable set before outcome delivery, and showed interactions whereby their selectivity for the outcome (reward vs lack of reward) was stronger for learnable versus unlearnable sets. Learnability and interaction responses were independent of sensory or reward cues (which were equated for learnable and unlearnable sets) but their strength correlated with the monkeys ability to avoid inferring false order in unlearnable sets. The findings suggest that the dACC is part of a network that monitors learnability and enables animals to appropriately focus learning on true patterns while avoiding false inferences about spurious and random associations.",
      "author": "Jin, Y., Jensen, G., Ferrera, V., Gottlieb, J.",
      "published_date": "2025-09-30T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 226,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:42.188525+00:00",
      "updated_at": "2025-09-30T08:28:42.188526+00:00"
    },
    {
      "id": "40fd83b47e56d20b3b06ad94bf37203d",
      "url": "https://www.biorxiv.org/content/10.1101/2025.09.29.679277v1?rss=1",
      "title": "Edge communities in functional brain networks reveal heterogeneous, overlapping organization across the human lifespan",
      "content": "Understanding changes in functional brain organization and their implications in development and aging is one of the central questions in neuroscience. In this study, we used an edge-centric approach to examine cross-sectional differences in functional brain network organization across the human lifespan using resting state functional MRI data from the Nathan Kline Institute - Rockland Sample dataset. By creating edge time series - a framewise multiplication of nodal time series - and clustering them based on their temporal similarities, we were able to identify clusters of edges instead of nodes. This method naturally allows multiple community affiliations per node (brain region), providing a nuanced perspective on network participation compared to conventional hard-partition approaches. To do so, we created age-neutral templates of edge communities - or \"eFC lures\" - that, when applied, yielded consistent edge communities across non-overlapping subsamples of data. The communities of edges revealed a trajectory of desegregation with aging, suggested to be linked to neural dedifferentiation of activity and cognitive decline in older adults. Additionally, age group-specific lures significantly enhanced the detection of edge community organization compared to the age-neutral version. Combined, these results offer new insights into the heterogeneous, event cluster-level shifts in brain functional organization as well as underscore the importance of age-targeted analytical frameworks throughout the human lifespan.",
      "author": "Jo, Y., Chumin, E. J., Betzel, R.",
      "published_date": "2025-09-30T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 213,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:42.188487+00:00",
      "updated_at": "2025-09-30T08:28:42.188488+00:00"
    },
    {
      "id": "e52876787c4ae1244d1579cee88ebc8c",
      "url": "https://www.biorxiv.org/content/10.1101/2025.09.29.678806v1?rss=1",
      "title": "Lifespan Trajectories of Asymmetry in White Matter Tracts",
      "content": "Asymmetry in white matter is believed to give rise to the brain's capacity for specialized processing and is involved in the lateralization of various cognitive processes, such as language and visuo-spatial reasoning. Although studies of white matter asymmetry have been previously documented, they have often been constrained by limited age ranges, sample sizes, or the scope of the tracts and structural features examined. While normative lifespan charts for brain structures are emerging, comprehensive charts detailing white matter asymmetries across numerous pathways and diverse structural measures have been notably absent. This study addresses this gap by leveraging a large-scale dataset of 26,199 typically developing and aging individuals, ranging from 0 to 100 years of age, from 42 primary neuroimaging studies. We generated comprehensive lifespan trajectories for 30 lateralized association and projection white matter tracts, examining 14 distinct microstructural and macrostructural features of these pathways. Our findings reveal that: (1) asymmetries are widespread across the brain's white matter and are present in all 30 pathways; (2) for a given pathway, the degree and direction of asymmetry differ between features of tissue microstructure and pathway macrostructure; (3) asymmetries vary across and within pathway types (association and projection tracts); and (4) these asymmetries are not static, following unique trajectories across the lifespan, with distinct changes during development, and a general trend of becoming more asymmetric with increasing age (particularly in later adulthood) across pathways. This study represents the most extensive characterization of white matter asymmetry across the lifespan to date, charting how lateralization patterns emerge, mature, and change throughout life. It provides a foundational resource for understanding the principles of white matter organization from early to late life, its relation to functional specialization and inter-individual variability, and offers a key reference for interpreting deviations during healthy development and aging as well as those associated with clinical populations.",
      "author": "Kanakaraj, P., Bogdanov, S., Kim, M. E., Samir, J., Gao, C., Ramadass, K. E., Rudravaram, G., Newlin, N. R., Archer, D. B., Hohman, T. J., Jefferson, A. L., Morgan, V. L., Roche, A., Englot, D. J., Resnick, S. M., Held, L. L. B., Cutting, L., Barquero, L. A., D'Archangel, M. A., Nguyen, T. Q., Humphreys, K. L., Niu, Y., Vinci-Booher, S., Cascio, C. J., The HABS-HD Study Team,, Alzheimer's Disease Neuroimaging Initiative,, The BIOCARD Study Team,, Li, Z., Vandekar, S., Zhang, P., Gore, J. C., Forkel, S. J., Landman, B. A., Schilling, K.",
      "published_date": "2025-09-30T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 303,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:42.188442+00:00",
      "updated_at": "2025-09-30T08:28:42.188447+00:00"
    },
    {
      "id": "f96d7613851f3e8132d927de7307fd5f",
      "url": "http://www.jneurosci.org/cgi/content/short/45/38/etwij45382025?rss=1",
      "title": "This Week in The Journal",
      "content": "",
      "author": "McKeon, P.",
      "published_date": "2025-09-17T16:30:36+00:00",
      "source": "Journal Neuroscience This Week",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:38.114909+00:00",
      "updated_at": "2025-09-30T08:28:38.114912+00:00"
    },
    {
      "id": "cbb24422cf8642c1dc9695f01c798a11",
      "url": "http://www.jneurosci.org/cgi/content/short/45/39/e0790252025?rss=1",
      "title": "Multivariate White Matter Microstructure Alterations in Older Adults with Coronary Artery Disease",
      "content": "<p>Patients with coronary artery disease (CAD) face an increased risk of cognitive impairment, dementia, and stroke. While white matter (WM) lesions are frequently reported in patients with CAD, the effects on WM microstructure alterations remain largely unknown. We aimed to identify WM microstructural alterations in individuals with CAD compared with healthy controls (HC) and to examine their relationships with cognitive performance. Forty-three (43) patients with CAD (35 males and 8 females) and 36 HC (26 males and 10 females) aged 50 and older underwent comprehensive neuropsychological testing and multimodal 3&nbsp;T magnetic resonance imaging (MRI). A novel multivariate approach&mdash;the Mahalanobis distance (D2)&mdash;was used to quantify WM abnormalities as the amount of deviation from the HC reference group. D2 integrates multiple MRI-derived diffusion-weighted imaging, R1 relaxometry, and magnetization transfer imaging metrics, while accounting for covariance between metrics. Relationships between WM D2 and cognition (executive function and processing speed) were also assessed. Compared with HCs, patients with CAD had higher D2 values in the whole WM (<i>p</i> = 0.015) and in the right anterior and bilateral middle cerebral artery territories (<i>p</i> &lt; 0.05). Myelin-sensitive metrics, particularly R1 relaxation rate and MT saturation, were the most important contributors to D2. Processing speed was positively associated with greater R1 in both the whole WM and left middle cerebral artery territory. These findings suggest that greater WM microstructural alterations observed in patients with CAD were mainly driven by differences in myelin content. These alterations may contribute to a heightened risk of cognitive impairment.</p>",
      "author": "Tremblay, S. A., Potvin-Jutras, Z., Sabra, D., Rezaei, A., Sanami, S., Gagnon, C., Intzandt, B., Mainville-Berthiaume, A., Wright, L., Leppert, I. R., Tardif, C. L., Steele, C. J., Iglesies-Grau, J., Nigam, A., Bherer, L., Gauthier, C. J.",
      "published_date": "2025-09-24T16:30:27+00:00",
      "source": "Journal Neuroscience Current",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 247,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:36.635322+00:00",
      "updated_at": "2025-09-30T08:28:36.635324+00:00"
    },
    {
      "id": "d5ad6f19f949485f73ab66f02d3dd597",
      "url": "http://www.jneurosci.org/cgi/content/short/45/39/e0386252025?rss=1",
      "title": "Age-Related Positivity Bias in Emotion Recognition Is Linked to Lower Cognitive Performance and Altered Amygdala-Orbitofrontal Connectivity",
      "content": "<p>Changes in emotion recognition are observed in aging, in dementia, after brain lesions and as a function of mental health factors, such as depression. In aging, older adults have been argued to show a \"positivity bias,\" which has been associated with a relatively spared recognition accuracy for positive emotion and an increased tendency to label emotions as positive. This bias has been suggested to support mental well-being. However, it has also been found in association with cognitive decline and brain lesions. Here, we investigated the behavioral and brain correlates of this age-related positivity bias. We used multimodal brain imaging in a large group of human adults (<i>n</i> = 665, 333 females) drawn from a population-derived cohort across the lifespan, together with a psychometric analysis of an emotion recognition task using facial expressions. Beyond reductions in expression recognition accuracy, older adults showed increased perceptual thresholds for negative emotions and a reduced threshold for the positive emotion, even after accounting for general face recognition abilities. This positivity bias in labeling emotions was strongly associated with lower cognitive performance in older people, but not with (nonclinical) depressive symptoms. It was also associated with reduced gray matter volume in the bilateral anterior hippocampus&ndash;amygdala and increased functional connectivity between these regions and the orbitofrontal cortex. Together, age-related positivity bias is associated with cognitive decline and structural and functional brain differences. A positivity bias in emotion recognition may therefore reflect an early marker of neurodegeneration, a hypothesis that could be tested in future longitudinal studies.</p>",
      "author": "Wolpe, N., Harlev, D., Bergmann, E., Cam-CAN, Henson, R. N.",
      "published_date": "2025-09-24T16:30:27+00:00",
      "source": "Journal Neuroscience Current",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 249,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:36.635268+00:00",
      "updated_at": "2025-09-30T08:28:36.635270+00:00"
    },
    {
      "id": "1d80988c0d1bbf8e59d3835b55251e83",
      "url": "http://www.jneurosci.org/cgi/content/short/45/39/e0091252025?rss=1",
      "title": "Looking into Working Memory to Verify Potential Search Targets",
      "content": "<p>Finding what you are looking for is a ubiquitous task in everyday life that relies on a two-way comparison between what is currently viewed and internal search goals held in memory. Despite a wealth of studies tracking visual verification among external contents of perception, complementary verification processes among internal contents of memory remain elusive. Building on a recently established gaze marker of internal visual focusing in working memory, we uncover the internal inspection process associated with confirming or dismissing potential search targets. We show how male and female human participants \"look back\" into working memory when faced with external stimuli that are perceived as potential targets and link such internal inspection to the time needed for visual verification. A direct comparison between visual verification among the contents of working memory or perception further revealed how verification in both domains engages frontal theta activity in scalp electroencephalography but also how mnemonic verification is slower to deploy than perceptual verification. This establishes internal verification as an integral component of visual search and provides new ways to look into this underexplored component of human search behavior.</p>",
      "author": "Wang (&#x738B;&#x601D;&#x601D;), S., van Ede, F.",
      "published_date": "2025-09-24T16:30:27+00:00",
      "source": "Journal Neuroscience Current",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 183,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:36.635225+00:00",
      "updated_at": "2025-09-30T08:28:36.635227+00:00"
    },
    {
      "id": "ac8ee9971e157677e50dc228e0df84c6",
      "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2522045122?af=R",
      "title": "FGF21 acting on the noradrenergic nervous system protects against influenza virus infection",
      "content": "Proceedings of the National Academy of Sciences, Volume 122, Issue 39, September 2025. <br />SignificanceFibroblast growth factor 21 (FGF21) is a liver-derived hormone that signals to the brain to govern several physiologic stresses. In this paper, we show that FGF21 concentrations in blood are increased by influenza virus infection in both ...",
      "author": "Wei FanYuan ZhangLaurent GautronDavid G. ThomasHeather W. Stout-DelgadoEdward J. SchenckTadiwanashe GwatiringaKartik N. RajagopalanDavid J. MangelsdorfSteven A. KlieweraDepartment of Pharmacology, University of Texas Southwestern Medical Center, Dallas, TX 75390bCenter for Hypothalamic Research, Department of Internal Medicine, University of Texas Southwestern Medical Center, Dallas, TX 75390cDivision of Pulmonary and Critical Care Medicine, Department of Medicine, Weill Cornell Medicine, New York, NY 10065dDivision of Pulmonary and Critical Care, Department of Internal Medicine, University of Texas Southwestern Medical Center, Dallas, TX 75390eHHMI, University of Texas Southwestern Medical Center, Dallas, TX 75390fDepartment of Molecular Biology, University of Texas Southwestern Medical Center, Dallas, TX 75390",
      "published_date": "2025-09-25T07:00:00+00:00",
      "source": "Pnas Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 52,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:35.021616+00:00",
      "updated_at": "2025-09-30T08:28:35.021617+00:00"
    },
    {
      "id": "3d0f9f34f58f61be935b3d77d357fadb",
      "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2502193122?af=R",
      "title": "Novelty as a drive of human exploration in complex stochastic environments",
      "content": "Proceedings of the National Academy of Sciences, Volume 122, Issue 39, September 2025. <br />SignificanceWould you choose to complete a task in a few seconds for a guaranteed reward, or spend half an hour exploring unknown paths that may or may not lead to something better? Using a multistep decision-making task and computational modeling, we ...",
      "author": "Alireza ModirshanechiWei-Hsiang LinHe A. XuMichael H. HerzogWulfram GerstneraSchool of Life Sciences, Brain-Mind Institute, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Lausanne 1015, SwitzerlandbSchool of Computer and Communication Sciences, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Lausanne 1015, SwitzerlandcHelmholtz Munich, Neuherberg 85764, GermanydMax Planck Institute for Biological Cybernetics, T\u00fcbingen 72012, Germany",
      "published_date": "2025-09-25T07:00:00+00:00",
      "source": "Pnas Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 56,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:35.021445+00:00",
      "updated_at": "2025-09-30T08:28:35.021450+00:00"
    },
    {
      "id": "f696dd0700cee6d33cee0d16412c334f",
      "url": "https://www.frontiersin.org/articles/10.3389/fninf.2025.1630133",
      "title": "Super-resolution microscopy and deep learning methods: what can they bring to neuroscience: from neuron to 3D spine segmentation",
      "content": "In recent years, advances in microscopy and the development of novel fluorescent probes have significantly improved neuronal imaging. Many neuropsychiatric disorders are characterized by alterations in neuronal arborization, neuronal loss\u2014as seen in Parkinson\u2019s disease\u2014or synaptic loss, as in Alzheimer\u2019s disease. Neurodevelopmental disorders can also impact dendritic spine morphogenesis, as observed in autism spectrum disorders and schizophrenia. In this review, we provide an overview of the various labeling and microscopy techniques available to visualize neuronal structure, including dendritic spines and synapses. Particular attention is given to available fluorescent probes, recent technological advances in super-resolution microscopy (SIM, STED, STORM, MINFLUX), and segmentation methods. Aimed at biologists, this review presents both classical segmentation approaches and recent tools based on deep learning methods, with the goal of remaining accessible to readers without programming expertise.",
      "author": "Lydia Danglot",
      "published_date": "2025-09-29T00:00:00+00:00",
      "source": "Frontiers Neuroinformatics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 130,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:33.711144+00:00",
      "updated_at": "2025-09-30T08:28:33.711149+00:00"
    },
    {
      "id": "a5df7a6f0b4e33c3671e617a5c4976d7",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1551555",
      "title": "Circuit-level modeling of prediction error computation of multi-dimensional features in voluntary actions",
      "content": "IntroductionPredictive processing posits that the brain minimizes discrepancies between internal predictions and sensory inputs, offering a unifying account of perception, cognition, and action. In voluntary actions, it is thought to suppress self-generated sensory outcomes. Although sensory mismatch signals have been extensively investigated and modeled, mechanistic insights into the neural computation of predictive processing in voluntary actions remain limited.MethodsWe developed a computational model comprising two-compartment excitatory pyramidal cells (PCs) and three major types of inhibitory interneurons with biologically realistic connectivity. The model incorporates experience-dependent inhibitory plasticity and feature selectivity to shape excitation-inhibition (E/I) balance. We then extended it to a two-dimensional prediction-error (PE) circuit in which each PC has two segregated, top-down modulated dendrites-each bell-tuned to a distinct feature-enabling combination selectivity.ResultsThe model reveals that top-down predictions can selectively suppress PCs with matching feature selectivity via experience-dependent inhibitory plasticity. This suppression depends on the response selectivity of inhibitory interneurons and on balanced excitation and inhibition across multiple pathways. The framework also accommodates predictions involving two independent features.DiscussionBy combining biological connectivity data with computational modeling, this study provides insights into the neural circuits and computations underlying the active suppression of sensory responses in voluntary actions. These findings contribute to understanding how the brain generates and processes predictions to guide behavior.",
      "author": "Yiling Li",
      "published_date": "2025-09-29T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 207,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:32.248088+00:00",
      "updated_at": "2025-09-30T08:28:32.248092+00:00"
    },
    {
      "id": "b5aeef87f3ef04a40d5b2d8660bcecba",
      "url": "https://www.frontiersin.org/articles/10.3389/fnhum.2025.1681538",
      "title": "When embodiment matters most: a confirmatory study on VR priming in motor imagery brain-computer interfaces training",
      "content": "BackgroundVirtual Reality (VR) feedback is increasingly integrated into Brain-Computer Interface (BCI) applications, enhancing the Sense of Embodiment (SoE) toward virtual avatars and fostering more vivid motor imagery (MI). VR-based MI-BCIs hold promise for motor rehabilitation, but their effectiveness depends on neurofeedback quality. Although SoE may enhance MI training, its role as a priming strategy prior to VR-BCI has not been systematically examined, as prior work assessed embodiment only after interaction. This study investigates whether embodiment priming influences MI-BCI outcomes, focusing on event-related desynchronization (ERD) and BCI performance.MethodsUsing a within-subject design, we combined data from a pilot study with an extended experiment, yielding 39 participants. Each completed an embodiment induction phase followed by MI training with EEG recordings. ERD and lateralization indices were analyzed across conditions to test the effect of prior embodiment.ResultsEmbodiment induction reliably increased SoE, yet no significant ERD differences were found between embodied and control conditions. However, lateralization indices showed greater variability in the embodied condition, suggesting individual differences in integrating embodied feedback.ConclusionOverall, findings indicate that real-time VR-based feedback during training, rather than prior embodiment, is the main driver of MI-BCI performance improvements. These results corroborate earlier findings that real-time rendering of embodied feedback during MI-BCI training constitutes the primary mechanism supporting performance gains, while highlighting the complex role of embodiment in VR-based MI-BCIs.",
      "author": "Athanasios Vourvopoulos",
      "published_date": "2025-09-25T00:00:00+00:00",
      "source": "Frontiers Human Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 216,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:30.620146+00:00",
      "updated_at": "2025-09-30T08:28:30.620147+00:00"
    },
    {
      "id": "f98db1e6cc3186d32089a67f5b23c578",
      "url": "https://www.frontiersin.org/articles/10.3389/fnhum.2025.1682852",
      "title": "Precision TMS through the integration of neuroimaging and machine learning: optimizing stimulation targets for personalized treatment",
      "content": "Transcranial Magnetic Stimulation (TMS), a non-invasive neuromodulation technique based on electromagnetic induction, modulates cortical excitability by inducing currents with a magnetic field. TMS has demonstrated significant clinical potential in the treatment of various neuropsychiatric disorders, including depression, anxiety, and Parkinson\u2019s disease. However, conventional TMS targeting methods that rely on anatomical landmarks do not adequately account for individual differences in brain structure and functional networks, leading to considerable variability in treatment responses. In recent years, advances in neuroimaging techniques\u2013such as functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI)\u2013together with the application of machine learning (ML) and artificial intelligence (AI) algorithms in big data analysis, have provided novel approaches for precise TMS targeting and individualized treatment. This review summarizes the latest developments in the integration of multimodal neuroimaging and AI technologies for precision neuromodulation with TMS. It focuses on critical issues such as imaging resolution, AI model generalizability, real-time feedback modulation, as well as data privacy and ethical considerations. Future prospects including closed-loop TMS control systems, cross-modal data fusion, and AI-assisted brain-computer interfaces (BCIs) are also discussed. Overall, AI-driven personalized TMS strategies hold promise for markedly enhancing treatment precision and clinical efficacy, thereby offering new theoretical and practical guidance for individualized treatment in neuropsychiatric and neurodegenerative disorders.",
      "author": "Panxiao Bao",
      "published_date": "2025-09-29T00:00:00+00:00",
      "source": "Frontiers Human Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 207,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:30.619981+00:00",
      "updated_at": "2025-09-30T08:28:30.619982+00:00"
    },
    {
      "id": "1808b94a3939016378e10217d98c3fb3",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1662886",
      "title": "Efficient spiking convolutional neural networks accelerator with multi-structure compatibility",
      "content": "Spiking Neural Networks (SNNs) possess excellent computational energy efficiency and biological credibility. Among them, Spiking Convolutional Neural Networks (SCNNs) have significantly improved performance, demonstrating promising applications in low-power and brain-like computing. To achieve hardware acceleration for SCNNs, we propose an efficient FPGA accelerator architecture with multi-structure compatibility. This architecture supports both traditional convolutional and residual topologies, and can be adapted to diverse requirements from small networks to complex networks. This architecture uses a clock-driven scheme to perform convolution and neuron updates based on the spike-encoded image at each timestep. Through hierarchical pipelining and channel parallelization strategies, the computation speed of SCNNs is increased. To address the issue of current accelerators only supporting simple network, this architecture combines configuration and scheduling methods, including grouped reuse computation and line-by-line multi-timestep computation to accelerate deep networks with lots of channels and large feature map sizes. Based on the proposed accelerator architecture, we evaluated two scales of networks, named small-scale LeNet and deep residual SCNN, for object detection. Experiments show that the proposed accelerator achieves a maximum recognition speed of 1, 605 frames/s at a 100 MHz clock for the LeNet network, consuming only 0.65 mJ per image. Furthermore, the accelerator, combined with the proposed configuration and scheduling methods, achieves acceleration for each residual module in the deep residual SCNN, reaching a processing speed of 2.59 times that of the CPU with a power consumption of only 16.77% of the CPU. This demonstrates that the proposed accelerator architecture can achieve higher energy efficiency, compatibility, and wider applicability.",
      "author": "Kairang Chen",
      "published_date": "2025-09-26T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 253,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:27.604644+00:00",
      "updated_at": "2025-09-30T08:28:27.604646+00:00"
    },
    {
      "id": "6cfe10a32467a38592d88c84f1ec1a2b",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1645903",
      "title": "Personalized temporal interference stimulation targeting striatum reduces functional stability and dynamic connectivity variability in the sensorimotor network",
      "content": "BackgroundFunctional stability within brain networks, particularly the sensorimotor network (SMN), is crucial for coherent motor control. Temporal Interference (TI) stimulation offers a non-invasive method to modulate deep brain structures like the striatum, yet its impact on dynamic functional stability across motor networks remains largely unexplored.MethodsTwenty-six healthy male participants separately underwent TI stimulation and Sham stimulation in a crossover, double-blind, randomized controlled trial with counterbalanced protocol. resting-state functional magnetic resonance imaging (rs-fMRI) was acquired before and during the stimulation. A total of 20 min TI stimulation (10 mA, \u0394f = 20 Hz) was applied to the right striatum using personalized electrode montages optimized. Dynamic functional connectivity (dFC) was computed using a sliding-window approach. Voxel-wise functional stability across the whole brain was quantified by Kendall\u2019s concordance coefficient of voxel-to-voxel dFC. Seed-based dFC variability in the right striatum was measured as the standard deviation of dFC across windows.Results(1) Functional stability: TI stimulation significantly decreased functional stability in bilateral SMA regions (predominantly SMA proper, with parts of pre-SMA) compared to Sham and baseline conditions (P < 0.01). (2) Dynamic functional connectivity: TI stimulation reduced dFC variability between the right striatum and left SMA region (predominantly SMA proper, with parts of pre-SMA) compared to baseline (P < 0.01). (3) Safety: No adverse cognitive effects or side effects were observed, with good blinding effectiveness maintained throughout the study.ConclusionOur findings indicate that TI stimulation targeting the striatum effectively modulates sensorimotor network stability and dFC variability within the cortico-striatal pathway, highlighting its potential as a non-invasive neuromodulation approach for motor network disorders.Clinical trial registration[www.chictr.org.cn;], identifier [ChiCTR2500098699].",
      "author": "Zhiqiang Zhu",
      "published_date": "2025-09-26T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 258,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:27.604609+00:00",
      "updated_at": "2025-09-30T08:28:27.604610+00:00"
    },
    {
      "id": "4a976146eb68e38d4b2342510272a8be",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1622978",
      "title": "Analysis of the correlation between serum vitamin D and hypothalamic-pituitary-adrenal axis hormone levels in patients with post-traumatic stress disorder",
      "content": "ObjectivePost-Traumatic Stress Disorder (PTSD) is a psychological disorder triggered by extreme traumatic events. It is characterized by impaired cognitive function and neuroendocrine dysfunction, particularly dysregulation of the hypothalamic-pituitary-adrenal axis. In recent years, the role of vitamin D in neuroprotection and cognitive function has garnered increasing interest; however, its relationship with hypothalamic\u2013pituitary\u2013adrenal (HPA) axis hormone levels in patients with post-traumatic stress disorder (PTSD) remains poorly understood.MethodsThis study aimed to investigate the correlation between serum vitamin D levels and HPA axis hormones in patients with PTSD. A total of 96 patients with severe trauma admitted to Rizhao People\u2019s Hospital between March 2022 and December 2024 were enrolled and categorized into PTSD and non-PTSD groups according to diagnostic criteria. PTSD symptoms were evaluated using the PTSD Checklist\u2013Civilian Version. Serum levels of 25-hydroxyvitamin D, corticotropin-releasing hormone, adrenocorticotropic hormone, and cortisol were measured. Spearman\u2019s correlation analysis and receiver operating characteristic curves were employed to assess associations between vitamin D, HPA axis biomarkers, and PCL-C Scores.ResultsThe results showed that serum 25-hydroxyvitamin D levels were significantly lower in the PTSD group compared to the non-PTSD group (P < 0.001), while CRH and ACTH levels were significantly higher, and cortisol levels were significantly lower (P < 0.001). Spearman correlation analysis indicated that vitamin D levels were negatively correlated with CRH and ACTH levels and positively correlated with cortisol levels (P < 0.05). ROC curve analysis revealed that serum 25-hydroxyvitamin D levels have diagnostic potential for PTSD, with a cutoff value of 16.32 ng/mL, an AUC of 0.698, sensitivity of 86.2%, and specificity of 51.1%.ConclusionThis study demonstrated a correlation between serum vitamin D levels and HPA axis hormone levels in patients with PTSD, suggesting that vitamin D deficiency may be associated with HPA axis dysregulation in PTSD. These findings underscore a potential link between vitamin D deficiency and PTSD, warranting further investigation into the role of vitamin D in the disorder\u2019s pathophysiology and its potential as a therapeutically modifiable factor.",
      "author": "Hui Ju",
      "published_date": "2025-09-29T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 321,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:27.604551+00:00",
      "updated_at": "2025-09-30T08:28:27.604552+00:00"
    },
    {
      "id": "5f7831a7a733025568f60315f92e1a25",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1651762",
      "title": "A novel fast detection algorithm for depression based on 3-channel EEG signals",
      "content": "Medically unexplained symptoms (MUS) are an emerging field in current research. Among middle-aged and elderly patients, most MUS symptoms are mainly caused by depression, but early symptoms do not meet the international somatization standards, which delays treatment. Therefore, developing a rapid auxiliary diagnosis method is of great significance. This paper proposes a novel model for identifying depression based on 3-channel electroencephalogram (EEG) signals from the prefrontal lobe of the human brain. For the collected resting-state EEG signals, variational mode decomposition (VMD) is first used for signal decomposition, and the power spectrum is employed to select intrinsic mode function (IMF) components. After extracting energy features via sample entropy, LightGBM is adopted for classification, with a classification accuracy of 97.42%. Through comparative experiments, the model proposed in this paper achieves a balance between high accuracy and timeliness. This is conducive to the development of a depression detection system based on portable real-time electroencephalography (EEG), and provides a solution for EEG signal devices in real-time depression detection and pre-triage of patients with Medically Unexplained Symptoms (MUS).",
      "author": "TaoLi Xie",
      "published_date": "2025-09-29T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 173,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:27.604503+00:00",
      "updated_at": "2025-09-30T08:28:27.604504+00:00"
    },
    {
      "id": "b9346f6da2f0df44769b26a3f530dc79",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1627497",
      "title": "GenCPM: a toolbox for generalized connectome-based predictive modeling",
      "content": "Understanding brain\u2014behavior relationships and predicting cognitive and clinical outcomes from neuromarkers are central tasks in neuroscience. Connectome-based Predictive Modeling (CPM) has been widely adopted to predict behavioral traits from brain connectivity data; however, existing implementations are largely restricted to continuous outcomes, often overlook essential non-imaging covariates, and are difficult to apply in clinical or disease cohort settings. To address these limitations, we present GenCPM, a generalized CPM framework implemented in open-source R software. GenCPM extends traditional CPM by supporting binary, categorical, and time-to-event outcomes and allows the integration of covariates such as demographic and genetic information, thereby improving predictive accuracy and interpretability. To handle high-dimensional data, GenCPM incorporates marginal screening and regularized regression techniques, including LASSO, ridge, and elastic net, for efficient selection of informative brain connections. We demonstrate the utility of GenCPM through analyses of the Anti-Amyloid Treatment in Asymptomatic Alzheimer's Disease (A4) Study and the Alzheimer's Disease Neuroimaging Initiative (ADNI), showing enhanced predictive performance and improved signal attribution compared to standard methods. GenCPM offers a flexible, scalable, and interpretable solution for predictive modeling in brain connectivity research, supporting broader applications in cognitive and clinical neuroscience.",
      "author": "Yize Zhao",
      "published_date": "2025-09-29T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 187,
      "reading_time": 1,
      "created_at": "2025-09-30T08:28:27.604472+00:00",
      "updated_at": "2025-09-30T08:28:27.604473+00:00"
    }
  ]
}