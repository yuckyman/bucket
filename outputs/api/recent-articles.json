{
  "last_updated": "2026-01-27T18:42:35.627069+00:00",
  "count": 20,
  "articles": [
    {
      "id": "560b53b8a9f1b10c46ba5f7d619d4dec",
      "url": "http://ieeexplore.ieee.org/document/11164370",
      "title": "Drawing the Line: Wearable Linear Haptics Motivated by Guided Breathing",
      "content": "Haptic wearables provide an intuitive human-machine interface to convey information through the sense of touch, which may have promising applications in guided breathing. In this paper, we detail the design and evaluation of three wearable prototypes (Vibration, Skin Drag, and Tapping) capable of administering discrete (individual, separate pulses and stimuli). and continuous (overlapping or uninterrupted stimuli) forms of linear haptic cycles with inspiration from slow, deep guided breathing. Characterization was performed to quantify and validate the performance of six haptic stimuli (discrete/continuous vibration, skin drag, and tapping). Devices were quantified with key metrics that described the applied stimuli and the dynamics of the wearable. A human subjects study (N = 25), composed of two-cycle tracking tasks, was conducted to determine device performance and user aptitude. Results indicated consistent directional recognition across all six stimuli, but discrete stimuli performed better in spatial localization tasks. Although outperformed in tracking/localization tasks, continuous stimuli, especially skin drag, were described as the most apt and intuitive pairing to guided breathing. Findings highlight the potential of these linear haptic stimuli in a number of applications, including guided breathing, navigation, virtual immersion, and communication.",
      "author": "",
      "published_date": "2025-09-15T13:17:38+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 187,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:23.390760+00:00",
      "updated_at": "2026-01-27T18:42:23.390762+00:00"
    },
    {
      "id": "273f5b74995aee1a22f54d9a1ad2625a",
      "url": "http://ieeexplore.ieee.org/document/11145272",
      "title": "Haptics of Pulse Palpation: Simulation and Validation Through Novel Sensor-Actuator System",
      "content": "Palpation of arteries holds significant physiological importance. Existing pulse actuator designs intended to replicate the haptic sensations of palpation primarily focus on normal force interactions, often overlooking the shear forces generated by oscillations of the arterial wall during blood flow. This study aims to evaluate the normal, longitudinal, and transverse forces exerted by arteries through both theoretical and experimental analyses during palpation. The experimental validation features a pulse actuator-sensor system. The actuator component is a hydroelectromagnetic actuator, while the haptic sensing is performed by the Subblescope. The Subblescope measures arterial force feedback from both soft and hard artery models, as well as from the radial pulse in 18 human subjects. Mathematical analysis establishes the operational range of the sensor-actuator system as 0.005 N to 2.5 N. The force feedback from the simulation has been used for designing the total force generation by the actuator. The reactive force along the Z-axis varies between 19.3 mN to 500 mN, while the transverse and longitudinal forces along the Y and X axes range from 6.9 mN to 88.01 mN and 5.46 mN to 87.85 mN, respectively. The pulse-force map of the hard artery reveals higher three-dimensional force interactions compared to the soft artery. The hydroelectromagnetic actuator effectively generates both normal and shear forces during pulsatile flow. Future work will focus on developing training modules that replicate pulse haptics associated with various physiological conditions such as diabetes.",
      "author": "",
      "published_date": "2025-08-29T13:18:28+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 233,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:23.390727+00:00",
      "updated_at": "2026-01-27T18:42:23.390729+00:00"
    },
    {
      "id": "053a64c388be11291b58cb5dddb68ec6",
      "url": "http://ieeexplore.ieee.org/document/11303561",
      "title": "Scene-sensitive Medial Temporal Lobe Subregions Are Recruited for the Integration of Non-scene Stimuli",
      "content": "A hallmark feature of episodic memory is the ability to flexibly recombine information across episodes to form new associations and guide behavior. This process, termed associative inference, relies on the hippocampus and surrounding medial temporal lobe (MTL) subregions. We previously found that cross-episode binding was improved when episodes were linked by scenes rather than by faces or objects. Here, we tested whether differential recruitment of category-sensitive MTL subregions underlies these behavioral differences. Participants completed study-test phases of the Associative Inference in Memory task while undergoing fMRI scanning. During the study phase, they encoded overlapping AB and BC pairs. A and C items were always objects. The linking B item was either a face or a scene. At test, memory for the direct (AB, BC) and indirect associations (inferred AC) was tested. Category sensitivity in MTL subregions was tested using an independent functional localizer and the low integration (AB) trials from the study phase of the Associative Inference in Memory task. Within the MTL, no subregions exhibited face sensitivity. The anterior hippocampal head, anterolateral and posteromedial entorhinal cortices, and parahippocampal cortex were identified as scene sensitive. Although accuracy of the indirect inferences did not differ between pairs linked by faces and scenes, MTL subregion recruitment differed across categories. Scene-sensitive subregions in MTL cortex (anterolateral entorhinal cortex, posteromedial entorhinal cortex, and parahippocampal cortex), but not the hippocampus (anterior hippocampal head), were recruited to support associative inference for faces during encoding. These findings suggest that regions in MTL cortex identified as scene sensitive here may be involved in integrating disparate elements of episodes into coherent representations, and may be recruited for non-scene stimuli when integration demands during encoding are high (e.g., during associative inference).",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 281,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:18.510701+00:00",
      "updated_at": "2026-01-27T18:42:18.510702+00:00"
    },
    {
      "id": "d07d9c88cf921ac53c3b05c1709a7d87",
      "url": "http://ieeexplore.ieee.org/document/11303567",
      "title": "Increasing Signal, Reducing Noise: Contrasting Neural Mechanisms of Attention in Visual Search",
      "content": "When invariant target\u2013distractor arrays are presented repeatedly during visual search, participants respond faster on repeated versus novel configuration trials. This effect reflects attentional guidance through long-term memory (LTM) templates\u2014a phenomenon termed contextual cueing. Subsequently, relocating the target within the same distractor layout abolishes any contextual cueing effects, and relearning the new target position is much harder than the initial learning\u2014likely due to consistent attentional misguidance toward the initial (learned) target position. Here, we studied how the different processes involved in contextual cueing and relearning affect the variability of neural responses in human participants as measured with EEG. Attention has previously been shown to reduce trial-by-trial variability in EEG [Arazi, A., Yeshurun, Y., & Dinstein, I. Neural variability is quenched by attention. Journal of Neuroscience, 39, 5975\u20135985, 2019], indicating that, in addition to increasing the neural response to an attended stimulus, attention may reduce the noise within the neural response itself. While repeated versus novel contexts did not modulate the trial-by-trial variability during initial learning, significant lateralized variability reductions were observed for repeated but not novel context trials in the relocations phase. This contrasts with how contextual cueing affected lateralized ERPs in past research. Zinchenko and colleagues [Zinchenko, A., Conci, M., T\u00f6llner, T., M\u00fcller, H. J., & Geyer, T. Automatic guidance (and misguidance) of visuospatial attention by acquired scene memory: Evidence from an N1pc polarity reversal. Psychological Science, 31, 1531\u20131543, 2020] found that lateralized ERPs signal correct and incorrect (i.e., misguided) attentional selection of target positions learned earlier. This phenomenon was observed during both the learning and relocation phases. Thus, variability and lateralized ERPs may represent different facets of attention, where variability becomes evident specifically under high attentional demand conditions, such as when participants must override the misguidance caused by LTM templates.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 291,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:18.510662+00:00",
      "updated_at": "2026-01-27T18:42:18.510664+00:00"
    },
    {
      "id": "f95a9f64dc97257327f12c06475d3643",
      "url": "http://ieeexplore.ieee.org/document/11303565",
      "title": "The Neural Bases of Graphical Perception: A Novel Instance of Cultural Recycling?",
      "content": "Graphical representations of quantitative data abound in our culture, and yet the brain mechanisms of graphicacy, by which viewers quickly extract statistical information from a data graphic, are unknown. Here, using scatterplots as stimuli, we tested two hypotheses about the brain areas underlying graphicacy. First, at the perceptual level, we hypothesized that the visual processing of scatterplots and their main trend recycles cortical regions devoted to the perception of the principal axis of objects. Second, at a higher level, we speculated that the math-responsive network active during arithmetic and mathematical truth judgments should also be involved in graphical perception. Using fMRI, we indeed found that the judgment of the trend in a scatterplot recruits a right lateral occipital area involved in detecting the orientation of objects, as well as a right anterior intraparietal region also recruited during mathematical tasks. Both behavior and brain activity were driven by the t value that indexes the statistical correlation in the data, and right intraparietal activation covaried with participants' graphicacy level. On the basis of this first approach to the neural bases of graphical perception, we suggest that, like literacy and numeracy, graphicacy relies on the recycling of brain areas previously attuned to a similar problem, here the perception of object orientation.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 208,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:18.510614+00:00",
      "updated_at": "2026-01-27T18:42:18.510615+00:00"
    },
    {
      "id": "cfdf4cdfc85c3633062d4f70f1a372c0",
      "url": "http://ieeexplore.ieee.org/document/11303571",
      "title": "Neural Evidence for Tonal Prediction: Multivariate Decoding of Predicted Tone Categories Using Functional Magnetic Resonance Imaging Data",
      "content": "Predictive processing plays a central role in language comprehension, allowing listeners to generate predictions about upcoming linguistic input. Although considerable evidence supports segmental prediction, less is known about whether listeners can form predictions about suprasegmental features such as lexical tone. This study investigates whether listeners can generate and neurally represent predicted tonal information in the absence of auditory input. Using a Mandarin Chinese tone sandhi paradigm, we established tonal predictions based on sentence and visual context, recording brain activity with functional magnetic resonance imaging. Multivariate pattern analysis showed that predicted tonal categories could be decoded from brain activity even without tonal stimuli present. These representations were localized in auditory areas, articulatory motor regions, and the right cerebellum. We also found that predicted tone representations had distinct neural substrates compared to perceived tone representations. The study provides direct neural evidence that listeners can form representations of lexical tone in predictions of auditory input. The findings expand our understanding of suprasegmental prediction in speech and highlight the cerebellum's role in linguistic prediction.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 170,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:18.510574+00:00",
      "updated_at": "2026-01-27T18:42:18.510576+00:00"
    },
    {
      "id": "f59b7d3ceaf0b7a86d83ae6499e5108a",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.25.701526v1?rss=1",
      "title": "Is torpor a quiescent state? Periodic motility and transient brain activation during daily torpor in Djungarian hamsters",
      "content": "Torpor is a hypometabolic state employed by many mammalian and non-mammalian species to cope with harsh environments. When exposed to a short photoperiod, Djungarian hamsters (Phodopus sungorus) enter daily torpor with body temperatures dropping as low as 15 degrees Celcius. Despite the widely-held notion that torpor is a form of deep sleep, torpid animals are not completely inactive but exhibit occasional movements reflected in an increase in EMG tone. Little is known about these EMG events during torpor and whether they have a functional role during the torpid state. We here analysed EEG, EMG, and brain temperature data from Djungarian hamsters, and used an automatic detection algorithm to identify periods of EMG activation during spontaneous daily torpor. The hamsters exhibited regular periods of motility that were invariably initiated during a decline in brain temperature and were followed by a brain temperature increase. The frequency of EMG events exhibited a negative correlation with brain temperature, such that lower brain temperature was associated with a higher frequency of EMG events. In addition, EMG events were associated with a pronounced increase in EEG power, especially between 9.5-15.5 Hz, which often started with an EEG pattern similar to an evoked potential preceding the increase in the EMG activity. On the contrary, micro-arousals during normothermic NREM sleep were associated with a decrease in EEG power, a decrease in brain temperature and were of shorter duration than torpor EMG events, indicating that the two phenomena may serve different purposes. We speculate that periodic motility associated with increased brain activity during torpor may play a role in thermoregulation, and help retain vigilance to potentially mitigate predation risk during this hypometabolic state.",
      "author": "Hauglund, N. L., Mukherji, R., Zhou, X., Hoerder-Suabedissen, A., Mao, R., Peirson, S. N., Herwig, A., Deboer, T., Vyazovskiy, V. V.",
      "published_date": "2026-01-27T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 274,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:05.757374+00:00",
      "updated_at": "2026-01-27T18:42:05.757375+00:00"
    },
    {
      "id": "ca2e440b6f566d4ed406de2191f9d166",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.25.701594v1?rss=1",
      "title": "A low-variance subspace underlies individual differences in resting state fMRI",
      "content": "People differ remarkably from one another, yet isolating individual differences in their brain activity remains challenging. Non-invasive whole-brain recordings of human brain activity, such as those from resting state fMRI (rs-fMRI), are complex and noisy, making it difficult to isolate stable dimensions of individual differences. Ideally, we want to find a few core dimensions that vary across people but have high test-retest reliability, giving the same value each time they are measured in the same person. However, it is still unknown whether any such reliable dimensions exist, and if they do, what could drive this reliability. Here, we show that there is a low-dimensional linear subspace of highly-reliable rs-fMRI activity. These dimensions form personal fingerprints, allowing participants to be identified with high accuracy despite fingerprints explaining only a fraction of the total variance. Many of these dimensions inherit their reliability from a single morphological, demographic, or behavioral property, and most dimensions can be predicted from the anatomical layout of cortical regions. These dimensions were identified using reliability component analysis (RCA), a new dimensionality reduction technique similar to principal component analysis (PCA) but which maximizes reliability instead of explained variance. Together, our findings suggest that stable individual signatures can be isolated from rs-fMRI. These signatures reflect persistent anatomical and physiological differences, and provide a principled low-dimensional basis for biomarker discovery.",
      "author": "Borovykh, A., Weissenbacher, M., Noble, S., Shinn, M.",
      "published_date": "2026-01-27T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 219,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:05.757332+00:00",
      "updated_at": "2026-01-27T18:42:05.757333+00:00"
    },
    {
      "id": "c0619613367d8e3b053076bf0a2efb18",
      "url": "https://www.biorxiv.org/content/10.64898/2026.01.25.701581v1?rss=1",
      "title": "Sample size dependence of Occam's razor in human decision-making",
      "content": "To make sense of a noisy world, living beings constantly face decisions between competing interpretations for ambiguous sensory data. This process parallels statistical model selection, where most frameworks, like the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), are based on a trade-off between a model's goodness-of-fit and its complexity. The same tradeoff has been observed in humans. However, a core tenet of normative frameworks is that the trade-off should depend on the sample size (N); as more data becomes available, the goodness-of-fit grows faster than the complexity penalty, weakening the overall bias towards simplicity. It is unknown whether humans also conform to this scaling principle, and if so, whether it arises from an optimal computation or a simpler heuristic. Here, we investigate this question using a preregistered visual task where subjects inferred the number of latent Gaussian sources generating clusters of data-points, and where the number of points (N) presented on each trial is varied systematically. We use three kinds of model to describe their behavior: a model with linear scaling of evidence in N (as in BIC and AIC), a model with no scaling, and a model with sub-linear scaling inspired by known biases in numerosity perception. Our results demonstrate that the normative, linear scaling model provides the worst account of human behavior. Instead, we find strong evidence for a sub-linear scaling of sample size. By inferring the shape of this scaling with Gaussian Processes, we reveal a distinct logarithmic trend for smaller N, and a flattening for higher values, both consistent with numerosity perception biases. This finding suggests that, when selecting for competing explanations for sensory data, humans do not perform a principled computation but rather employ a more efficient heuristic that repurposes lower perceptual mechanisms to dynamically weight evidence against model complexity.",
      "author": "Rinaldi, F. G., Piasini, E.",
      "published_date": "2026-01-27T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 298,
      "reading_time": 1,
      "created_at": "2026-01-27T18:42:05.757287+00:00",
      "updated_at": "2026-01-27T18:42:05.757292+00:00"
    },
    {
      "id": "e0f2dff572049a5cf7e73b98b654d348",
      "url": "https://www.reddit.com/r/Python/comments/1qolit2/4_pyrefly_type_narrowing_patterns_that_make_type/",
      "title": "4 Pyrefly Type Narrowing Patterns that make Type Checking more Intuitive",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>Since Python is a duck-typed language, programs often narrow types by checking a structural property of something rather than just its class name. For a type checker, understanding a wide variety of narrowing patterns is essential for making it as easy as possible for users to type check their code and reduce the amount of changes made purely to \u201csatisfy the type checker\u201d.</p> <p>In this blog post, we\u2019ll go over some cool forms of narrowing that Pyrefly supports, which allows it to understand common code patterns in Python.</p> <p>To the best of our knowledge, Pyrefly is the only type checker for Python that supports all of these patterns.</p> <p>Contents: 1. hasattr/getattr 2. tagged unions 3. tuple length checks 4. saving conditions in variables</p> <p>Blog post: <a href=\"https://pyrefly.org/blog/type-narrowing/\">https://pyrefly.org/blog/type-narrowing/</a> Github: <a href=\"https://github.com/facebook/pyrefly\">https://github.com/facebook/pyrefly</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BeamMeUpBiscotti\"> /u/BeamMeUpBiscotti </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qolit2/4_pyrefly_type_narrowing_patterns_that_make_type/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qolit2/4_pyrefly_type_narrowing_patterns_that_make_type/\">[comments]</a></span>",
      "author": "/u/BeamMeUpBiscotti",
      "published_date": "2026-01-27T18:08:23+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 151,
      "reading_time": 1,
      "created_at": "2026-01-27T18:41:26.799725+00:00",
      "updated_at": "2026-01-27T18:41:26.799726+00:00"
    },
    {
      "id": "dbb414f977bea21d7d61ba4b124bf814",
      "url": "https://doi.org/10.1016/j.cub.2025.11.059",
      "title": "Flexible use of a multi-purpose tool by a cow",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46711159\">Comments</a>",
      "author": "",
      "published_date": "2026-01-21T20:33:50+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-27T18:41:25.573820+00:00",
      "updated_at": "2026-01-27T18:41:25.573821+00:00"
    },
    {
      "id": "cebd15af001e2140c1e0175b8223c6ab",
      "url": "https://twitter.com/karpathy/status/2015883857489522876",
      "title": "A few random notes from Claude coding quite a bit last few weeks",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46771564\">Comments</a>",
      "author": "",
      "published_date": "2026-01-26T21:09:19+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-27T18:41:25.573698+00:00",
      "updated_at": "2026-01-27T18:41:25.573700+00:00"
    },
    {
      "id": "726daed51afa7ac7d88be04d98c0e955",
      "url": "https://www.nytimes.com/2026/01/26/science/archaeology-neanderthals-tools.html",
      "title": "430k-year-old well-preserved wooden tools are the oldest ever found",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46781530\">Comments</a>",
      "author": "",
      "published_date": "2026-01-27T15:46:29+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-27T18:41:25.573672+00:00",
      "updated_at": "2026-01-27T18:41:25.573676+00:00"
    },
    {
      "id": "726daed51afa7ac7d88be04d98c0e955",
      "url": "https://www.nytimes.com/2026/01/26/science/archaeology-neanderthals-tools.html",
      "title": "430k-year-old well-preserved wooden tools are the oldest ever found",
      "content": "<p><a href=\"https://archive.ph/mHlUT\" rel=\"nofollow\">https://archive.ph/mHlUT</a><p><a href=\"https://apnews.com/article/oldest-wooden-tools-marathousa-1-6e07cfc79f6d8ba648138f805531b933\" rel=\"nofollow\">https://apnews.com/article/oldest-wooden-tools-marathousa-1-...</a><p><a href=\"https://archaeologymag.com/2026/01/430000-year-old-wooden-tools-marathousa/\" rel=\"nofollow\">https://archaeologymag.com/2026/01/430000-year-old-wooden-to...</a></p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46781530\">https://news.ycombinator.com/item?id=46781530</a></p>\n<p>Points: 138</p>\n<p># Comments: 78</p>",
      "author": "bookofjoe",
      "published_date": "2026-01-27T15:46:29+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 18,
      "reading_time": 1,
      "created_at": "2026-01-27T18:41:24.480919+00:00",
      "updated_at": "2026-01-27T18:41:24.480921+00:00"
    },
    {
      "id": "978857dc74738e2d18afa361be71099b",
      "url": "https://www.cnn.com/2026/01/27/food/amazon-fresh-go-closures",
      "title": "Amazon to shut down Go and Fresh stores",
      "content": "<p><a href=\"https://www.wsj.com/business/retail/amazon-to-shut-down-all-amazon-go-and-amazon-fresh-stores-0301dfb7\" rel=\"nofollow\">https://www.wsj.com/business/retail/amazon-to-shut-down-all-...</a></p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46781707\">https://news.ycombinator.com/item?id=46781707</a></p>\n<p>Points: 185</p>\n<p># Comments: 156</p>",
      "author": "gmays",
      "published_date": "2026-01-27T15:58:05+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 14,
      "reading_time": 1,
      "created_at": "2026-01-27T18:41:24.480893+00:00",
      "updated_at": "2026-01-27T18:41:24.480895+00:00"
    },
    {
      "id": "ab57780fb2fa685bad46bdab69c6588d",
      "url": "https://news.ycombinator.com/item?id=46783600",
      "title": "Show HN: LemonSlice \u2013 Give your voice agents a face",
      "content": "<p>Hey HN, we're the co-founders of LemonSlice (<a href=\"https://lemonslice.com\">https://lemonslice.com</a>). We train interactive avatar video models. Our API lets you upload a photo and immediately jump into a FaceTime-style call with that character. Here's a demo: <a href=\"https://www.loom.com/share/941577113141418e80d2834c83a5a0a9\" rel=\"nofollow\">https://www.loom.com/share/941577113141418e80d2834c83a5a0a9</a><p>Chatbots are everywhere. Voice AI has recently taken off. But we believe video avatars will be the most common form factor for conversational AI. Most people would rather watch something than read it. The problem is that generating video in real-time is hard, and overcoming the uncanny valley is even harder.<p>We haven\u2019t broken the uncanny valley yet. Nobody has. But we\u2019re getting close and our photorealistic avatars are currently best-in-class (judge for yourself: <a href=\"https://lemonslice.com/try/taylor\">https://lemonslice.com/try/taylor</a>). Plus, we're the only avatar model that can do animals and heavily stylized cartoons. Try it: <a href=\"https://lemonslice.com/try/alien\">https://lemonslice.com/try/alien</a>. Warning! Talking to this little guy may improve your mood.<p>Today we're releasing our new model* - Lemon Slice 2, a 20B-parameter diffusion transformer that generates infinite-length video at 20fps on a single GPU - and opening up our API.<p>How did we get a video diffusion model to run in real-time? There was no single trick, just a lot of them stacked together. The first big change was making our model causal. Standard video diffusion models are bidirectional (they look at frames both before and after the current one), which means you can't stream.<p>From there it was about fitting everything on one GPU. We switched from full to sliding window attention, which killed our memory bottleneck. We distilled from 40 denoising steps down to just a few - quality degraded less than we feared, especially after using GAN-based distillation (though tuning that adversarial loss to avoid mode collapse was its own adventure).<p>And the rest was inference work: modifying RoPE from complex to real (this one was cool!), precision tuning, fusing kernels, a special rolling KV cache, lots of other caching, and more. We kept shaving off milliseconds wherever we could and eventually got to real-time.<p>We set up a guest playground for HN so you can create and talk to characters without logging in: <a href=\"https://lemonslice.com/hn\">https://lemonslice.com/hn</a>. For those who want to build with our API (we have a new LiveKit integration that we\u2019re pumped about!), grab a coupon code in the HN playground for your first Pro month free ($100 value). See the docs: <a href=\"https://lemonslice.com/docs\">https://lemonslice.com/docs</a>. Pricing is usage-based at $0.12-0.20/min for video generation.<p>Looking forward to your feedback! And we\u2019d love to see any cool characters you make - please share their links in the comments<p>*We did a Show HN last year for our V1 model: <a href=\"https://news.ycombinator.com/item?id=43785044\">https://news.ycombinator.com/item?id=43785044</a>. It was technically impressive but so bad compared to what we have today.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46783600\">https://news.ycombinator.com/item?id=46783600</a></p>\n<p>Points: 10</p>\n<p># Comments: 4</p>",
      "author": "lcolucci",
      "published_date": "2026-01-27T17:55:15+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 450,
      "reading_time": 2,
      "created_at": "2026-01-27T18:41:24.480757+00:00",
      "updated_at": "2026-01-27T18:41:24.480766+00:00"
    },
    {
      "id": "bcb6b72cd62b8e161b8fb3f40caa20b0",
      "url": "http://ieeexplore.ieee.org/document/11125831",
      "title": "HapticGiant: A Novel Very Large Kinesthetic Haptic Interface With Hierarchical Force Control",
      "content": "Research in virtual reality and haptic technologies has consistently aimed to enhance immersion. While advanced head-mounted displays are now commercially available, kinesthetic haptic interfaces still face challenges such as limited workspaces, insufficient degrees of freedom, and kinematics not matching the human arm. In this paper, we present HapticGiant, a novel large-scale kinesthetic haptic interface designed to match the properties of the human arm as closely as possible and to facilitate natural user locomotion while providing full haptic feedback. The interface incorporates a novel admittance-type force control scheme, leveraging hierarchical optimization to render both arbitrary serial kinematic chains and Cartesian admittances. Notably, the proposed control scheme natively accounts for system limitations, including joint and Cartesian constraints, as well as singularities. Experimental results demonstrate the effectiveness of HapticGiant and its control scheme, paving the way for highly immersive virtual reality applications.",
      "author": "",
      "published_date": "2025-08-14T13:17:43+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 139,
      "reading_time": 1,
      "created_at": "2026-01-27T17:55:56.892473+00:00",
      "updated_at": "2026-01-27T18:31:36.471417+00:00",
      "metadata": {
        "processed_at": "2026-01-27T18:31:36.471430+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "76c23c5300399082bb749ca190929d23",
      "url": "http://ieeexplore.ieee.org/document/11130394",
      "title": "\u201cPersuasive Vibrations\u201d: Studying the Influence of Vibration Parameters on Speech Persuasion",
      "content": "This paper investigates the notion of \u201cPersuasive Vibrations\u201d, which showed that augmenting a person's speech with vibrotactile feedback could artificially increase persuasion. However, while the initial paper has shown the effect, the underlying reasons why vibrations enhance persuasion remain unknown. Through two different user studies, this paper aims to study how the underlying parameters of the vibratory feedback (e.g., frequency, amplitude, or audio-vibration synchronization) influence persuasion. The first study aimed to identify the parameters of vibrotactile feedback that can positively influence persuasion. The second study evaluated vibrotactile feedback that might impair the persuasive effect. In a nutshell, the first experiment suggests that the isolation of different properties of the vibratory signal could tend to provide higher persuasion compared to no vibratory feedback. A lower frequency at 100 Hz seems the most efficient way to generate a persuasive effect. In contrast, the second experiment suggests that some alteration of the vibratory signal (e.g., latency) does not decrease the levels of persuasion compared to the no-vibration condition. All in all, the results suggest that using lower frequencies could have a better effect on persuasion. These results could serve as a basis for haptic design in applications like videoconferencing, virtual meetings, and training systems where supporting user speech is essential.",
      "author": "",
      "published_date": "2025-08-19T13:17:09+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 207,
      "reading_time": 1,
      "created_at": "2026-01-27T17:55:56.892442+00:00",
      "updated_at": "2026-01-27T18:31:36.471440+00:00",
      "metadata": {
        "processed_at": "2026-01-27T18:31:36.471442+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "23dd8d4f1e90a8b410a6b21713fcf022",
      "url": "http://ieeexplore.ieee.org/document/11121155",
      "title": "A Survey on Tactile Change Blindness",
      "content": "While vibrotactile displays continue to gain popularity, it remains that the phenomenon of tactile change blindness negatively impacts the human ability to detect changes between and within tactile signals. This paper surveys the research literature on tactile change detection and blindness under various parameters, including the number of tactors used, the intensity and length of the stimulus, and whether distractors between stimuli (i.e., transients) were used during experimentation, among others. The goal of this survey is to summarize what has been done in an attempt to better understand the parameters that exacerbate tactile change blindness and identify potential areas of future research. When such an understanding is reached, the design of haptic and multimodal displays may ideally be improved.",
      "author": "",
      "published_date": "2025-08-08T13:16:47+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 119,
      "reading_time": 1,
      "created_at": "2026-01-27T17:55:56.892400+00:00",
      "updated_at": "2026-01-27T18:31:36.471444+00:00",
      "metadata": {
        "processed_at": "2026-01-27T18:31:36.471446+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "87344862fd4253ecc468dd068b2c0436",
      "url": "http://ieeexplore.ieee.org/document/11284873",
      "title": "From Restoration to Augmentation: New Approaches to Haptic Feedback for Artificial Limbs",
      "content": "Haptic feedback is essential for precise motor control, making its integration into artificial limbs a critical design challenge. Current approaches to restoring lost tactile input in patients have focused on interfacing with somatosensory pathways at the brain, nerve, or skin level, achieving partial restoration. However, augmentative artificial limbs, devices that provide novel movement capabilities beyond the biological body, pose unique challenges. These limbs lack dedicated sensory pathways, raising fundamental questions about how to deliver tactile feedback for these devices without disrupting existing somatosensory function. A promising direction lies in exploiting intrinsic tactile feedback, which emerges naturally at the interface between wearable devices and the body. When an artificial limb moves or interacts with objects, the skin detects rich tactile cues transmitted through this interface. Amplifying and refining this intrinsic feedback, via materials optimized for transmission of tactile signals and wearable designs, could enable more intuitive and interpretable haptic feedback for augmentative limbs. This approach offers a pathway toward enhancing embodiment and motor control of augmentative artificial limbs.",
      "author": "",
      "published_date": "2025-12-08T13:16:55+00:00",
      "source": "Transactions Haptics",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2026-01-27T17:55:56.892372+00:00",
      "updated_at": "2026-01-27T18:31:36.471448+00:00",
      "metadata": {
        "processed_at": "2026-01-27T18:31:36.471450+00:00",
        "processing_method": "github_actions"
      }
    }
  ]
}