{
  "last_updated": "2025-11-27T06:34:38.991441+00:00",
  "count": 20,
  "articles": [
    {
      "id": "e3720077385c5b7632d6af62297ccf38",
      "url": "https://www.frontiersin.org/articles/10.3389/fnhum.2025.1666476",
      "title": "Visuomotor adaptation enhances representational acuity without altering spatial bias",
      "content": "IntroductionPrism adaptation is a well-established paradigm for studying sensorimotor plasticity, known to produce not only motor after-effects but also changes in spatial cognition. Whether visuomotor rotation\u2014a similar form of sensorimotor adaptation\u2014elicits comparable cognitive transfer remains unclear.MethodsParticipants performed visuomotor rotation tasks involving either leftward or rightward 15\u00b0 rotations. The perturbation was introduced either abruptly (within one trial) or gradually (over 34 trials). To assess potential cognitive transfer, participants completed a perceptual line bisection task before and after adaptation.ResultsNo condition (leftward/rightward or abrupt/gradual) induced measurable cognitive after-effects in line bisection performance, indicating an absence of transfer from sensorimotor to spatial-cognitive domains. However, a novel finding emerged: visuomotor rotation enhanced participants\u2019 representational acuity, reflected in improved sensitivity when judging the midpoint of a line. This effect was most pronounced following gradual perturbations and persisted beyond the adaptation phase.DiscussionThese findings demonstrate a clear dissociation between the cognitive and perceptual consequences of visuomotor adaptation. Visuomotor rotation thus provides a reliable means to study sensorimotor plasticity without altering spatial representation\u2014a methodological advantage for investigating populations with atypical spatial biases. The enhancement of representational acuity further suggests that sensorimotor learning can refine spatial discrimination independently of cognitive recalibration.",
      "author": "Olivier White",
      "published_date": "2025-11-27T00:00:00+00:00",
      "source": "Frontiers Human Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 191,
      "reading_time": 1,
      "created_at": "2025-11-27T06:33:54.755419+00:00",
      "updated_at": "2025-11-27T06:33:54.755424+00:00"
    },
    {
      "id": "06e23127c9c25994f364d4708efde8a3",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1724872",
      "title": "taVNS as a potential countermeasure for neurocognitive decline in microgravity",
      "content": "Exposure to microgravity induces significant physiological, cognitive, and psychomotor changes in the human body. While countermeasures such as resistance exercise and cardiovascular conditioning have been developed to address musculoskeletal and circulatory issues, there remains a critical gap in mitigating neurophysiological and cognitive deficits caused by microgravity. Transcutaneous auricular vagus nerve stimulation (taVNS) is a non-invasive neuromodulation technique that promises to enhance psychomotor function, and cognitive performance in microgravity as well as on Earth. This article examines the challenges of spaceflight, particularly cognitive impairments and related psychomotor dysfunction, and explores the potential application of taVNS in space. The neurophysiological mechanisms underlying microgravity-related decline and the proposed mechanism of action of taVNS are discussed, focusing on its effects on neuroplasticity, autonomic regulation, and sensorimotor integration. taVNS emerges as a promising countermeasure to mitigate neuropsychological impairments associated with exposure to microgravity.",
      "author": "T. Zaehle",
      "published_date": "2025-11-27T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 138,
      "reading_time": 1,
      "created_at": "2025-11-27T06:33:52.008533+00:00",
      "updated_at": "2025-11-27T06:33:52.008535+00:00"
    },
    {
      "id": "1539af32ba0500fa6f89804239e41c8f",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1725953",
      "title": "Cardiac structure and inflammation drive memory impairment via dual pathways of heart-brain axis dysregulation in atrial fibrillation patients undergoing catheter ablation",
      "content": "BackgroundAtrial fibrillation (AF) patients undergoing catheter ablation (CA) frequently present with cardiac structural and functional alterations and persistent memory impairment. This study aimed to investigate how cardiac structure and function impacts memory-related brain structure and function, whether CA reverses impaired memory networks, and to delineate the dual-pathway regulatory mechanism of the heart-brain axis underlying AF-associated memory deficits.MethodsThirty-eight AF patients underwent longitudinal assessments [memory function tests, clinical evaluations including blood biomarkers and cardiac function, structural/functional magnetic resonance imaging (MRI)] before CA and on postoperative day 7. Forty-five healthy controls (CN) were included for comparison. Hippocampal functional connectivity (FC) and voxel-based morphometry were used to quantify memory-related brain networks and gray matter (GM) volume. Bivariate correlations explored relationships between GM alterations, hippocampal FC, memory performance, and clinical features (cardiac structural parameters and blood-borne inflammatory markers).ResultsFirst, compared with CN, AF patients showed memory impairment, reduced GM volume in the bilateral calcarine cortex, cuneus, lingual gyrus, inferior/middle temporal gyri, and left fusiform gyrus, and increased hippocampal FC with the bilateral middle frontal gyrus, insula, Rolandic operculum, left inferior frontal gyrus (opercular/orbital/triangular parts), and right postcentral/supramarginal/superior temporal gyri. Second, left ventricular end-diastolic diameter (LVDd) was positively associated with GM volume in the left middle temporal gyrus (MTG.L, p = 0.016), right inferior temporal gyrus (p = 0.006), and left cuneus (p = 0.026); MTG.L GM volume correlated positively with Auditory Verbal Learning Test (AVLT)-Recall scores (p = 0.044), while hippocampal FC with the right postcentral gyrus correlated negatively with both inflammatory markers (PCT, p = 0.010) and AVLT-Delayed Recall (20 min) scores (p = 0.013). Third, post catheter ablation (post-CA), AF patients exhibited increased hippocampal FC with the right middle frontal gyrus, right midcingulate cortex, and left superior frontal gyrus, and decreased FC with the right lingual gyrus and calcarine cortex.ConclusionCardiac structural parameters (LVDd) associate with memory-related brain atrophy, whereas blood-borne inflammatory markers link to hippocampal memory network dysregulation\u2014two distinct pathways driving AF-related memory impairment. These findings clarify the dual-pathway regulatory mechanism of the heart-brain axis, offering novel insights into AF-associated cognitive dysfunction and potential CA-mediated memory recovery.",
      "author": "Jiu Chen",
      "published_date": "2025-11-27T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 343,
      "reading_time": 1,
      "created_at": "2025-11-27T06:33:52.008500+00:00",
      "updated_at": "2025-11-27T06:33:52.008502+00:00"
    },
    {
      "id": "7fe0158ddfa6e2dca39219ceb4882c42",
      "url": "https://www.frontiersin.org/articles/10.3389/fnins.2025.1715236",
      "title": "The effects of polarization characteristics on visual fatigue: an empirical study based on subjective and objective indicators",
      "content": "IntroductionThis study aimed to examine the impact of circularly polarized versus linearly polarized displays on visual health, with a focus on visual fatigue during prolonged screen use.MethodsEighteen healthy adults participated in a within-subject design. Under controlled illumination and display parameters, participants performed a 40-min standardized visual task. Electroencephalography (EEG) and Vertical electrooculography (VEOG) were recorded concurrently, while subjective questionnaires were administered to assess visual fatigue and alertness.ResultsCompared with linearly polarized displays, circularly polarized displays were associated with significantly smaller increases in subjective visual fatigue and smaller declines in alertness (ps < 0.05). Blink rate remained stable under circular polarization but increased significantly under linear polarization and higher than in circular polarization (p < 0.05). EEG analyses revealed that circular polarization preserved stable neural activity, whereas linear polarization elicited a significant increase in the (\u03b8 + \u03b1)/\u03b2 ratio and a significant decrease in alpha centroid frequency (ps < 0.05), indicating reduced cortical activation and slowed neural processing.ConclusionCircularly polarized displays, by more closely resembling the optical properties of natural light, can effectively mitigate visual fatigue induced by prolonged screen viewing. These findings provide both theoretical insights and empirical evidence to inform the development of healthier display technologies and ergonomics-related standards.",
      "author": "Yunhong Zhang",
      "published_date": "2025-11-27T00:00:00+00:00",
      "source": "Frontiers Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 198,
      "reading_time": 1,
      "created_at": "2025-11-27T06:33:52.008439+00:00",
      "updated_at": "2025-11-27T06:33:52.008443+00:00"
    },
    {
      "id": "afef520a97a582ce9a61f922c8ae8019",
      "url": "https://www.sandia.gov/labnews/2024/04/04/willis-whitfield-a-simple-man-with-a-simple-solution-that-changed-the-world/",
      "title": "Willis Whitfield: A simple man with a simple solution that changed the world",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46039387\">Comments</a>",
      "author": "",
      "published_date": "2025-11-24T21:12:25+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2025-11-27T06:33:23.547167+00:00",
      "updated_at": "2025-11-27T06:33:23.547169+00:00"
    },
    {
      "id": "a8adb8e21ba8ade167865b32c18747a2",
      "url": "http://ieeexplore.ieee.org/document/11235877",
      "title": "Separate Timescales for Spatial and Anatomical Information Processing of Body Stimuli",
      "content": "Observing different body stimuli can influence the speed and accuracy of our responses. Prior work indicates this effect is influenced by factors such as spatial congruence and perspective. We hypothesized that the influence of these factors would vary depending on the amount of time that participants had to process visual stimuli. Experiment 1 was a RT task (n = 29) with stimuli varying in spatial congruence (congruent, incongruent, neutral), perspective (first- or third-person), and stimulus type (body or control). Experiment 2 (n = 50) used the same stimuli in a \u201cForced Response\u201d paradigm, which controlled the time participants had to prepare a response. This allowed us to assess responses as a function of preparation time. Experiment 1 showed effects of spatial congruence, with longer RTs and more errors for spatially incongruent stimuli. This effect was greater for body stimuli. Experiment 2 showed that spatial information was processed faster than anatomical information, inducing incorrect responses at short preparation times for spatially incongruent body stimuli. There was little-to-no corresponding effect for control stimuli. Both experiments also showed weak-to-no effects of perspective, which appear to have been driven by spatial congruence. Our results indicate that spatial information is processed faster than anatomical information during observation of body stimuli. These data are consistent with the dual visual streams hypothesis, whereby spatial information would be processed rapidly via the dorsal stream, whereas anatomical processing would occur later via the ventral stream. These data also indicate differences in processing between body and control stimuli.",
      "author": "",
      "published_date": "2025-11-07T13:16:23+00:00",
      "source": "Cognitive Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 248,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:55.672821+00:00",
      "updated_at": "2025-11-27T06:24:41.729563+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729573+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "fb588439a33dbfe3eb0b3edf79a54446",
      "url": "http://ieeexplore.ieee.org/document/11235874",
      "title": "Transient Inhibition of the Posterior Parietal Cortex Affects Action-related But Not Action-unrelated Visual Processing during Path Integration",
      "content": "Path integration refers to the ability to monitor self-motion cues to keep track of changes in position and orientation. This function is often assumed to rely predominantly on medial temporal lobe structures containing grid, place, and head direction cells. Recent evidence, however, suggests that key navigational computations may occur outside this system, for example, in posterior parietal areas. Here, we adopted a novel perspective derived from animal research and examined whether human path integration relies on processing streams in the posterior parietal cortex (PPC), depending on the involvement of actively controlled motion as opposed to passive perception of visual optic flow. We compared the effects of inhibiting the PPC via TMS on two path integration tasks in a virtual reality, only one of which involved active control of a visually simulated forward movement. Behavioral performance showed that distance judgments were selectively affected in the action-related path integration task. This finding shows that the processing of actively controlled motion depends on computations in the PPC, whereas passive processing of optic flow is largely independent of the PPC computations. Our results reinforce the hypothesis that the PPC plays a critical role for the integration of goal locations and self-positional signals within an egocentric frame of reference. In addition to the medial temporal lobe, the posterior parietal system is recruited during tasks involving actively controlled movements, whereas medial temporal computations are sufficient for passive monitoring of positional changes.",
      "author": "",
      "published_date": "2025-11-07T13:16:23+00:00",
      "source": "Cognitive Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 235,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:55.672770+00:00",
      "updated_at": "2025-11-27T06:24:41.729577+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729578+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "2b015497e36bc6db03bb62ed1bfea673",
      "url": "https://www.sciencedirect.com/science/article/pii/S0006899325006031?dgcid=rss_sd_all",
      "title": "Mitochondria serve as indispensable components of neuron-glia crosstalk in the trajectory of Alzheimer\u2019s disease",
      "content": "<p>Publication date: 1 January 2026</p><p><b>Source:</b> Brain Research, Volume 1870</p><p>Author(s): Maryam Sardari, Oveis Hosseinzadeh Sahafi, Ameneh Rezayof</p>",
      "author": "",
      "published_date": null,
      "source": "Brain Research",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 16,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:46.479425+00:00",
      "updated_at": "2025-11-27T06:24:41.729583+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729585+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "b84f4acfaa385c55c9bcc74850be8c16",
      "url": "http://doi.org/10.1037/cns0000380",
      "title": "Sensory-processing sensitivity as a confounder in the positive relationship between mindful awareness and psychological distress: A theoretical review.",
      "content": "Mindfulness meditation is credited as a positive driver of promoting psychological well-being and reducing stress, anxiety, and depression symptoms. However, dispositional mindfulness has been somewhat correlated with psychological distress, as awareness has been positively correlated with psychological symptoms and negative affective states in many studies. This counterintuitive phenomenon has been tentatively explained in a variety of ways, including a wrong interpretation of the items of the mindfulness assessment scales in nonmeditators. The most credited explanation is that increasing attention to present-moment experiences would boost affective reaction to negative experiences and therefore exacerbate related psychological symptoms. This hypothesis is unsatisfactory, as there is much contrasting evidence in this regard. Therefore, we propose a new hypothesis: in dispositional studies, the assessment of the awareness skill of mindfulness would be affected by sensory-processing sensitivity, which could be a confounder in its relationship with psychological distress. Sensory-processing sensitivity refers to a temperamental trait characterized by both awareness of sensorial stimulation and reactivity to experience. Thus, highly sensitive persons usually report increased awareness of subtleties in the environment, ease of overstimulation, and increased affective reaction to stimulation. In support of our hypothesis, we showed in particular how the most widely used scale for assessing mindful awareness could be paired with and interpreted as a measure of sensory-processing sensitivity. We then propose a set of testable hypotheses to drive future research on this topic. If supported by future experimental results, our hypothesis would shed new light on the overall field of dispositional mindfulness studies. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2023-11-02T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 257,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241481+00:00",
      "updated_at": "2025-11-27T06:24:41.729587+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729588+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a89cc9d9bab0838b2e06072add1ef2ed",
      "url": "http://doi.org/10.1037/cns0000335",
      "title": "A shared perceptual inference for cross-modally induced illusions of self-attribution.",
      "content": "The representation of our own body is malleable. Evidence indicates that multisensory stimulation can trigger an illusory sense of ownership over a fake hand, a partner\u2019s face, or a virtual body. Despite our understanding of the processes supporting the construction of bodily self, we know less about the processes that trigger illusory ownership of nonbody attributes (e.g., voice during articulation) and about whether multisensory stimulation can drive a shared inference across distinct attributes. Here, we compared the classic rubber hand illusion with another multisensory illusion that elicits a sense of ownership over a stranger\u2019s voice during talking. We observed that, given congruent multisensory input, the degree to which one perceived the sense of ownership over the fake hand predicted the degree to which one perceived the sense of ownership over the stranger\u2019s voice, after controlling for task demand and suggestibility. Thus, our results provide evidence for a shared inference supporting subjective sense of self across fundamentally different attributes. We suggest that individual reliance on multisensory signals to drive such an inference can be further explored. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2022-08-25T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 184,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241440+00:00",
      "updated_at": "2025-11-27T06:24:41.729591+00:00",
      "metadata": {
        "processed_at": "2025-11-27T06:24:41.729592+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "fa250840ddf6808c93613cad856c7c25",
      "url": "http://doi.org/10.1037/cns0000353",
      "title": "Unmuting lucid dreams: Speech decoding and vocalization in real time.",
      "content": "Since the 1970s, scientists have been searching for ways to communicate with people in lucid dreams (LDs), during which it is possible to maintain consciousness. Previously, dreamers could hear sounds from reality and respond with some simple signals, but they could not speak back. In this study, facial surface electromyography (EMG) was tested as a proof of concept for unmuting people in LDs. Remmyo, an EMG distinctive constructed language, was used. The software was developed to translate facial EMG impulses into Remmyo sounds and letters, translate words into English, and digitally vocalize the final text in English. Four LD practitioners were trained to pronounce a short phrase or a word in Remmyo and were then asked to achieve the same task in LDs under polysomnographic observation. LDs were verified by preagreed eye movements in rapid eye movement (REM) sleep. Four volunteers tried to speak in Remmyo in 15 LDs. Due to software failures, mispronunciations, and missing sounds, the decoding efficiency in real time or in recordings ranged from 13% to 81%. The first phrase and word heard from sleeping people were \u201cno war\u201d and \u201cfreedom.\u201d The later was automatically translated and vocalized in English in real time for 11 times. Despite controversial results, the study shows that, with further development, people could possibly talk in LDs and could be heard in reality with the help of EMG sensors. To achieve this goal, a range of possible obstacles is discussed. This technology could provide opportunities for LD studies and their practical applications. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2023-03-13T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 260,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241402+00:00",
      "updated_at": "2025-11-27T05:44:11.241405+00:00"
    },
    {
      "id": "3cace5c5d9bdc4eeebb05365c3e99538",
      "url": "http://doi.org/10.1037/cns0000402",
      "title": "Creating a world in the head: The conscious apprehension of neural content originating from internal sources.",
      "content": "Klein et al. (2023) argued that the evolutionary transition from respondent to agent during the Cambrian explosion would be a promising vantage point from which to gain insight into the evolution of organic sentience. They focused on how increased competition for resources\u2014in consequence of the proliferation of new, neurally sophisticated life-forms\u2014made awareness of the external world (in the service of agentic acts) an adaptive priority. The explanatory scope of Klein et al. (2023) was limited to consideration of the conscious apprehension of externally sourced content\u2014that is, content delivered from the sensory registration of objects occupying phenomenal space. But consciousness\u2014at least for humans\u2014takes its objects from internal as well as external sources. In the present article, we extend their analysis to the question of how internally sourced content (i.e., mental states) became the object of conscious apprehension. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2024-09-09T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 145,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241357+00:00",
      "updated_at": "2025-11-27T05:44:11.241359+00:00"
    },
    {
      "id": "4dc92cbf63e410e4d59f6ffd2f7dec90",
      "url": "http://doi.org/10.1037/cns0000406",
      "title": "Not all minds think alike: Examining the impact of time and task on visual and verbal thought.",
      "content": "Research suggests that individuals have different phenomenological experiences across various tasks. However, little is known about how these experiences vary by task or over time. This study examined participants\u2019 experiences of task-unrelated thoughts (i.e., TUTs), visual, and verbal thoughts across two experimental sessions and two different tasks. In addition, we examined relations between participants\u2019 thoughts and key individual difference factors. In Session 1, participants (<em>n</em> = 85) engaged in a focused-attention meditation and a reading task, then completed a second identical session with a new text. Throughout both tasks, participants were prompted to report on the characteristics of their thoughts. Participants\u2019 ratings of TUT, visual, and verbal thoughts were subject to change over time. Furthermore, on average, participants visualized more and had fewer TUTs while reading compared to meditation; however, no task difference was found for verbal-thinking reports. This suggests that visual imagery is more malleable than verbal-thinking. There was a strong negative correlation between visual and verbal thoughts, suggesting that at any given time, individuals\u2019 thoughts tended to be either predominantly visual or verbal. Finally, individual differences in the tendency to become immersed in narratives and motivation to engage with other people\u2019s perspectives (i.e., mind-reading motivation) were related to higher reports of visual imagery during reading, whereas verbal-thinking was negatively associated with mind-reading motivation and unrelated to TUT. Overall, this study revealed that individuals\u2019 phenomenological experiences vary during tasks and across time, providing a foundation for future work to examine why and how variability in these phenomenological experiences emerge. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2024-10-14T00:00:00+00:00",
      "source": "Clinical Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 259,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:11.241322+00:00",
      "updated_at": "2025-11-27T05:44:11.241324+00:00"
    },
    {
      "id": "c88e3d8b4ab22943bcc6cf02ea5b3b61",
      "url": "https://www.collaboraonline.com/blog/collabora-online-now-available-on-desktop/",
      "title": "Collabora Online Desktop Released with Improved UI from LibreOffice",
      "content": "<p>Article URL: <a href=\"https://www.collaboraonline.com/blog/collabora-online-now-available-on-desktop/\">https://www.collaboraonline.com/blog/collabora-online-now-available-on-desktop/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46064210\">https://news.ycombinator.com/item?id=46064210</a></p>\n<p>Points: 8</p>\n<p># Comments: 0</p>",
      "author": "nogajun",
      "published_date": "2025-11-27T00:59:58+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2025-11-27T05:44:01.054840+00:00",
      "updated_at": "2025-11-27T05:44:01.054842+00:00"
    },
    {
      "id": "eaac861dd37ec58db34c6268ac7c5602",
      "url": "https://arxiv.org/abs/2511.21000",
      "title": "PileUp: A Tufting Approach to Soft, Tactile, and Volumetric E-Textile Interfaces",
      "content": "arXiv:2511.21000v1 Announce Type: new \nAbstract: We present PileUp, a tufted pile e-textile sensing approach that offers unique affordances through the tactile expressiveness and richness of its continuous, threaded-volume construction. By integrating conductive yarns in looped or cut pile forms, PileUp transforms soft 3-dimensional textiles into multimodal sensors capable of detecting mechanical deformations such as pressure, bending, and strain, as well as environmental conditions like moisture. We propose a design space that outlines the relationships between texture, form factor, and sensing affordances of tufted textiles. We characterize electrical responses under compression, bending, and strain, reporting sensor behaviors. To demonstrate versatility, we present three application scenarios in which PileUp sensors are seamlessly integrated into soft fabrics: a meditation rug with multi-zone sensing, a fleece sleeve that detects arm motion, and a moisture-sensing wall art. Our results establish tufting as an accessible yet expressive fabrication method for creating integrated sensing textiles, distinguishing our work from traditional flat textile sensors.",
      "author": "Seoyoung Choi, Rashmi Balegar Mohan, Heather Jin Hee Kim, Jisoo Ha, Jeyeon Jo",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 156,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600890+00:00",
      "updated_at": "2025-11-27T05:22:22.600891+00:00"
    },
    {
      "id": "a2add133d5520496c5fa803121537354",
      "url": "https://arxiv.org/abs/2511.20791",
      "title": "Beyond the Legal Lens: A Sociotechnical Taxonomy of Lived Privacy Incidents and Harms",
      "content": "arXiv:2511.20791v1 Announce Type: new \nAbstract: To understand how privacy incidents lead to harms, HCI researchers have historically leveraged legal frameworks. However, these frameworks expect acute, tangible harms and thus may not cover the full range of human experience relevant to modern-day digital privacy. To address this gap, our research builds upon these existing frameworks to develop a more comprehensive representation of people's lived experiences with privacy harms. We analyzed 369 privacy incidents reported by individuals from the general public. We found a broader range of privacy incidents and harms than accounted for in existing legal frameworks. The majority of reported privacy harms were not based on tangible harm, but on fear and loss of psychological safety. We also characterize the actors, motives, and information associated with various incidents. This work contributes a new framework for understanding digital privacy harms that can be utilized both in research and practice.",
      "author": "Kirsten Chapman, Garrett Smith, Kaitlyn Klabacka, Harrison Winslow, Louise Barkhuus, Cori Faklaris, Sauvik Das, Pamela Wisniewski, Bart Piet Knijnenburg, Heather Lipford, Xinru Page",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 148,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600860+00:00",
      "updated_at": "2025-11-27T05:22:22.600862+00:00"
    },
    {
      "id": "292b88c6e85f21229007b0c743726ef5",
      "url": "https://arxiv.org/abs/2511.20660",
      "title": "Transforming Higher Education with AI-Powered Video Lectures",
      "content": "arXiv:2511.20660v1 Announce Type: new \nAbstract: The integration of artificial intelligence (AI) into video lecture production has the potential to transform higher education by streamlining content creation and enhancing accessibility. This paper investigates a semi automated workflow that combines Google Gemini for script generation, Amazon Polly for voice synthesis, and Microsoft PowerPoint for video assembly. Unlike fully automated text to video platforms, this hybrid approach preserves pedagogical intent while ensuring script to slide synchronization, narrative coherence, and customization. Case studies demonstrate the effectiveness of Gemini in generating accurate and context-sensitive scripts for visually rich academic presentations, while Polly provides natural-sounding narration with controllable pacing. A two course pilot study was conducted to evaluate AI generated instructional videos (AIIV) against human instructional videos (HIV). Both qualitative and quantitative results indicate that AIIVs are comparable to HIVs in terms of learning outcomes, with students reporting high levels of clarity, coherence, and usability. However, limitations remain, particularly regarding audio quality and the absence of human-like avatars. The findings suggest that AI assisted video production can reduce instructor workload, improve scalability, and deliver effective learning resources, while future improvements in synthetic voices and avatars may further enhance learner engagement.",
      "author": "Dengsheng Zhang",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 194,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600832+00:00",
      "updated_at": "2025-11-27T05:22:22.600834+00:00"
    },
    {
      "id": "b2b929e629b19ee72ecef79bdd055d3e",
      "url": "https://arxiv.org/abs/2511.20659",
      "title": "Iteration and Co-design of a Physical Web Application for Outdoor Activities with Older Adults",
      "content": "arXiv:2511.20659v1 Announce Type: new \nAbstract: Existing research and physical activity guidelines highlight the benefits of outdoor physical activities for ageing populations. There is potential for technology to facilitate outdoor activity through Physical Web infrastructure. We proposed that embedding Physical Web applications that are engaging and interactive in public open spaces as part of interactive wellness parks can encourage older adults to participate in physical activities outdoors and motivate rehabilitation. We have created an initial design prototype based on design requirements generated from a qualitative field study with 24 older adults to explore their perceptions, experiences, and routines of outdoor physical activities. In this paper, we present an initial prototype and findings from a co-design session with 12 older adults, eliciting their feedback on the design and their ideas for future design iterations.",
      "author": "Fatima Badmos, Emma Murphy, Michael Ward, Damon Berry",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 132,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600800+00:00",
      "updated_at": "2025-11-27T05:22:22.600801+00:00"
    },
    {
      "id": "e0b602457ea2e79ac669aac107f0817e",
      "url": "https://arxiv.org/abs/2511.20657",
      "title": "Intelligent Agents with Emotional Intelligence: Current Trends, Challenges, and Future Prospects",
      "content": "arXiv:2511.20657v1 Announce Type: new \nAbstract: The development of agents with emotional intelligence is becoming increasingly vital due to their significant role in human-computer interaction and the growing integration of computer systems across various sectors of society. Affective computing aims to design intelligent systems that can recognize, evoke, and express human emotions, thereby emulating human emotional intelligence. While previous reviews have focused on specific aspects of this field, there has been limited comprehensive research that encompasses emotion understanding, elicitation, and expression, along with the related challenges. This survey addresses this gap by providing a holistic overview of core components of artificial emotion intelligence. It covers emotion understanding through multimodal data processing, as well as affective cognition, which includes cognitive appraisal, emotion mapping, and adaptive modulation in decision-making, learning, and reasoning. Additionally, it addresses the synthesis of emotional expression across text, speech, and facial modalities to enhance human-agent interaction. This paper identifies and analyzes the key challenges and issues encountered in the development of affective systems, covering state-of-the-art methodologies designed to address them. Finally, we highlight promising future directions, with particular emphasis on the potential of generative technologies to advance affective computing.",
      "author": "Raziyeh Zall, Alireza Kheyrkhah, Erik Cambria, Zahra Naseri, M. Reza Kangavari",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 190,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600764+00:00",
      "updated_at": "2025-11-27T05:22:22.600766+00:00"
    },
    {
      "id": "a72601fde4a9ad0eaa6844b5ec47b8b8",
      "url": "https://arxiv.org/abs/2511.20656",
      "title": "Context-Aware Visual Prompting: Automating Geospatial Web Dashboards with Large Language Models and Agent Self-Validation for Decision Support",
      "content": "arXiv:2511.20656v1 Announce Type: new \nAbstract: The development of web-based geospatial dashboards for risk analysis and decision support is often challenged by the difficulty in visualization of big, multi-dimensional environmental data, implementation complexity, and limited automation. We introduce a generative AI framework that harnesses Large Language Models (LLMs) to automate the creation of interactive geospatial dashboards from user-defined inputs including UI wireframes, requirements, and data sources. By incorporating a structured knowledge graph, the workflow embeds domain knowledge into the generation process and enable accurate and context-aware code completions. A key component of our approach is the Context-Aware Visual Prompting (CAVP) mechanism, which extracts encodes and interface semantics from visual layouts to guide LLM driven generation of codes. The new framework also integrates a self-validation mechanism that uses an agent-based LLM and Pass@k evaluation alongside semantic metrics to assure output reliability. Dashboard snippets are paired with data visualization codebases and ontological representations, enabling a pipeline that produces scalable React-based completions using the MVVM architectural pattern. Our results demonstrate improved performance over baseline approaches and expanded functionality over third party platforms, while incorporating multi-page, fully functional interfaces. We successfully developed a framework to implement LLMs, demonstrated the pipeline for automated code generation, deployment, and performed chain-of-thought AI agents in self-validation. This integrative approach is guided by structured knowledge and visual prompts, providing an innovative geospatial solution in enhancing risk analysis and decision making.",
      "author": "Haowen Xu, Jose Tupayachi, Xiao-Ying Yu",
      "published_date": "2025-11-27T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 230,
      "reading_time": 1,
      "created_at": "2025-11-27T05:22:22.600732+00:00",
      "updated_at": "2025-11-27T05:22:22.600733+00:00"
    }
  ]
}