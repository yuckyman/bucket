{
  "last_updated": "2025-11-03T05:22:16.353389+00:00",
  "count": 20,
  "articles": [
    {
      "id": "d1b3a64c1957f2b048e1e94f5d37c6e5",
      "url": "https://erpinfo.org/blog/2024/3/15/registration-full",
      "title": "Registration is now full for the 2024 ERP Boot Camp",
      "content": "<p class=\"\">The demand for the<a href=\"https://erpinfo.org/2024-erp-boot-camp\"> 2024 ERP Boot Camp</a> was far beyond our expectations, and we reached our maximum registration of 30 people within one day. We already have a waiting list of over 30 people, so we have closed the registration site.</p><p class=\"\">We realize that this is very disappointing to many people. We hope to offer another workshop like this next summer, or possibly earlier.</p><p class=\"\">If you would like to get announcements about upcoming boot camps and webinars, you should <a href=\"https://erpinfo.org/bootcamp-email-list\">join our email list</a>.</p><p class=\"\">You may also consider hosting a <a href=\"https://erpinfo.org/mini-erp-boot-camps\">Mini ERP Boot Camp</a> at your institution (in person or over Zoom).</p>",
      "author": "Steve Luck",
      "published_date": "2024-03-16T15:14:42+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 106,
      "reading_time": 1,
      "created_at": "2025-11-03T05:22:07.369585+00:00",
      "updated_at": "2025-11-03T05:22:07.369587+00:00"
    },
    {
      "id": "e1385798428586a67ced89a895faeb47",
      "url": "https://erpinfo.org/blog/2024/6/10/erp-core-decoding-paper",
      "title": "New Paper: Using Multivariate Pattern Analysis to Increase Effect Sizes for ERP Amplitude Comparisons",
      "content": "<p class=\"\">Carrasco, C. D., Bahle, B., Simmons, A. M., &amp; Luck, S. J. (2024). Using multivariate pattern analysis to increase effect sizes for event-related potential analyses. Psychophysiology, 61, e14570. <a href=\"https://doi.org/10.1111/psyp.14570\">https://doi.org/10.1111/psyp.14570</a> [<a href=\"https://doi.org/10.1101/2023.11.07.566051\">preprint</a>]</p><p class=\"\">Multivariate pattern analysis (MVPA) can be used to \u201cdecode\u201d subtle information from ERP signals, such as which of several faces a participant is perceiving or the orientation that someone is holding in working memory (see <a href=\"https://erpinfo.org/blog/2018/9/16/decoding\">this previous blog post</a>). This approach is so powerful that we started wondering whether it might also give us greater statistical power in more typical experiments where the goal is to determine whether an ERP component differs in amplitude across experimental conditions. For example, might we more easily be able to tell if N400 amplitude is different between two different classes of words by using decoding? If so, that might make it possible to detect effects that would otherwise be too small to be significant.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"688\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/08f353c7-f484-4e87-b5d3-a256fe1206e2/N170_ES.png?format=1000w\" width=\"971\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">To address this question, we compared decoding with the conventional ERP analysis approach with using the 6 experimental paradigms in the <a href=\"https://doi.org/10.18115/D5JW4R\">ERP CORE</a>. In the conventional ERP analysis, we measured the mean amplitude during the standard measurement window from each participant in the two conditions of the paradigm (e.g., faces versus cars for N170, deviants versus standards for MMN). We quantified the magnitude of the difference between conditions using Cohen\u2019s <em>dz</em> (the variant of Cohen\u2019s <em>d</em> corresponding to a paired <em>t</em> test). For example, the effect size in the conventional ERP comparison of faces versus cars in the N170 paradigm was approximately 1.7 (see the figure).</p><p class=\"\">We also applied decoding to each paradigm. For example, in the N170 paradigm, we trained a support vector machine (SVM) to distinguish between ERPs elicited by faces and ERPs elicited by cars. This was done separately for each subject, and we converted the decoding accuracy into Cohen\u2019s <em>dz</em> so that it could be compared with the <em>dz</em> from the conventional ERP analysis. As you can see from the bar labeled SVM in the figure above, the effect size for the SVM-based decoding analysis was almost twice as large as the effect size for the conventional ERP analysis. That\u2019s a huge difference!</p><p class=\"\">We found a similar benefit for SVM-based decoding over conventional ERP analyses in 7 of the 10 cases we tested (see the figure below). In the other 3 cases, the ERP and SVM effects were approximately equivalent. So, there doesn\u2019t seem to be a downside to using decoding, at least in terms of effect size. But there can be a big benefit.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1371\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/d16f0782-7205-4d50-95e1-c6729cbc153e/All_Components.png?format=1000w\" width=\"4641\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">Because decoding has many possible benefits, we\u2019ve added it into <a href=\"ERPLAB Toolbox\">ERPLAB Toolbox</a>. It\u2019s super easy to use, and we\u2019ve created <a href=\"https://erpinfo.org/blog/2023/6/23/decoding-webinar\">detailed documentation and a video</a> to explain how it works at a conceptual level and to show you how to use it.</p><p class=\"\">We encourage you to apply it to your own data. It may give you the power to detect effects that are too small to be detected with conventional ERP analyses.</p>",
      "author": "Steve Luck",
      "published_date": "2024-06-10T18:01:45+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 525,
      "reading_time": 2,
      "created_at": "2025-11-03T05:22:07.369558+00:00",
      "updated_at": "2025-11-03T05:22:07.369559+00:00"
    },
    {
      "id": "906f73f5c36ba087882a0ad17e01fc20",
      "url": "https://erpinfo.org/blog/2024/6/11/erplab-studio",
      "title": "New software package: ERPLAB Studio",
      "content": "<p class=\"\">We are excited to announce the release of a new EEG/ERP analysis package, <a href=\"https://github.com/ucdavis/erplab/releases\">ERPLAB Studio</a>. We think it\u2019s a huge improvement over the classic EEGLAB user interface. See our cheesy <a href=\"https://www.youtube.com/watch?v=lIaKVQ9DD6E\">\u201cadvertisement\u201d video</a> to get a quick overview. </p><p class=\"\">Rather than operating as an EEGLAB plugin, ERPLAB Studio is a standalone Matlab program that provides a more efficient and user-friendly interface to the most commonly used EEGLAB and ERPLAB routines.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/c874d4ec-5186-4de9-981b-58010c7a06e1/Interface.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">With ERPLAB Studio, you automatically see the EEG or ERP waveforms as soon as you load a file. And as soon as you perform an operation, you see what the new EEG/ERP looks like. For example, when you filter the data, you immediately see the filtered waveforms.</p><p class=\"\">You can even select multiple datasets and apply an operation like artifact detection on all of them in one step. And then you can immediately see the results, such as which EEG epochs have been marked with artifacts.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/b45f514d-2d21-4a5a-8be6-f3a8ff99c388/Artifacts.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">We give you access to EEGLAB\u2019s ICA-based artifact correction tools, but with a nice bonus. You can plot the ICA activations in the same window with the EEG data, making it easy to see which ICA components correspond to specific artifacts such as eyeblinks.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/8bc191da-9040-4042-ae9c-550cd98def7d/ICA.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">The program has an EEG tab for processing continuous and epoched EEG data, and an ERP tab for processing averaged ERPs.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/84bdd9df-b02e-4fc5-83b9-1139a91938f5/Tabs.jpg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">The automatic ERP plotting makes it easy for you to view the data laid out according to the electrode locations. And we have an Advanced Waveform Viewer that can make publication-quality plots.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1080\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/a932631f-fc30-415f-b11d-660d2bf90da5/ERP.jpeg?format=1000w\" width=\"1920\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">ERPLAB Studio is mainly just a new user interface. Under the hood, we\u2019re running the same EEGLAB and ERPLAB routines you\u2019ve always used. And scripting is identical.</p><p class=\"\">ERPLAB Studio is included in <a href=\"https://github.com/ucdavis/erplab/releases\">version 11 and higher of ERPLAB</a>. You simply follow our <a href=\"https://github.com/ucdavis/erplab/wiki/installation\">download/installation instructions</a> and then type estudio from the Matlab command line. </p><p class=\"\">If you\u2019re new to ERPLAB, we strongly recommend that you go through our <a href=\"https://github.com/ucdavis/erplab/wiki/ERPLAB-Studio-Tutorial\" target=\"_blank\">tutorial</a> before starting to process your own data. </p><p class=\"\">If you already know how to use the original version of ERPLAB (which we now call ERPLAB Classic), you can quickly learn how to use ERPLAB Studio with our <a href=\"https://ucdavis.box.com/s/i4jfv22gv6rj9t5obctuk6yaruxqomcc\">Transition Guide</a>.</p><p class=\"\">We also have a <a href=\"https://github.com/ucdavis/erplab/wiki/ERPLAB-Studio-Manual\">manual</a> that describes every feature in detail. </p>",
      "author": "Steve Luck",
      "published_date": "2024-06-12T02:02:16+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 444,
      "reading_time": 2,
      "created_at": "2025-11-03T05:22:07.369490+00:00",
      "updated_at": "2025-11-03T05:22:07.369492+00:00"
    },
    {
      "id": "79d603b3db5911be59b9e07e11acc674",
      "url": "https://erpinfo.org/blog/2024/6/28/recording-and-slides-now-available-for-erplab-studio-webinar",
      "title": "Recording and slides now available for ERPLAB Studio webinar",
      "content": "<p class=\"\">We held a webinar to demonstration ERPLAB Studio on 28 June 2024.</p><p class=\"\"><a href=\"https://youtu.be/k-nGv00rTP8\">Click here</a> to access a recording.</p><p class=\"\"><a href=\"https://ucdavis.box.com/s/4fseqz6327dtuouauj12rgvivy1d1nmo\">Click here </a>to access a PDF of the slides.</p>",
      "author": "Steve Luck",
      "published_date": "2024-06-28T22:21:45+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 30,
      "reading_time": 1,
      "created_at": "2025-11-03T05:22:07.369428+00:00",
      "updated_at": "2025-11-03T05:22:07.369429+00:00"
    },
    {
      "id": "f485a145c3b839418e3d039dc3a92ea6",
      "url": "https://erpinfo.org/blog/2025/3/20/new-paper-oddball",
      "title": "New Paper: Does the P3b component reflect working memory updating?",
      "content": "<p class=\"\">Carrasco, C. D., Simmons, A. M., Kiat, J. E., &amp; Luck, S. J. (in press). Enhanced working memory representations for rare events. <em>Psychophysiology</em>. <a href=\"https://doi.org/10.1111/psyp.70038\">https://doi.org/10.1111/psyp.70038</a> [<a href=\"https://doi.org/10.1101/2024.03.20.585952\">preprint</a>]</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n<hr />\n\n\n  <p class=\"\">For decades, many ERP researchers have believed that the P3b wave (sometimes called P300) is a scalp manifestation of a process that updates working memory. This idea originated with Manny Donchin\u2019s <em>context updating</em> hypothesis of the P3b (<a href=\"https://doi.org/10.1111/j.1469-8986.1981.tb01815.x\">Donchin, 1981</a>). Donchin\u2019s idea of <em>context</em> was pretty different from working memory, but as this hypothesis percolated through the field over time, it gradually morphed into the idea that the P3b reflects the updating of working memory.</p><p class=\"\">Rolf Verleger mounted a major attack on the original context updating hypothesis in a classic review article in BBS (<a href=\"https://doi.org/10.1017/S0140525X00058015\">Verleger, 1988</a>), which was followed by a vigorous rebuttal by <a href=\"https://doi.org/10.1017/S0140525X00058027\">Donchin and Coles (1988)</a>. These are interesting papers to read, but they did not settle the issue. In the ensuing years, as the field became more focused on working memory instead of context, I\u2019m aware of no studies that directly tested the hypothesis that the P3b reflects working memory updating. </p><p class=\"\">One reason for the lack of direct evidence is that the oddball paradigms typically used to elicit the P3b do not provide a sensitive assessment of working memory. In a typical paradigm, for example, participants would see a sequence of 90% Xs and 10% Os, and the task would be to press one button for X and another button for O. The responses are made immediately, so it is not necessary to store the stimuli in working memory. Even if participants were asked to make a delayed response, the Xs and Os are so easily discriminable that memory performance would likely be at ceiling.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"698\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/f1d6cee6-5eab-4240-bda0-1a2b7bf4bf88/Figure_1.png?format=1000w\" width=\"720\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 1</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">A few years ago, my lab (especially Carlos Carrasco, Aaron Simmons, and John Kiat) got interested in trying to test the working memory encoding hypothesis. We ran a couple of experiments, but we couldn\u2019t quite figure out the right design. Finally, we figured it out. We used a modified oddball paradigm in which a little dot appeared at one of many locations around an circle (see Figure 1). </p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1112\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/f68d38c2-c87c-4512-b0a7-fbecd75969ff/Figure_2.png?format=1000w\" width=\"1728\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 2</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">On each trial, participants pressed one button if the circle was close to one of the four cardinal axes (left, right, top, and bottom) and a different button if it was close to one of the four diagonals (upper left, upper right, lower left, and lower right). One of these two categories was rare (the <em>oddballs</em>) and the other was frequent (the <em>standards</em>; counterbalanced across trial blocks). As is usual in oddball paradigms, the P3b was much larger for trials in the rare category than for trials in the frequent category (see Figure 2).</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1256\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/89260e9e-be6e-48b1-adc4-58b77a418deb/Figure_3.png?format=1000w\" width=\"1996\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 3</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">The main question was whether the location of the dot was encoded in working memory better for the oddball trials than for the standard trials. To assess this, the experimental design contained occasional <em>probe</em> trials on which participants used the mouse to click on the exact location of the dot (see Figure 3). That is, after participants made the cardinal/diagonal buttonpress responses, they were sometimes then asked to click on the remembered location of the dot. This happened on only 12.5% of trials, selected at random, so that participants would mainly focus on the oddball task. </p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"706\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/166f7236-f77b-431b-8ebf-9633580dcf31/Figure_4.png?format=1000w\" width=\"1800\" />\n\n            \n          \n        \n          \n        \n\n        \n          \n          <figcaption class=\"image-caption-wrapper\">\n            <p class=\"\">Figure 4</p>\n          </figcaption>\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">We looked at the accuracy of these probe responses, calculated as the (absolute value of the) angular distance between the true location and the reported location. As shown in Figure 4, the response error of the probe responses was reduced for the oddball trials relative to the standard trials. In other words, working memory was better for the P3b-eliciting oddball trials than for the standards. Moreover, we found that participants with large P3b amplitudes on oddball trials had smaller response errors on oddball trials (whereas this correlation was not present for standards).</p><p class=\"\">At first glance, these findings seem like support for the idea that the P3b reflects working memory updating. However, the story is not that simple. For example, when we looked at single-trial P3b amplitudes, response errors were not lower for trials with larger P3b amplitudes than for trials with smaller P3b amplitudes.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1122\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/a7a11955-3619-4499-9ef4-61f72a408561/Figure_5.png?format=1000w\" width=\"1378\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">We also used ERP decoding to test whether the exact location of the dot was better stored in working memory on oddball trials than on standard trials (<a href=\"https://erpinfo.org/blog/2023/6/23/decoding-webinar\">click here</a> for information about how ERP decoding works and how you can decode your own data using ERPLAB Toolbox). As shown in Figure 5, we could decode the location of the dot better on oddball trials than on standard trials during the period following the P3b component. Note, however, that this was a pretty small difference that only barely crossed the threshold for statistical significance (p = .048). I would really like to see this effect replicated before fully believing it. However, the behavioral effect was rock solid (and replicated in a follow-up experiment).</p><p class=\"\">What can we conclude from these findings? When we started the project, I knew that it would be difficult to draw any strong causal conclusions about the relationship between the P3b component and working memory updating. That is, even if we saw both a larger P3b and improved working memory on oddball trials, this would just be a correlation and could potentially be explained by a third variable such as attention. But if we saw a big difference in working memory between oddballs and standards, and if we found that working memory was better on trials with larger P3b amplitudes, this would be at least consistent with the idea that the process that produces the P3b on the scalp is also involved in working memory encoding.</p><p class=\"\">However, although we saw an enormous difference in P3b amplitude between oddball trials and standard trials, we saw only small differences in working memory between oddballs and standards, whether measured via behavioral response errors on probe trials or EEG decoding accuracy. If the process that generates the scalp P3b voltage plays a major role in working memory encoding, then we would have expected a much larger working memory difference between oddballs and standards. Moreover, although we found that participants with larger P3b amplitudes had smaller response errors, we did not find any evidence that working memory was any better on trials with larger P3b amplitudes (even though we looked very hard for such a relationship). The bottom line is that, although I was really hoping we would finally provide some direct evidence for the working memory encoding hypothesis, the results of this study have actually convinced me that the P3b is probably not related to working memory encoding.</p><p class=\"\">What, then, explains the small but statistically significant differences in working memory accuracy between oddballs and standards, along with the subjectwise correlation between P3b amplitude and behavioral response errors? A very plausible explanation is that both the P3b component and working memory encoding are facilitated by increased attention. That is, there are several sources of evidence that rare events trigger increased attention, and this could independently produce a larger P3b and improved working memory.</p><p class=\"\">Of course, this is just one experiment, so I wouldn\u2019t say that the working memory encoding hypothesis is completely dead. But given our new findings and the general lack of direct evidence for the hypothesis, it\u2019s on life support.</p><p class=\"\">If the P3b doesn\u2019t reflect working memory encoding, then what does it reflect? This seems like a significant question: the P3b is huge and is observed across a broad range of experimental paradigms, and it\u2019s reasonable to assume that the underlying process must be important for the brain to devote so many watts of energy to it. In fact, I find it embarrassing that our field has not answered this question in the 60 years since <a href=\"https://doi.org/10.1126/science.150.3700.1187\">the P3b was first discovered</a>.</p><p class=\"\">My best bet is that the P3b is related to the process of making decisions about actions (where the term <em>actions</em> is broadly construed to include the withholding of responses and mental actions such as counting). This is related to the fact that the amplitude of the P3b is related to the probability of a task-defined category, not the probability of a physical stimulus category. Rolf Verleger has a nice review of the evidence for this idea (<a href=\"https://doi.org/10.1111/psyp.13542\">Verleger, 2020</a>). But it is still not clear to me why the brain devotes so many watts of energy to creating a large P3b when a rare task-defined category occurs. Verleger notes that several hypotheses about the P3b are compatible with the finding of a larger P3b for oddballs than for standards, but in my view these hypotheses have a hard time explaining the enormous size of the P3b observed for oddballs. This is a longstanding mystery in need of a solution!</p>",
      "author": "Steve Luck",
      "published_date": "2025-03-21T03:42:26+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 1547,
      "reading_time": 7,
      "created_at": "2025-11-03T05:22:07.369403+00:00",
      "updated_at": "2025-11-03T05:22:07.369406+00:00"
    },
    {
      "id": "11da1006ee59369caf1a8b22a800f02c",
      "url": "https://erpinfo.org/blog/2025/8/20/boot-camp-summer-2026",
      "title": "10-Day ERP Boot Camp to be held June 15-24, 2026 in Davis, California",
      "content": "<p class=\"\">We have received another 5 years of funding from the National Institute of Mental Health, so we plan to hold ERP Boot Camps in each of the next 5 summers. The next one will be June 15-24, 2026 in Davis, California. The application portal will open around January 1, 2026.</p><p class=\"\">As in most previous years, all attendees will receive a scholarship that covers most or all travel and lodging expenses. There will be no registration fee.</p><p class=\"\"><a href=\"https://erpinfo.org/summer-boot-camp\">Click here</a> for more information about the ERP Boot Camp.</p><p class=\"\">If you would like to receive announcements about upcoming boot camps, <a href=\"https://erpinfo.org/bootcamp-email-list\">join our email list</a>. If you have any questions after reading this page and the <a href=\"https://erpinfo.org/application-info\">application information</a> page, please email us at <a href=\"mailto:erpbootcamp@gmail.com\">erpbootcamp@gmail.com</a>.</p>",
      "author": "Steve Luck",
      "published_date": "2025-08-20T15:07:14+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 125,
      "reading_time": 1,
      "created_at": "2025-11-03T05:22:07.369212+00:00",
      "updated_at": "2025-11-03T05:22:07.369215+00:00"
    },
    {
      "id": "3a5812554d93388e03949fea3bcca377",
      "url": "https://arxiv.org/abs/2510.27681",
      "title": "Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in Creative Work",
      "content": "arXiv:2510.27681v1 Announce Type: new \nAbstract: As AI becomes more deeply embedded in knowledge work, building assistants that support human creativity and expertise becomes more important. Yet achieving synergy in human-AI collaboration is not easy. Providing AI with detailed information about a user's demographics, psychological attributes, divergent thinking, and domain expertise may improve performance by scaffolding more effective multi-turn interactions. We implemented a personalized LLM-based assistant, informed by users' psychometric profiles and an AI-guided interview about their work style, to help users complete a marketing task for a fictional startup. We randomized 331 participants to work with AI that was either generic (n = 116), partially personalized (n = 114), or fully personalized (n=101). Participants working with personalized AI produce marketing campaigns of significantly higher quality and creativity, beyond what AI alone could have produced. Compared to generic AI, personalized AI leads to higher self-reported levels of assistance and feedback, while also increasing participant trust and confidence. Causal mediation analysis shows that personalization improves performance indirectly by enhancing collective memory, attention, and reasoning in the human-AI interaction. These findings provide a theory-driven framework in which personalization functions as external scaffolding that builds common ground and shared partner models, reducing uncertainty and enhancing joint cognition. This informs the design of future AI assistants that maximize synergy and support human creative potential while limiting negative homogenization.",
      "author": "Sean Kelley, David De Cremer, Christoph Riedl",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 223,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943355+00:00",
      "updated_at": "2025-11-03T05:21:54.943356+00:00"
    },
    {
      "id": "bf57183751616e57c24930217b234b27",
      "url": "https://arxiv.org/abs/2510.27572",
      "title": "Beyond Visualization: Building Decision Intelligence Through Iterative Dashboard Refinement",
      "content": "arXiv:2510.27572v1 Announce Type: new \nAbstract: Effective business intelligence (BI) dashboards evolve through iterative refinement rather than single-pass design. Addressing the lack of structured improvement frameworks in BI practice, this study documents the four-stage evolution of a Power BI dashboard analyzing profitability decline in a fictional retail firm, Global Superstore. Using a dataset of \\$12.64 million in sales across seven markets and three product categories, the project demonstrates how feedback-driven iteration and gap analysis convert exploratory visuals into decision-support tools. Guided by four executive questions on profitability, market prioritization, discount effects, and shipping costs, each iteration resolved analytical or interpretive shortcomings identified through collaborative review. Key findings include margin erosion in furniture (6.94% vs. 13.99% for technology), a 20% discount threshold beyond which profitability declined, and \\$1.35 million in unrecovered shipping costs. Contributions include: (a) a replicable feedback-driven methodology grounded in iterative gap analysis; (b) DAX-based technical enhancements improving interpretive clarity; (c) an inductively derived six-element narrative framework; and (d) evidence that narrative coherence emerges organically through structured refinement. The methodology suggests transferable value for both BI practitioners and educators, pending validation across diverse organizational contexts.",
      "author": "Likitha Tadakala, Muskan Saraf, Sajjad Rezvani Boroujeni, Hossein Abedi, Tom Bush",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 185,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943322+00:00",
      "updated_at": "2025-11-03T05:21:54.943323+00:00"
    },
    {
      "id": "6c826fb524d78ff73a0eee259bf58eb5",
      "url": "https://arxiv.org/abs/2510.27521",
      "title": "Independent Clinical Evaluation of General-Purpose LLM Responses to Signals of Suicide Risk",
      "content": "arXiv:2510.27521v1 Announce Type: new \nAbstract: We introduce findings and methods to facilitate evidence-based discussion about how large language models (LLMs) should behave in response to user signals of risk of suicidal thoughts and behaviors (STB). People are already using LLMs as mental health resources, and several recent incidents implicate LLMs in mental health crises. Despite growing attention, few studies have been able to effectively generalize clinical guidelines to LLM use cases, and fewer still have proposed methodologies that can be iteratively applied as knowledge improves about the elements of human-AI interaction most in need of study. We introduce an assessment of LLM alignment with guidelines for ethical communication, adapted from clinical principles and applied to expressions of risk factors for STB in multi-turn conversations. Using a codebook created and validated by clinicians, mobilizing the volunteer participation of practicing therapists and trainees (N=43) based in the U.S., and using generalized linear mixed-effects models for statistical analysis, we assess a single fully open-source LLM, OLMo-2-32b. We show how to assess when a model deviates from clinically informed guidelines in a way that may pose a hazard and (thanks to its open nature) facilitates future investigation as to why. We find that contrary to clinical best practice, OLMo-2-32b, and, possibly by extension, other LLMs, will become less likely to invite continued dialog as users send more signals of STB risk in multi-turn settings. We also show that OLMo-2-32b responds differently depending on the risk factor expressed. This empirical evidence highlights that just as chatbots pose hazards if their responses reinforce delusions or assist in suicidal acts, they may also discourage further help-seeking or cause feelings of dismissal or abandonment by withdrawing from conversations when STB risk is expressed.",
      "author": "Nick Judd, Alexandre Vaz, Kevin Paeth, Layla In\\'es Davis, Milena Esherick, Jason Brand, In\\^es Amaro, Tony Rousmaniere",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 285,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943291+00:00",
      "updated_at": "2025-11-03T05:21:54.943292+00:00"
    },
    {
      "id": "f8ffd8ec636d5ee8fcbd7ba234c908ae",
      "url": "https://arxiv.org/abs/2510.27401",
      "title": "\"Koyi Sawaal Nahi Hai\": Reimagining Maternal Health Chatbots for Collective, Culturally Grounded Care",
      "content": "arXiv:2510.27401v1 Announce Type: new \nAbstract: In recent years, LLM-based maternal health chatbots have been widely deployed in low-resource settings, but they often ignore real-world contexts where women may not own phones, have limited literacy, and share decision-making within families. Through the deployment of a WhatsApp-based maternal health chatbot with 48 pregnant women in Lahore, Pakistan, we examine barriers to use in populations where phones are shared, decision-making is collective, and literacy varies. We complement this with focus group discussions with obstetric clinicians. Our findings reveal how adoption is shaped by proxy consent and family mediation, intermittent phone access, silence around asking questions, infrastructural breakdowns, and contested authority. We frame barriers to non-use as culturally conditioned rather than individual choices, and introduce the Relational Chatbot Design Grammar (RCDG): four commitments that enable mediated decision-making, recognize silence as engagement, support episodic use, and treat fragility as baseline to reorient maternal health chatbots toward culturally grounded, collective care.",
      "author": "Imaan Hameed, Huma Umar, Fozia Umber, Maryam Mustafa",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943252+00:00",
      "updated_at": "2025-11-03T05:21:54.943254+00:00"
    },
    {
      "id": "ab08390195f5d2def384a2bc5cab3a76",
      "url": "https://arxiv.org/abs/2510.27272",
      "title": "Inferring trust in recommendation systems from brain, behavioural, and physiological data",
      "content": "arXiv:2510.27272v1 Announce Type: new \nAbstract: As people nowadays increasingly rely on artificial intelligence (AI) to curate information and make decisions, assigning the appropriate amount of trust in automated intelligent systems has become ever more important. However, current measurements of trust in automation still largely rely on self-reports that are subjective and disruptive to the user. Here, we take music recommendation as a model to investigate the neural and cognitive processes underlying trust in automation. We observed that system accuracy was directly related to users' trust and modulated the influence of recommendation cues on music preference. Modelling users' reward encoding process with a reinforcement learning model further revealed that system accuracy, expected reward, and prediction error were related to oscillatory neural activity recorded via EEG and changes in pupil diameter. Our results provide a neurally grounded account of calibrating trust in automation and highlight the promises of a multimodal approach towards developing trustable AI systems.",
      "author": "Vincent K. M. Cheung, Pei-Cheng Shih, Masato Hirano, Masataka Goto, Shinichi Furuya",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943224+00:00",
      "updated_at": "2025-11-03T05:21:54.943225+00:00"
    },
    {
      "id": "132617f2b94f5ea5b582533813eac545",
      "url": "https://arxiv.org/abs/2510.27247",
      "title": "Reconstructing Unseen Sentences from Speech-related Biosignals for Open-vocabulary Neural Communication",
      "content": "arXiv:2510.27247v1 Announce Type: new \nAbstract: Brain-to-speech (BTS) systems represent a groundbreaking approach to human communication by enabling the direct transformation of neural activity into linguistic expressions. While recent non-invasive BTS studies have largely focused on decoding predefined words or sentences, achieving open-vocabulary neural communication comparable to natural human interaction requires decoding unconstrained speech. Additionally, effectively integrating diverse signals derived from speech is crucial for developing personalized and adaptive neural communication and rehabilitation solutions for patients. This study investigates the potential of speech synthesis for previously unseen sentences across various speech modes by leveraging phoneme-level information extracted from high-density electroencephalography (EEG) signals, both independently and in conjunction with electromyography (EMG) signals. Furthermore, we examine the properties affecting phoneme decoding accuracy during sentence reconstruction and offer neurophysiological insights to further enhance EEG decoding for more effective neural communication solutions. Our findings underscore the feasibility of biosignal-based sentence-level speech synthesis for reconstructing unseen sentences, highlighting a significant step toward developing open-vocabulary neural communication systems adapted to diverse patient needs and conditions. Additionally, this study provides meaningful insights into the development of communication and rehabilitation solutions utilizing EEG-based decoding technologies.",
      "author": "Deok-Seon Kim, Seo-Hyun Lee, Kang Yin, Seong-Whan Lee",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 186,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943194+00:00",
      "updated_at": "2025-11-03T05:21:54.943196+00:00"
    },
    {
      "id": "28da4e96a6cfa2767843c8adecf8847c",
      "url": "https://arxiv.org/abs/2510.27126",
      "title": "AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys",
      "content": "arXiv:2510.27126v1 Announce Type: new \nAbstract: Conventional online surveys provide limited personalization, often resulting in low engagement and superficial responses. Although AI survey chatbots improve convenience, most are still reactive: they rely on fixed dialogue trees or static prompt templates and therefore cannot adapt within a session to fit individual users, which leads to generic follow-ups and weak response quality. We address these limitations with AURA (Adaptive Understanding through Reinforcement Learning for Assessment), a reinforcement learning framework for AI-driven adaptive conversational surveys. AURA quantifies response quality using a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and Specificity) and selects follow-up question types via an epsilon-greedy policy that updates the expected quality gain within each session. Initialized with priors extracted from 96 prior campus-climate conversations (467 total chatbot-user exchanges), the system balances exploration and exploitation across 10-15 dialogue exchanges, dynamically adapting to individual participants in real time. In controlled evaluations, AURA achieved a +0.12 mean gain in response quality and a statistically significant improvement over non-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in specification prompts and a 10x increase in validation behavior. These results demonstrate that reinforcement learning can give survey chatbots improved adaptivity, transforming static questionnaires into interactive, self-improving assessment systems.",
      "author": "Jinwen Tang, Yi Shang",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 201,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943163+00:00",
      "updated_at": "2025-11-03T05:21:54.943164+00:00"
    },
    {
      "id": "7f0c72e3347a62e3075d23e89fa41f13",
      "url": "https://arxiv.org/abs/2510.27075",
      "title": "Functional connectivity guided deep neural network for decoding high-level visual imagery",
      "content": "arXiv:2510.27075v1 Announce Type: new \nAbstract: This study introduces a pioneering approach in brain-computer interface (BCI) technology, featuring our novel concept of high-level visual imagery for non-invasive electroencephalography (EEG)-based communication. High-level visual imagery, as proposed in our work, involves the user engaging in the mental visualization of complex upper limb movements. This innovative approach significantly enhances the BCI system, facilitating the extension of its applications to more sophisticated tasks such as EEG-based robotic arm control. By leveraging this advanced form of visual imagery, our study opens new horizons for intricate and intuitive mind-controlled interfaces. We developed an advanced deep learning architecture that integrates functional connectivity metrics with a convolutional neural network-image transformer. This framework is adept at decoding subtle user intentions, addressing the spatial variability in high-level visual tasks, and effectively translating these into precise commands for robotic arm control. Our comprehensive offline and pseudo-online evaluations demonstrate the framework's efficacy in real-time applications, including the nuanced control of robotic arms. The robustness of our approach is further validated through leave-one-subject-out cross-validation, marking a significant step towards versatile, subject-independent BCI applications. This research highlights the transformative impact of advanced visual imagery and deep learning in enhancing the usability and adaptability of BCI systems, particularly in robotic arm manipulation.",
      "author": "Byoung-Hee Kwon, Minji Lee, Seong-Whan Lee",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 206,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943130+00:00",
      "updated_at": "2025-11-03T05:21:54.943132+00:00"
    },
    {
      "id": "5f169c0079f6947abc058a09d38680da",
      "url": "https://arxiv.org/abs/2510.27058",
      "title": "Adaptive Human-Computer Interaction Strategies Through Reinforcement Learning in Complex",
      "content": "arXiv:2510.27058v1 Announce Type: new \nAbstract: This study addresses the challenges of dynamics and complexity in intelligent human-computer interaction and proposes a reinforcement learning-based optimization framework to improve long-term returns and overall experience. Human-computer interaction is modeled as a Markov decision process, with state space, action space, reward function, and discount factor defined to capture the dynamics of user input, system feedback, and interaction environment. The method combines policy function, value function, and advantage function, updates parameters through policy gradient, and continuously adjusts during interaction to balance immediate feedback and long-term benefits. To validate the framework, multimodal dialog and scene-aware datasets are used as the experimental platform, with multiple sensitivity experiments conducted on key factors such as discount factor, exploration rate decay, environmental noise, and data imbalance. Evaluation is carried out using cumulative reward, average episode reward, convergence speed, and task success rate. Results show that the proposed method outperforms existing approaches across several metrics, achieving higher task completion while maintaining strategy stability. Comparative experiments further confirm its advantages in interaction efficiency and long-term return, demonstrating the significant value of reinforcement learning in optimizing human-computer interaction.",
      "author": "Rui Liu, Yifan Zhuang, Runsheng Zhang",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 185,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943091+00:00",
      "updated_at": "2025-11-03T05:21:54.943093+00:00"
    },
    {
      "id": "f95c15971d8e7eb1742856d5b8cf3ca1",
      "url": "https://arxiv.org/abs/2510.26999",
      "title": "AIOT based Smart Education System: A Dual Layer Authentication and Context-Aware Tutoring Framework for Learning Environments",
      "content": "arXiv:2510.26999v1 Announce Type: new \nAbstract: The AIoT-Based Smart Education System integrates Artificial Intelligence and IoT to address persistent challenges in contemporary classrooms: attendance fraud, lack of personalization, student disengagement, and inefficient resource use. The unified platform combines four core modules: (1) a dual-factor authentication system leveraging RFID-based ID scans and WiFi verification for secure, fraud-resistant attendance; (2) an AI-powered assistant that provides real-time, context-aware support and dynamic quiz generation based on instructor-supplied materials; (3) automated test generators to streamline adaptive assessment and reduce administrative overhead; and (4) the EcoSmart Campus module, which autonomously regulates classroom lighting, air quality, and temperature using IoT sensors and actuators. Simulated evaluations demonstrate the system's effectiveness in delivering robust real-time monitoring, fostering inclusive engagement, preventing fraudulent practices, and supporting operational scalability. Collectively, the AIoT-Based Smart Education System offers a secure, adaptive, and efficient learning environment, providing a scalable blueprint for future educational innovation and improved student outcomes through the synergistic application of artificial intelligence and IoT technologies.",
      "author": "Adithya Neelakantan, Pratik Satpute, Prerna Shinde, Tejas Manjunatha Devang",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 162,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:54.943052+00:00",
      "updated_at": "2025-11-03T05:21:54.943056+00:00"
    },
    {
      "id": "00f155be5f18ff0de0c92c12a984202d",
      "url": "https://arxiv.org/abs/2510.25998",
      "title": "Integrated Information Theory: A Consciousness-First Approach to What Exists",
      "content": "arXiv:2510.25998v2 Announce Type: replace \nAbstract: This overview of integrated information theory (IIT) emphasizes IIT's \"consciousness-first\" approach to what exists. Consciousness demonstrates to each of us that something exists--experience--and reveals its essential properties--the axioms of phenomenal existence. IIT formulates these properties operationally, yielding the postulates of physical existence. To exist intrinsically or absolutely, an entity must have cause-effect power upon itself, in a specific, unitary, definite and structured manner. IIT's explanatory identity claims that an entity's cause-effect structure accounts for all properties of an experience--essential and accidental--with no additional ingredients. These include the feeling of spatial extendedness, temporal flow, of objects binding general concepts with particular configurations of features, and of qualia such as colors and sounds. IIT's intrinsic ontology has implications for understanding meaning, perception, and free will, for assessing consciousness in patients, infants, other species, and artifacts, and for reassessing our place in nature.",
      "author": "Giulio Tononi, Melanie Boly",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 145,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:53.759585+00:00",
      "updated_at": "2025-11-03T05:21:53.759587+00:00"
    },
    {
      "id": "91736a226890ae83546a878670eacf04",
      "url": "https://arxiv.org/abs/2501.04139",
      "title": "Anomalous contrast as an adaptive violation of the Talbot-Plateau law",
      "content": "arXiv:2501.04139v2 Announce Type: replace \nAbstract: Purpose: To better understand anomalous contrast mechanisms that allow flicker-fused stimuli to be visible even when they provide the same average luminance as background. Method: Stimulus flicker was used to elicit differential activation of ON and OFF retinal channels at frequencies above the flicker-fusion threshold. Providing balanced light energy to ON and OFF channels will normally cause the stimulus to vanish into the background. Results: We used ultra-brief bright pulses, combined with ultra-long dark pulses, to elicit \"anomalous contrast\" that rendered the stimulus visible, even though it had the same average luminance as the background. The duration and intensity of flicker components were varied to gain insight into the conditions that would elicit this effect. Conclusions: Anomalous contrast displays violated the Talbot-Plateau law, but in doing so, provided an adaptive way to register and signal contours that matched background luminance. These findings contribute additional details about this visual adaptation, and we discuss how the retinal circuitry provides for stimulus visibility.",
      "author": "Ernest Greene, Jack Morrison",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 165,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:53.759557+00:00",
      "updated_at": "2025-11-03T05:21:53.759559+00:00"
    },
    {
      "id": "2fd3e0ad4e0cfdb509b43f78c4273bfe",
      "url": "https://arxiv.org/abs/2510.27366",
      "title": "A Sensing Whole Brain Zebrafish Foundation Model for Neuron Dynamics and Behavior",
      "content": "arXiv:2510.27366v1 Announce Type: new \nAbstract: Neural dynamics underlie behaviors from memory to sleep, yet identifying mechanisms for higher-order phenomena (e.g., social interaction) is experimentally challenging. Existing whole-brain models often fail to scale to single-neuron resolution, omit behavioral readouts, or rely on PCA/conv pipelines that miss long-range, non-linear interactions. We introduce a sparse-attention whole-brain foundation model (SBM) for larval zebrafish that forecasts neuron spike probabilities conditioned on sensory stimuli and links brain state to behavior. SBM factorizes attention across neurons and along time, enabling whole-brain scale and interpretability. On a held-out subject, it achieves mean absolute error <0.02 with calibrated predictions and stable autoregressive rollouts. Coupled to a permutation-invariant behavior head, SBM enables gradient-based synthesis of neural patterns that elicit target behaviors. This framework supports rapid, behavior-grounded exploration of complex neural phenomena.",
      "author": "Sam Fatehmanesh Vegas, Matt Thomson, James Gornet, David Prober",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 131,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:53.759527+00:00",
      "updated_at": "2025-11-03T05:21:53.759529+00:00"
    },
    {
      "id": "d33e39dd01c669199a7c3c9889b17153",
      "url": "https://arxiv.org/abs/2510.26955",
      "title": "Neurons as Detectors of Coherent Sets in Sensory Dynamics",
      "content": "arXiv:2510.26955v1 Announce Type: new \nAbstract: We model sensory streams as observations from high-dimensional stochastic dynamical systems and conceptualize sensory neurons as self-supervised learners of compact representations of such dynamics. From prior experience, neurons learn coherent sets-regions of stimulus state space whose trajectories evolve cohesively over finite times-and assign membership indices to new stimuli. Coherent sets are identified via spectral clustering of the stochastic Koopman operator (SKO), where the sign pattern of a subdominant singular function partitions the state space into minimally coupled regions. For multivariate Ornstein-Uhlenbeck processes, this singular function reduces to a linear projection onto the dominant singular vector of the whitened state-transition matrix. Encoding this singular vector as a receptive field enables neurons to compute membership indices via the projection sign in a biologically plausible manner. Each neuron detects either a predictive coherent set (stimuli with common futures) or a retrospective coherent set (stimuli with common pasts), suggesting a functional dichotomy among neurons. Since neurons lack access to explicit dynamical equations, the requisite singular vectors must be estimated directly from data, for example, via past-future canonical correlation analysis on lag-vector representations-an approach that naturally extends to nonlinear dynamics. This framework provides a novel account of neuronal temporal filtering, the ubiquity of rectification in neural responses, and known functional dichotomies. Coherent-set clustering thus emerges as a fundamental computation underlying sensory processing and transferable to bio-inspired artificial systems.",
      "author": "Joshua L. Pughe-Sanford, Xuehao Ding, Jason J. Moore, Anirvan M. Sengupta, Charles Epstein, Philip Greengard, Dmitri B. Chklovskii",
      "published_date": "2025-11-03T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 228,
      "reading_time": 1,
      "created_at": "2025-11-03T05:21:53.759497+00:00",
      "updated_at": "2025-11-03T05:21:53.759499+00:00"
    }
  ]
}