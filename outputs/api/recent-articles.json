{
  "last_updated": "2026-01-18T16:27:36.196868+00:00",
  "count": 20,
  "articles": [
    {
      "id": "e6aec394cb4250dfb0de3856bb9c0993",
      "url": "https://www.reddit.com/r/Python/comments/1qft1ma/sunday_daily_thread_whats_everyone_working_on/",
      "title": "Sunday Daily Thread: What's everyone working on this week?",
      "content": "<!-- SC_OFF --><div class=\"md\"><h1>Weekly Thread: What's Everyone Working On This Week? \ud83d\udee0\ufe0f</h1> <p>Hello <a href=\"/r/Python\">/r/Python</a>! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!</p> <h2>How it Works:</h2> <ol> <li><strong>Show &amp; Tell</strong>: Share your current projects, completed works, or future ideas.</li> <li><strong>Discuss</strong>: Get feedback, find collaborators, or just chat about your project.</li> <li><strong>Inspire</strong>: Your project might inspire someone else, just as you might get inspired here.</li> </ol> <h2>Guidelines:</h2> <ul> <li>Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.</li> <li>Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.</li> </ul> <h2>Example Shares:</h2> <ol> <li><strong>Machine Learning Model</strong>: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!</li> <li><strong>Web Scraping</strong>: Built a script to scrape and analyze news articles. It's helped me understand media bias better.</li> <li><strong>Automation</strong>: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!</li> </ol> <p>Let's build and grow together! Share your journey and learn from others. Happy coding! \ud83c\udf1f</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qft1ma/sunday_daily_thread_whats_everyone_working_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qft1ma/sunday_daily_thread_whats_everyone_working_on/\">[comments]</a></span>",
      "author": "/u/AutoModerator",
      "published_date": "2026-01-18T00:00:31+00:00",
      "source": "Reddit Python",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 211,
      "reading_time": 1,
      "created_at": "2026-01-18T16:26:24.727882+00:00",
      "updated_at": "2026-01-18T16:26:24.727884+00:00"
    },
    {
      "id": "694b5bf81bc64f0db720e7ba8a1aca12",
      "url": "https://www.theguardian.com/us-news/ng-interactive/2026/jan/18/tech-ai-bubble-burst-reverse-centaur",
      "title": "AI companies will fail. We can salvage something from the wreckage",
      "content": "<p>Article URL: <a href=\"https://www.theguardian.com/us-news/ng-interactive/2026/jan/18/tech-ai-bubble-burst-reverse-centaur\">https://www.theguardian.com/us-news/ng-interactive/2026/jan/18/tech-ai-bubble-burst-reverse-centaur</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46668170\">https://news.ycombinator.com/item?id=46668170</a></p>\n<p>Points: 28</p>\n<p># Comments: 8</p>",
      "author": "kawera",
      "published_date": "2026-01-18T14:45:37+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-18T16:26:22.168068+00:00",
      "updated_at": "2026-01-18T16:26:22.168070+00:00"
    },
    {
      "id": "ded644dbb0f23200b64a0a31311307fa",
      "url": "https://www.wsj.com/tech/ai/why-the-tech-world-thinks-the-american-dream-is-dying-daf793dc",
      "title": "Why the Tech World Thinks the American Dream Is Dying",
      "content": "<p>Article URL: <a href=\"https://www.wsj.com/tech/ai/why-the-tech-world-thinks-the-american-dream-is-dying-daf793dc\">https://www.wsj.com/tech/ai/why-the-tech-world-thinks-the-american-dream-is-dying-daf793dc</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46668394\">https://news.ycombinator.com/item?id=46668394</a></p>\n<p>Points: 7</p>\n<p># Comments: 13</p>",
      "author": "Brajeshwar",
      "published_date": "2026-01-18T15:09:37+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-18T16:26:22.168047+00:00",
      "updated_at": "2026-01-18T16:26:22.168049+00:00"
    },
    {
      "id": "88cac2b3d2d4a9028cff77ea981214e0",
      "url": "https://www.reddit.com/r/Python/comments/1qgai08/robyn_finally_supports_python_314/",
      "title": "Robyn (finally) supports Python 3.14 \ud83c\udf89",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>For the unaware - <a href=\"https://github.com/sparckles/Robyn\">Robyn</a> is a fast, async Python web framework built on a Rust runtime.</p> <p>Python 3.14 support has been pending for a while.</p> <p>Wanted to share it with folks outside the Robyn community.</p> <p>You can check out the release at - <a href=\"https://github.com/sparckles/Robyn/releases/tag/v0.74.0\">https://github.com/sparckles/Robyn/releases/tag/v0.74.0</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/stealthanthrax\"> /u/stealthanthrax </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qgai08/robyn_finally_supports_python_314/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qgai08/robyn_finally_supports_python_314/\">[comments]</a></span>",
      "author": "/u/stealthanthrax",
      "published_date": "2026-01-18T14:50:34+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 68,
      "reading_time": 1,
      "created_at": "2026-01-18T15:41:43.913791+00:00",
      "updated_at": "2026-01-18T16:18:48.319705+00:00",
      "metadata": {
        "processed_at": "2026-01-18T16:18:48.319714+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "61d7c232aa117b31d483616663ddbc77",
      "url": "https://www.reddit.com/r/Python/comments/1qgaac2/i_built_event2vector_a_scikitstyle_library_for/",
      "title": "I built event2vector, a scikit\u2011style library for event sequence embeddings in Python)",
      "content": "<!-- SC_OFF --><div class=\"md\"><h1>What event2vec Project Does</h1> <p>I\u2019ve been working on my Python library, Event2Vector (event2vec), for embedding event sequences (logs, clickstreams, POS tags, life\u2011event sequences, etc.) into vectors in a way that is easy to inspect and reason about.</p> <p>Instead of a complex RNN/transformer, the model uses a simple additive recurrent update: the hidden state for a sequence is constrained to behave like the sum of its event embeddings (the \u201clinear additive hypothesis\u201d). This makes sequence trajectories geometrically interpretable and supports vector arithmetic on histories (e.g., A \u2212 B + C style analogies on event trajectories).</p> <p>From the Python side, you primarily interact with a scikit\u2011learn\u2011style estimator:</p> <pre><code>python from event2vector import Event2Vec model = Event2Vec( num_event_types=len(vocab), geometry=&quot;euclidean&quot;, # or &quot;hyperbolic&quot; embedding_dim=128, pad_sequences=True, num_epochs=50, ) model.fit(train_sequences, verbose=True) embeddings = model.transform(train_sequences) </code></pre> <p>There are both Euclidean and hyperbolic (Poincar\u00e9 ball) variants, so you can choose flat vs hierarchical geometry for your event space.</p> <h1>Target Audience</h1> <p>Python users working with discrete event sequences: logs, clickstreams, POS tags, user journeys, synthetic processes, etc.</p> <p>E.g. posts about shopping patterns <a href=\"https://substack.com/home/post/p-181632020?source=queue\">https://substack.com/home/post/p-181632020?source=queue</a> or geometry of languages <a href=\"https://sulcantonin.substack.com/p/the-geometry-of-language-families\">https://sulcantonin.substack.com/p/the-geometry-of-language-families</a></p> <p>People who want interpretable, geometric representations of sequences rather than just \u201cit works but I can\u2019t see what it\u2019s doing.\u201d</p> <p>It is currently more of a research/analysis tool and prototyping library than a fully battle\u2011hardened production system, but:</p> <p>It is MIT\u2011licensed and on PyPI (<code>pip install event2vector</code>).</p> <p>It has a scikit\u2011style API (<code>fit</code>, <code>fit_transform</code>, <code>transform</code>, <code>most_similar</code>) and optional padded batching + GPU support, so it should drop into many Python ML workflows.</p> <h1>Comparison</h1> <p><strong>Versus Word2Vec and similar context\u2011window models:</strong></p> <p>Word2Vec is excellent for capturing local co\u2011occurrence and semantic similarity, but it does not model the ordered trajectory of a sequence; contexts are effectively treated as bags of neighbors.</p> <p>Event2Vector, in contrast, explicitly treats the hidden state as an ordered sum of event embeddings, and its training objective enforces that likely future events lie along the trajectory of that sum. This lets it capture sequential structure and trajectory geometry that Word2Vec is not designed to represent.</p> <p>In the paper, an unsupervised experiment on the Brown Corpus shows that Event2Vector\u2019s additive sequence embeddings produce clearer clusters of POS\u2011tag patterns than a Word2Vec baseline when you compose tag sequences and visualize them.</p> <p><strong>Versus generic RNNs / LSTMs / transformers:</strong></p> <p>Those models are more expressive and often better for pure prediction, but their hidden states are usually hard to interpret geometrically.</p> <p>Event2Vector intentionally trades some expressivity for a simple, reversible additive structure: sequences are trajectories in a space where addition/subtraction have a clear meaning, and you can inspect them with PCA/t\u2011SNE or do analogical reasoning.</p> <h1>Python\u2011centric details</h1> <ul> <li>Install: <code>pip install event2vector</code> </li> <li>Github Repo: <a href=\"https://github.com/sulcantonin/event2vec_public/tree/main\">https://github.com/sulcantonin/event2vec_public/tree/main</a></li> </ul> <p>Accepts integer\u2011encoded sequences (Python lists / tensors), with optional padding for minibatching.</p> <p>Provides a tiny synthetic quickstart (START\u2192A/B\u2192C\u2192END) that trains in seconds on CPU and plots embeddings with matplotlib, plus a Brown Corpus POS example that mirrors the paper.</p> <p>I\u2019d love feedback from the Python side on:</p> <p>Whether the estimator/API design feels natural.</p> <p>What examples or utilities you\u2019d want for real\u2011world logs / clickstreams.</p> <p>Any obvious packaging or ergonomics improvements that would make you more likely to try it in your own projects.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sulcantonin\"> /u/sulcantonin </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qgaac2/i_built_event2vector_a_scikitstyle_library_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qgaac2/i_built_event2vector_a_scikitstyle_library_for/\">[comments]</a></span>",
      "author": "/u/sulcantonin",
      "published_date": "2026-01-18T14:41:43+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 548,
      "reading_time": 2,
      "created_at": "2026-01-18T15:41:43.913766+00:00",
      "updated_at": "2026-01-18T16:18:48.319719+00:00",
      "metadata": {
        "processed_at": "2026-01-18T16:18:48.319721+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "495b4b9dfe70fd838cc961f0beae6f82",
      "url": "https://www.reddit.com/r/Python/comments/1qflvmi/i_built_timetracer_recordreplay_api_calls_locally/",
      "title": "I built TimeTracer, record/replay API calls locally + dashboard",
      "content": "<!-- SC_OFF --><div class=\"md\"><p><strong>What My Project Does</strong><br /> TimeTracer helps record API requests into JSON \u201ccassettes\u201d (timings + inputs/outputs) and replay them locally with dependencies mocked (or hybrid replay). It also includes a dashboard + timeline view to inspect requests, failures, and slow calls, and supports capturing httpx, requests, SQLAlchemy, and Redis.</p> <p><strong>Target Audience</strong><br /> Python developers building FastAPI/Flask services who want a simpler way to reproduce staging/production issues locally, debug faster, and create repeatable test scenarios from real requests.</p> <p><strong>Comparison</strong><br /> There are existing tools that record/replay HTTP calls (like VCR-style approaches), and other tools focused on tracing/observability. TimeTracer is my attempt to combine record/replay with API debugging workflows and a simple dashboard/timeline, especially for services that talk to external APIs, databases, and Redis.</p> <p><strong>Install</strong><br /> pip install timetracer</p> <p><strong>GitHub</strong><br /> <a href=\"https://github.com/usv240/timetracer\">https://github.com/usv240/timetracer</a></p> <p>Contributions welcome, if anyone\u2019s interested in helping (features, tests, docs, or new integrations), I\u2019d love the support.</p> <p><strong>Looking for feedback:</strong> what would make you actually use something like this, pytest integration, better diffing, or more framework support?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/usv240\"> /u/usv240 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qflvmi/i_built_timetracer_recordreplay_api_calls_locally/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qflvmi/i_built_timetracer_recordreplay_api_calls_locally/\">[comments]</a></span>",
      "author": "/u/usv240",
      "published_date": "2026-01-17T19:07:41+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2026-01-18T15:41:43.913682+00:00",
      "updated_at": "2026-01-18T16:18:48.319724+00:00",
      "metadata": {
        "processed_at": "2026-01-18T16:18:48.319725+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "0517fe9001b218ca08ac2ddab2185de4",
      "url": "https://www.reddit.com/r/Python/comments/1qfgocp/which_is_better_for_desktop_applications_flat_or/",
      "title": "Which is better for desktop applications, Flat or QT?",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>I was studying how to make Python applications without using JS. Then I discovered Flet, a Python framework that compiles to Flutter when creating screens. But I saw that it also makes desktop applications. So here's the question: which is better for making desktop applications with Python, Flet or Qt?</p> <p>If there are other technologies, please mention them; I'm a beginner in Python and I'm exploring this world.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DynamicBR\"> /u/DynamicBR </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qfgocp/which_is_better_for_desktop_applications_flat_or/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qfgocp/which_is_better_for_desktop_applications_flat_or/\">[comments]</a></span>",
      "author": "/u/DynamicBR",
      "published_date": "2026-01-17T15:51:49+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 89,
      "reading_time": 1,
      "created_at": "2026-01-18T15:41:43.913641+00:00",
      "updated_at": "2026-01-18T16:18:48.319728+00:00",
      "metadata": {
        "processed_at": "2026-01-18T16:18:48.319729+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "a831d2be8356ed81d4542f7a83fead9e",
      "url": "https://www.reddit.com/r/Python/comments/1qfmaq0/pypecdp_a_fully_async_python_driver_for_chrome/",
      "title": "pypecdp - a fully async python driver for chrome using pipes",
      "content": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone. I built a fully asynchronous chrome driver in Python using POSIX pipes. Instead of websockets, it uses file descriptors to connect to the browser.</p> <p><strong>What My Project Does</strong></p> <ul> <li>Directly connects and controls the browser, no middleware</li> <li>100% asynchronous, nothing gets blocked</li> <li>Built completely using built-in Python asyncio <ul> <li>Except one <code>deprecated</code> dependency for python-cdp modules</li> </ul></li> <li>Best for running multiple browsers on same machine</li> <li>No risk of zombie chromes if code crashes</li> <li>Easy customization via class inheritance</li> <li>No automation signatures as there is no framework in between</li> </ul> <p><strong>Target Audience</strong></p> <p>Webscrappers, people interested in browser based automation.</p> <p><strong>Comparison</strong></p> <p>Several Python based browser automation tools exist but very few are fully asynchronous and none is POSIX pipe based.</p> <p><strong>Limitations</strong></p> <p>Currently limited to POSIX based systems only (Linux/Mac).</p> <p>Bug reports, feature requests and contributions are welcome!</p> <p><a href=\"https://github.com/sohaib17/pypecdp\">https://github.com/sohaib17/pypecdp</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sohaib0717\"> /u/sohaib0717 </a> <br /> <span><a href=\"https://www.reddit.com/r/Python/comments/1qfmaq0/pypecdp_a_fully_async_python_driver_for_chrome/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/Python/comments/1qfmaq0/pypecdp_a_fully_async_python_driver_for_chrome/\">[comments]</a></span>",
      "author": "/u/sohaib0717",
      "published_date": "2026-01-17T19:24:04+00:00",
      "source": "Reddit Python",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 160,
      "reading_time": 1,
      "created_at": "2026-01-18T15:41:43.913610+00:00",
      "updated_at": "2026-01-18T16:18:48.319732+00:00",
      "metadata": {
        "processed_at": "2026-01-18T16:18:48.319733+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "36c053d6c5b8f254cdb73cb1206e714f",
      "url": "https://www.wsj.com/tech/ai/anthropic-claude-code-ai-7a46460e",
      "title": "Claude Is Taking the AI World by Storm, and Even Non-Nerds Are Blown Away",
      "content": "<p>Article URL: <a href=\"https://www.wsj.com/tech/ai/anthropic-claude-code-ai-7a46460e\">https://www.wsj.com/tech/ai/anthropic-claude-code-ai-7a46460e</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46668424\">https://news.ycombinator.com/item?id=46668424</a></p>\n<p>Points: 7</p>\n<p># Comments: 1</p>",
      "author": "pretext",
      "published_date": "2026-01-18T15:12:51+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-18T15:41:41.473453+00:00",
      "updated_at": "2026-01-18T15:41:41.473460+00:00"
    },
    {
      "id": "4abc4f6e46c3a5198ba4d8c761136acb",
      "url": "https://www.empa.ch/web/s604/flamm-hemmendes-epoxidharz-nachhaltiger-machen",
      "title": "More sustainable epoxy thanks to phosphorus",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46614487\">Comments</a>",
      "author": "",
      "published_date": "2026-01-14T10:43:31+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-18T15:20:42.381282+00:00",
      "updated_at": "2026-01-18T15:20:42.381283+00:00"
    },
    {
      "id": "e2ac3cc13f48fbcbd28259da9abe169a",
      "url": "https://sftw.substack.com/p/310-to-yuma",
      "title": "The Harvesting of Lettuce",
      "content": "<p>Article URL: <a href=\"https://sftw.substack.com/p/310-to-yuma\">https://sftw.substack.com/p/310-to-yuma</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46667821\">https://news.ycombinator.com/item?id=46667821</a></p>\n<p>Points: 8</p>\n<p># Comments: 0</p>",
      "author": "HR01",
      "published_date": "2026-01-18T13:54:09+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-18T15:20:41.265320+00:00",
      "updated_at": "2026-01-18T15:20:41.265322+00:00"
    },
    {
      "id": "d8fbaf5ba268126656238a3bdc1282fc",
      "url": "https://www.qu8n.com/posts/most-important-software-engineering-skill-2026",
      "title": "Software engineers can no longer neglect their soft skills",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46667572\">Comments</a>",
      "author": "",
      "published_date": "2026-01-18T13:14:20+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-18T14:42:11.678371+00:00",
      "updated_at": "2026-01-18T14:42:11.678373+00:00"
    },
    {
      "id": "557ff130fa0a827e366dbded4cf5206f",
      "url": "https://ossa-ma.github.io/blog/openads",
      "title": "The A in AGI Stands for Ads",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46668021\">Comments</a>",
      "author": "",
      "published_date": "2026-01-18T14:25:49+00:00",
      "source": "Hacker News",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 2,
      "reading_time": 1,
      "created_at": "2026-01-18T14:42:11.678264+00:00",
      "updated_at": "2026-01-18T14:42:11.678268+00:00"
    },
    {
      "id": "e5246537140813f4ad3087687aae473d",
      "url": "http://ieeexplore.ieee.org/document/11170404",
      "title": "Rendering Affective Touch With an Array of Pneumatic Unit Cell Actuators",
      "content": "Rendering affective touch through haptic interfaces has gathered significant interest due to its ability to elicit emotional responses. Among various forms of affective touch, this study focuses on stroke stimuli. An illusory stroke stimulus is rendered using eight discrete Pneumatic Unit Cell (PUC) actuators on the left forearm. The study systematically investigates how rendering parameters\u2014including the traveling speed of the illusory stroke, the stimulus onset asynchrony (SOA) of consecutive indentations, and indentation pressure\u2014affect the perceived pleasantness and continuity of the stimulus. Results reveal that higher speeds significantly improved both pleasantness and continuity, with speed emerging as the most influential factor. In contrast, SOA has no significant effect on either perceived pleasantness or continuity. Indentation pressure shows a moderate impact on pleasantness, with high pressures reducing pleasantness but having no significant effect on continuity. Additionally, a positive correlation is observed between perceived pleasantness and continuity, underscoring the relevance of the continuity illusion created by sequential indentations with discrete actuators in evoking pleasant sensations. These findings demonstrate the potential of PUC actuators for creating affective touch stimuli and provide preliminary insights into the influence of rendering parameters on affective touch in human-machine and human-robot interactions.",
      "author": "",
      "published_date": "2025-09-18T13:16:54+00:00",
      "source": "Transactions Haptics",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 193,
      "reading_time": 1,
      "created_at": "2026-01-18T14:20:34.264930+00:00",
      "updated_at": "2026-01-18T14:20:34.264932+00:00"
    },
    {
      "id": "c2badf68cacc9dec3d628aadc12aa158",
      "url": "http://ieeexplore.ieee.org/document/11303644",
      "title": "Monolingual, Non-tone Bilingual, and Tone Bilingual Infants: Language Experiences Alter Speech and Nonspeech Perception",
      "content": "Studies on first-year infants' pitch perception have witnessed shifts of perceptual focus from acoustic to linguistic information and from a wide range of contrasts to those relevant to their native language. Nevertheless, how linguistic experience interacts with this developmental process remains an open question. This study compared the neural discrimination of speech/lexical and nonspeech/violin tone contrasts by 5- to 6- and 11- to 12-month-old infants across three types of language backgrounds: monolingual infants learning a non-tone language (Mono), bilingual infants learning two non-tone languages (Bi-NT), and bilingual infants learning a non-tone and a tone language (Bi-Tone). Although Mono infants do not show significant responses to the lexical tone contrast, both Bi-NT and Bi-Tone infants showed positive mismatch responses at both ages, indicating an enhancement effect brought by a complex language environment as early as 5 months after birth. Regarding the violin tone perception, distinct patterns were observed across language backgrounds: a perceptual decrease for Mono infants, no significant response for Bi-NT infants, and a perceptual increase for Bi-Tone infants over the first year. These patterns suggest that pitch perception may be affected across domains by language experiences at this stage, where interactions in cognitive processing between speech and nonspeech prosodic information may occur.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 203,
      "reading_time": 1,
      "created_at": "2026-01-18T14:20:29.623022+00:00",
      "updated_at": "2026-01-18T14:20:29.623023+00:00"
    },
    {
      "id": "ab043b2bcd71b18a1c1891ad57329ddc",
      "url": "http://ieeexplore.ieee.org/document/11303559",
      "title": "Oscillatory Correlates of Metacontrol: Beta and Theta Band Contributions to Feedback-dependent Cognitive Adaptation",
      "content": "The ability to adapt to varying task demands is essential for goal-directed behavior. Cognitive control styles regulate this adaptation, with persistence reflecting high top\u2013down control and flexibility reflecting lower control. Metacontrol facilitates the dynamic adjustment between these states based on current demands. The present study investigated short-term feedback-dependent adaptations in cognitive control style during conflict monitoring. Behavioral results demonstrated that RT feedback promoted a more persistent cognitive control style in subsequent trials, improving performance in incongruent conditions while diminishing facilitative effects in congruent conditions. On the neurophysiological level, theta-band activity primarily reflected these changes during conflict processing. Crucially, intertrial interval analyses revealed a key role of beta-band activity in using RT feedback. Correlations with behavioral congruency effects suggested that decreased beta-band activity reflected a generally more flexible control style, whereas increased beta-band activity was associated with generally greater persistence. By demonstrating that pretrial beta-band modulations reflect cognitive control dispositions, this study provides novel insights into the neural mechanisms underlying metacontrol.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 160,
      "reading_time": 1,
      "created_at": "2026-01-18T14:20:29.622988+00:00",
      "updated_at": "2026-01-18T14:20:29.622990+00:00"
    },
    {
      "id": "bc13a0801056a86eedc2331fa460c37f",
      "url": "http://ieeexplore.ieee.org/document/11303564",
      "title": "The Role of Rapid Eye Movement Sleep in Neural Differentiation of Memories in the Hippocampus",
      "content": "When faced with a familiar situation, we can use memory to make predictions about what will happen next. If such predictions turn out to be erroneous, the brain can adapt by differentiating the representations of the cue from the mispredicted item itself, reducing the likelihood of future prediction errors. Prior work by Kim, G., Norman, K. A., and Turk-Browne, N. B. Neural differentiation of incorrectly predicted memories. Journal of Neuroscience, 37, 2022\u20132031 [2017] found that violating a sequential association in a statistical learning paradigm triggered differentiation of the neural representations of the associated items in the hippocampus. Here, we used fMRI to test the preregistered hypothesis that this hippocampal differentiation occurs only when violations are followed by rapid eye movement (REM) sleep. Participants first learned that some items predict others (e.g., A predicts B) and then encountered a violation in which a predicted item (B) failed to appear when expected after its associated item (A); the predicted item later appeared on its own after an unrelated item. Participants were then randomly assigned to one of three conditions: remain awake, take a nap containing non-REM sleep only, or take a nap with both non-REM and REM sleep. While the predicted results were not observed in the preregistered left CA2/3/dentate gyrus (DG) ROI, we did observe evidence for our hypothesis in closely related hippocampal ROIs, uncorrected for multiple comparisons: In right CA2/3/DG, differentiation in the group with REM sleep was greater than in the groups without REM sleep (wake and non-REM nap); this differentiation was item-specific and concentrated in right DG. REM-related differentiation effects were also greater in bilateral DG when the predicted item was more strongly reactivated during the violation. Overall, these results provide initial evidence linking REM sleep to changes in the hippocampal representations of memories in humans.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 298,
      "reading_time": 1,
      "created_at": "2026-01-18T14:20:29.622955+00:00",
      "updated_at": "2026-01-18T14:20:29.622957+00:00"
    },
    {
      "id": "053a64c388be11291b58cb5dddb68ec6",
      "url": "http://ieeexplore.ieee.org/document/11303561",
      "title": "Scene-sensitive Medial Temporal Lobe Subregions Are Recruited for the Integration of Non-scene Stimuli",
      "content": "A hallmark feature of episodic memory is the ability to flexibly recombine information across episodes to form new associations and guide behavior. This process, termed associative inference, relies on the hippocampus and surrounding medial temporal lobe (MTL) subregions. We previously found that cross-episode binding was improved when episodes were linked by scenes rather than by faces or objects. Here, we tested whether differential recruitment of category-sensitive MTL subregions underlies these behavioral differences. Participants completed study-test phases of the Associative Inference in Memory task while undergoing fMRI scanning. During the study phase, they encoded overlapping AB and BC pairs. A and C items were always objects. The linking B item was either a face or a scene. At test, memory for the direct (AB, BC) and indirect associations (inferred AC) was tested. Category sensitivity in MTL subregions was tested using an independent functional localizer and the low integration (AB) trials from the study phase of the Associative Inference in Memory task. Within the MTL, no subregions exhibited face sensitivity. The anterior hippocampal head, anterolateral and posteromedial entorhinal cortices, and parahippocampal cortex were identified as scene sensitive. Although accuracy of the indirect inferences did not differ between pairs linked by faces and scenes, MTL subregion recruitment differed across categories. Scene-sensitive subregions in MTL cortex (anterolateral entorhinal cortex, posteromedial entorhinal cortex, and parahippocampal cortex), but not the hippocampus (anterior hippocampal head), were recruited to support associative inference for faces during encoding. These findings suggest that regions in MTL cortex identified as scene sensitive here may be involved in integrating disparate elements of episodes into coherent representations, and may be recruited for non-scene stimuli when integration demands during encoding are high (e.g., during associative inference).",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 281,
      "reading_time": 1,
      "created_at": "2026-01-18T14:20:29.622913+00:00",
      "updated_at": "2026-01-18T14:20:29.622914+00:00"
    },
    {
      "id": "d07d9c88cf921ac53c3b05c1709a7d87",
      "url": "http://ieeexplore.ieee.org/document/11303567",
      "title": "Increasing Signal, Reducing Noise: Contrasting Neural Mechanisms of Attention in Visual Search",
      "content": "When invariant target\u2013distractor arrays are presented repeatedly during visual search, participants respond faster on repeated versus novel configuration trials. This effect reflects attentional guidance through long-term memory (LTM) templates\u2014a phenomenon termed contextual cueing. Subsequently, relocating the target within the same distractor layout abolishes any contextual cueing effects, and relearning the new target position is much harder than the initial learning\u2014likely due to consistent attentional misguidance toward the initial (learned) target position. Here, we studied how the different processes involved in contextual cueing and relearning affect the variability of neural responses in human participants as measured with EEG. Attention has previously been shown to reduce trial-by-trial variability in EEG [Arazi, A., Yeshurun, Y., & Dinstein, I. Neural variability is quenched by attention. Journal of Neuroscience, 39, 5975\u20135985, 2019], indicating that, in addition to increasing the neural response to an attended stimulus, attention may reduce the noise within the neural response itself. While repeated versus novel contexts did not modulate the trial-by-trial variability during initial learning, significant lateralized variability reductions were observed for repeated but not novel context trials in the relocations phase. This contrasts with how contextual cueing affected lateralized ERPs in past research. Zinchenko and colleagues [Zinchenko, A., Conci, M., T\u00f6llner, T., M\u00fcller, H. J., & Geyer, T. Automatic guidance (and misguidance) of visuospatial attention by acquired scene memory: Evidence from an N1pc polarity reversal. Psychological Science, 31, 1531\u20131543, 2020] found that lateralized ERPs signal correct and incorrect (i.e., misguided) attentional selection of target positions learned earlier. This phenomenon was observed during both the learning and relocation phases. Thus, variability and lateralized ERPs may represent different facets of attention, where variability becomes evident specifically under high attentional demand conditions, such as when participants must override the misguidance caused by LTM templates.",
      "author": "",
      "published_date": "2025-12-18T13:16:55+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 291,
      "reading_time": 1,
      "created_at": "2026-01-18T14:20:29.622875+00:00",
      "updated_at": "2026-01-18T14:20:29.622876+00:00"
    },
    {
      "id": "b8090082b18153f12c075a153c6e8df8",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811926000212?dgcid=rss_sd_all",
      "title": "Distinct post-sentence neural patterns representing lexical items vs. sentence integration",
      "content": "<p>Publication date: 15 February 2026</p><p><b>Source:</b> NeuroImage, Volume 327</p><p>Author(s): Yonghyeon Gwon, Chun Kee Chung</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 13,
      "reading_time": 1,
      "created_at": "2026-01-18T14:20:16.317220+00:00",
      "updated_at": "2026-01-18T14:20:16.317222+00:00"
    }
  ]
}