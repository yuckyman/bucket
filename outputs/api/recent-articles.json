{
  "last_updated": "2025-10-18T04:16:33.441085+00:00",
  "count": 20,
  "articles": [
    {
      "id": "132a662524e3f66c5f49e360eddb4c4b",
      "url": "https://arxiv.org/abs/2510.14277",
      "title": "GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds and Characters",
      "content": "arXiv:2510.14277v1 Announce Type: new \nAbstract: We introduce GenLARP, a virtual reality (VR) system that transforms personalized stories into immersive live action role-playing (LARP) experiences. GenLARP enables users to act as both creators and players, allowing them to design characters based on their descriptions and live in the story world. Generative AI and agents powered by Large Language Models (LLMs) enrich these experiences.",
      "author": "Yichen Yu, Yifan Jiang, Mandy Lui, Qiao Jin",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 62,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.233135+00:00",
      "updated_at": "2025-10-18T04:16:33.339352+00:00",
      "metadata": {
        "processed_at": "2025-10-18T04:16:33.339377+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "eaab92b0431b3ef22abe65172c04d246",
      "url": "https://arxiv.org/abs/2510.14267",
      "title": "TapNav: Adaptive Spatiotactile Screen Readers for Tactually Guided Touchscreen Interactions for Blind and Low Vision People",
      "content": "arXiv:2510.14267v1 Announce Type: new \nAbstract: Screen readers are audio-based software that Blind and Low Vision (BLV) people use to interact with computing devices, such as tablets and smartphones. Although this technology has significantly improved the accessibility of touchscreen devices, the sequential nature of audio limits the bandwidth of information users can receive and process. We introduce TapNav, an adaptive spatiotactile screen reader prototype developed to interact with touchscreen interfaces spatially. TapNav's screen reader provides adaptive auditory feedback that, in combination with a tactile overlay, conveys spatial information and location of interface elements on-screen. We evaluated TapNav with 12 BLV users who interacted with TapNav to explore a data visualization and interact with a bank transactions application. Our qualitative findings show that touch points and spatially constrained navigation helped users anticipate outcomes for faster exploration, and offload cognitive load to touch. We provide design guidelines for creating tactile overlays for adaptive spatiotactile screen readers and discuss their generalizability beyond our exploratory data analysis and everyday application navigation scenarios.",
      "author": "Ricardo Gonzalez, Fannie Liu, Blair MacIntyre, David Saffo",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 167,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.233113+00:00",
      "updated_at": "2025-10-18T04:16:33.339382+00:00",
      "metadata": {
        "processed_at": "2025-10-18T04:16:33.339384+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "1f46a7f742e44396ae60bdcb0995d517",
      "url": "https://arxiv.org/abs/2510.14247",
      "title": "VisAider: AI-Assisted Context-Aware Visualization Support for Data Presentations",
      "content": "arXiv:2510.14247v1 Announce Type: new \nAbstract: Effective real-time data presentation is essential in small-group interactive contexts, where discussions evolve dynamically and presenters must adapt visualizations to shifting audience interests. However, most existing interactive visualization systems rely on fixed mappings between user actions and visualization commands, limiting their ability to support richer operations such as changing visualization types, adjusting data transformations, or incorporating additional datasets on the fly during live presentations. This work-in-progress paper presents VisAider, an AI-assisted interactive data presentation prototype that continuously analyzes the live presentation context, including the available dataset, active visualization, ongoing conversation, and audience profile, to generate ranked suggestions for relevant visualization aids. Grounded in a formative study with experienced data analysts, we identified key challenges in adapting visual content in real time and distilled design considerations to guide system development. A prototype implementation demonstrates the feasibility of this approach in simulated scenarios, and preliminary testing highlights challenges in inferring appropriate data transformations, resolving ambiguous visualization tasks, and achieving low-latency responsiveness. Ongoing work focuses on addressing these limitations, integrating the system into presentation environments, and preparing a summative user study to evaluate usability and communicative impact.",
      "author": "Kentaro Takahira, Yuki Ueno",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.233081+00:00",
      "updated_at": "2025-10-18T04:16:33.339387+00:00",
      "metadata": {
        "processed_at": "2025-10-18T04:16:33.339389+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "bfe1505ed4f6fa5eaa70a289330f3e09",
      "url": "https://arxiv.org/abs/2510.14141",
      "title": "Understanding Data Usage when Making High-Stakes Frontline Decisions in Homelessness Services",
      "content": "arXiv:2510.14141v1 Announce Type: new \nAbstract: Frontline staff of emergency shelters face challenges such as vicarious trauma, compassion fatigue, and burnout. The technology they use is often not designed for their unique needs, and can feel burdensome on top of their already cognitively and emotionally taxing work. While existing literature focuses on data-driven technologies that automate or streamline frontline decision-making about vulnerable individuals, we discuss scenarios in which staff may resist such automation. We then suggest how data-driven technologies can better align with their human-centred decision-making processes. This paper presents findings from a qualitative fieldwork study conducted from 2022 to 2024 at a large emergency shelter in Canada. The goal of this fieldwork was to co-design, develop, and deploy an interactive data-navigation interface that supports frontline staff when making collaborative, high-stakes decisions about individuals experiencing homelessness. By reflecting on this fieldwork, we contribute insight into the role that administrative shelter data play during decision-making, and unpack staff members' apparent reluctance to outsource decisions about vulnerable individuals to data systems. Our findings suggest a data-outsourcing continuum, which we discuss in terms of how designers may create technologies to support compassionate, data-driven decision-making in nonprofit domains.",
      "author": "Teale W. Masrani, Geoffrey Messier, Amy Voida, Gina Dimitropoulos, Helen Ai He",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 193,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.233048+00:00",
      "updated_at": "2025-10-18T04:16:33.339391+00:00",
      "metadata": {
        "processed_at": "2025-10-18T04:16:33.339392+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "af2494df3bdf66e61db702dfccefc05e",
      "url": "https://arxiv.org/abs/2510.13814",
      "title": "Reversing the Lens: Using Explainable AI to Understand Human Expertise",
      "content": "arXiv:2510.13814v1 Announce Type: new \nAbstract: Both humans and machine learning models learn from experience, particularly in safety- and reliability-critical domains. While psychology seeks to understand human cognition, the field of Explainable AI (XAI) develops methods to interpret machine learning models. This study bridges these domains by applying computational tools from XAI to analyze human learning. We modeled human behavior during a complex real-world task -- tuning a particle accelerator -- by constructing graphs of operator subtasks. Applying techniques such as community detection and hierarchical clustering to archival operator data, we reveal how operators decompose the problem into simpler components and how these problem-solving structures evolve with expertise. Our findings illuminate how humans develop efficient strategies in the absence of globally optimal solutions, and demonstrate the utility of XAI-based methods for quantitatively studying human cognition.",
      "author": "Roussel Rahman, Aashwin Ananda Mishra, Wan-Lin Hu",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "processed",
      "priority": "medium",
      "tags": [],
      "word_count": 134,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.233014+00:00",
      "updated_at": "2025-10-18T04:16:33.339394+00:00",
      "metadata": {
        "processed_at": "2025-10-18T04:16:33.339396+00:00",
        "processing_method": "github_actions"
      }
    },
    {
      "id": "456a3aca9258f2274f99c42dd52630d3",
      "url": "https://arxiv.org/abs/2510.13813",
      "title": "Puzzlegram: a Serious Game Designed for the Elderly in Group Settings",
      "content": "arXiv:2510.13813v1 Announce Type: new \nAbstract: An original serious game prototype named 'Puzzlegram' is created for the elderly demographic in group settings as the target players. Puzzlegram is precisely designed to accentuate memory, auditory interaction as well as haptic response to visual signals with the use of music. Music is introduced as a key component for establishing the game design that provides a source of meaningful contextualization (familiar music from the past) for setting the game mechanics, which facilitated the construction of the serious game design process. The discussion topics raised include the need to design serious games for fostering meaningful interactions, as well as developing a thorough framework for constructing purposeful design for serious games. A potential integral of artificial intelligence to Puzzlegram may involve assigning a novel dimension to its existing problem solving task by adapting to varying states of cognitive function for monitoring purposes based on an individual's interaction with the game.",
      "author": "Sunny Choi",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.232986+00:00",
      "updated_at": "2025-10-18T03:47:19.232987+00:00"
    },
    {
      "id": "6f1f761779cdee839ff875e70bcb3794",
      "url": "https://arxiv.org/abs/2510.13812",
      "title": "MindBenchAI: An Actionable Platform to Evaluate the Profile and Performance of Large Language Models in a Mental Healthcare Context",
      "content": "arXiv:2510.13812v1 Announce Type: new \nAbstract: Individuals are increasingly utilizing large language model (LLM)based tools for mental health guidance and crisis support in place of human experts. While AI technology has great potential to improve health outcomes, insufficient empirical evidence exists to suggest that AI technology can be deployed as a clinical replacement; thus, there is an urgent need to assess and regulate such tools. Regulatory efforts have been made and multiple evaluation frameworks have been proposed, however,field-wide assessment metrics have yet to be formally integrated. In this paper, we introduce a comprehensive online platform that aggregates evaluation approaches and serves as a dynamic online resource to simplify LLM and LLM-based tool assessment: MindBenchAI. At its core, MindBenchAI is designed to provide easily accessible/interpretable information for diverse stakeholders (patients, clinicians, developers, regulators, etc.). To create MindBenchAI, we built off our work developing MINDapps.org to support informed decision-making around smartphone app use for mental health, and expanded the technical MINDapps.org framework to encompass novel large language model (LLM) functionalities through benchmarking approaches. The MindBenchAI platform is designed as a partnership with the National Alliance on Mental Illness (NAMI) to provide assessment tools that systematically evaluate LLMs and LLM-based tools with objective and transparent criteria from a healthcare standpoint, assessing both profile (i.e. technical features, privacy protections, and conversational style) and performance characteristics (i.e. clinical reasoning skills).",
      "author": "Bridget Dwyer, Matthew Flathers, Akane Sano, Allison Dempsey, Andrea Cipriani, Asim H. Gazi, Carla Gorban, Carolyn I. Rodriguez, Charles Stromeyer IV, Darlene King, Eden Rozenblit, Gillian Strudwick, Jake Linardon, Jiaee Cheong, Joseph Firth, Julian Herpertz, Julian Schwarz, Margaret Emerson, Martin P. Paulus, Michelle Patriquin, Yining Hua, Soumya Choudhary, Steven Siddals, Laura Ospina Pinillos, Jason Bantjes, Steven Scheuller, Xuhai Xu, Ken Duckworth, Daniel H. Gillison, Michael Wood, John Torous",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 224,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.232952+00:00",
      "updated_at": "2025-10-18T03:47:19.232954+00:00"
    },
    {
      "id": "a8460891218f7cbc6340d7b681f43143",
      "url": "https://arxiv.org/abs/2510.13811",
      "title": "Generative AI in Heritage Practice: Improving the Accessibility of Heritage Guidance",
      "content": "arXiv:2510.13811v1 Announce Type: new \nAbstract: This paper discusses the potential for integrating Generative Artificial Intelligence (GenAI) into professional heritage practice with the aim of enhancing the accessibility of public-facing guidance documents. We developed HAZEL, a GenAI chatbot fine-tuned to assist with revising written guidance relating to heritage conservation and interpretation. Using quantitative assessments, we compare HAZEL's performance to that of ChatGPT (GPT-4) in a series of tasks related to the guidance writing process. The results of this comparison indicate a slightly better performance of HAZEL over ChatGPT, suggesting that the GenAI chatbot is more effective once the underlying large language model (LLM) has been fine-tuned. However, we also note significant limitations, particularly in areas requiring cultural sensitivity and more advanced technical expertise. These findings suggest that, while GenAI cannot replace human heritage professionals in technical authoring tasks, its potential to automate and expedite certain aspects of guidance writing could offer valuable benefits to heritage organisations, especially in resource-constrained contexts.",
      "author": "Jessica Witte, Edmund Lee, Lisa Brausem, Verity Shillabeer, Chiara Bonacchi",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.232912+00:00",
      "updated_at": "2025-10-18T03:47:19.232914+00:00"
    },
    {
      "id": "1cf440b4b06aab4a0605863ccd30cd09",
      "url": "https://arxiv.org/abs/2510.13810",
      "title": "Choreographing Trash Cans: On Speculative Futures of Weak Robots in Public Spaces",
      "content": "arXiv:2510.13810v1 Announce Type: new \nAbstract: Delivering groceries or cleaning airports, mobile robots exist in public spaces. While these examples showcase robots that execute tasks, this paper explores mobile robots that encourage posthuman collaboration rather than managing environments independently. With feigned fragility, cuteness and incomplete functionalities, the so-called \"weak robots\" invite passersby to engage not only on a utilitarian level, but also through imaginative and emotional responses. After examining the workings of \"weak robots\" by queering notions of function and ability, we introduce two speculative design fiction vignettes that describe choreographies of such robots in future urban spaces -- one exploring a utopian weak robot and the other a dystopian weak robot. We introduce these speculations in order to discuss how different values may drive design decisions, and how such decisions may shape and drive different socio-technical futures in which robots and humans share public spaces that incentivise collaboration.",
      "author": "Minja Axelsson, Lea Luka Sikau",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 148,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:19.232873+00:00",
      "updated_at": "2025-10-18T03:47:19.232878+00:00"
    },
    {
      "id": "e5ee3247f77859dca98e1b45a2701a79",
      "url": "https://arxiv.org/abs/2510.14486",
      "title": "Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks",
      "content": "arXiv:2510.14486v1 Announce Type: new \nAbstract: Brains learn to represent information from a large set of stimuli, typically by weak supervision. Unsupervised learning is therefore a natural approach for exploring the design of biological neural networks and their computations. Accordingly, redundancy reduction has been suggested as a prominent design principle of neural encoding, but its ``mechanistic'' biological implementation is unclear. Analogously, unsupervised training of artificial neural networks yields internal representations that allow for accurate stimulus classification or decoding, but typically rely on biologically-implausible implementations. We suggest that interactions between parallel subnetworks in the brain may underlie such learning: we present a model of representation learning by ensembles of neural networks, where each network learns to encode stimuli into an abstract representation space by cross-supervising interactions with other networks, for inputs they receive simultaneously or in close temporal proximity. Aiming for biological plausibility, each network has a small ``receptive field'', thus receiving a fixed part of the external input, and the networks do not share weights. We find that for different types of network architectures, and for both visual or neuronal stimuli, these cross-supervising networks learn semantic representations that are easily decodable and that decoding accuracy is comparable to supervised networks -- both at the level of single networks and the ensemble. We further show that performance is optimal for small receptive fields, and that sparse connectivity between networks is nearly as accurate as all-to-all interactions, with far fewer computations. We thus suggest a sparsely interacting collective of cross-supervising networks as an algorithmic framework for representational learning and collective computation in the brain.",
      "author": "Roy Urbach, Elad Schneidman",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 261,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.058152+00:00",
      "updated_at": "2025-10-18T03:47:18.058153+00:00"
    },
    {
      "id": "481ae57a96b36ec0c1933f184466871c",
      "url": "https://arxiv.org/abs/2510.14382",
      "title": "Joint encoding of \"what\" and \"when\" predictions through error-modulated plasticity in reservoir spiking networks",
      "content": "arXiv:2510.14382v1 Announce Type: new \nAbstract: The brain understands the external world through an internal model that generates predictions and refines them based on prediction errors. A complete prediction specifies what will happen, when it will happen, and with what probability, which we refer to as a \"prediction object\". Existing models typically capture only what and when, omit probabilities, and rely on biologically-implausible algorithms. Here we show that a single population of spiking neurons can jointly encode the prediction object through a biologically grounded learning mechanism. We implement a heterogeneous Izhikevich spiking reservoir with readouts trained by an error-modulated, attention-gated three-factor Hebbian rule and test it on a novel paradigm that controls both the timing and probability of upcoming stimuli. By integrating real-time learning of \"when\" with offline consolidation of \"what\", the model encodes the complete prediction object, firing at the correct times with magnitudes proportional to the probabilities. Critically, it rapidly adapts to changes in both stimulus timing and probability, an ability that global least-squares methods such as FORCE lack without explicit resets. During learning, the model self-organizes its readout weights into near-orthogonal subspaces for \"what\" and \"when,\" showing that multiplexed encoding arises naturally from generic recurrent dynamics under local, error-gated modulation. These results challenge the view that \"what\" and \"when\" predictions require separate modules, suggesting instead that mixed selectivity within shared populations supports flexible predictive cognition. The model also predicts phase-specific neuromodulation and overlapping neural subspaces, offering a parsimonious alternative to hierarchical predictive-coding accounts.",
      "author": "Yohei Yamada, Zenas C. Chao",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 245,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.058113+00:00",
      "updated_at": "2025-10-18T03:47:18.058114+00:00"
    },
    {
      "id": "90f2d38ebe522ebdbc2c999fedbb2e26",
      "url": "https://arxiv.org/abs/2510.14227",
      "title": "Sensorimotor Contingencies and The Sensorimotor Approach to Cognition",
      "content": "arXiv:2510.14227v1 Announce Type: new \nAbstract: 4E views of cognition seek to replace many of the long-held assumptions of tra- ditional cognitive science. One of the most radical shifts is the rejection of the sandwich model of cognition [8], which holds that mental processes are located be- tween action and perception. Subversion of such a long-held assumption requires an accessible theoretical alternative with firm experimental support. One unifying thread among the emerging 4E camps is their shared insistence that sensorimotor contingencies (SMCs) are such an alternative.",
      "author": "Denizhan Pak",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 85,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.058076+00:00",
      "updated_at": "2025-10-18T03:47:18.058077+00:00"
    },
    {
      "id": "d47895391b2b090b0787818810b2f674",
      "url": "https://arxiv.org/abs/2510.14188",
      "title": "Using Information Geometry to Characterize Higher-Order Interactions in EEG",
      "content": "arXiv:2510.14188v1 Announce Type: new \nAbstract: In neuroscience, methods from information geometry (IG) have been successfully applied in the modelling of binary vectors from spike train data, using the orthogonal decomposition of the Kullback-Leibler divergence and mutual information to isolate different orders of interaction between neurons. While spike train data is well-approximated with a binary model, here we apply these IG methods to data from electroencephalography (EEG), a continuous signal requiring appropriate discretization strategies. We developed and compared three different binarization methods and used them to identify third-order interactions in an experiment involving imagined motor movements. The statistical significance of these interactions was assessed using phase-randomized surrogate data that eliminated higher-order dependencies while preserving the spectral characteristics of the original signals. We validated our approach by implementing known second- and third-order dependencies in a forward model and quantified information attenuation at different steps of the analysis. This revealed that the greatest loss in information occurred when going from the idealized binary case to enforcing these dependencies using oscillatory signals. When applied to the real EEG dataset, our analysis detected statistically significant third-order interactions during the task condition despite the relatively sparse data (45 trials per condition). This work demonstrates that IG methods can successfully extract genuine higher-order dependencies from continuous neural recordings when paired with appropriate binarization schemes.",
      "author": "Eric Albers, Paul Marriott, Masami Tatsuno",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.058051+00:00",
      "updated_at": "2025-10-18T03:47:18.058052+00:00"
    },
    {
      "id": "b9f15b336673b983603e02a651d0416a",
      "url": "https://arxiv.org/abs/2510.13894",
      "title": "Bayes or Heisenberg: Who(se) Rules?",
      "content": "arXiv:2510.13894v1 Announce Type: new \nAbstract: Although quantum systems are generally described by quantum state vectors, we show that in certain cases their measurement processes can be reformulated as probabilistic equations expressed in terms of probabilistic state vectors. These probabilistic representations can, in turn, be approximated by the neural network dynamics of the Tensor Brain (TB) model.\n  The Tensor Brain is a recently proposed framework for modeling perception and memory in the brain, providing a biologically inspired mechanism for efficiently integrating generated symbolic representations into reasoning processes.",
      "author": "Volker Tresp Hang Li, Federico Harjes, Yunpu Ma",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 86,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.058015+00:00",
      "updated_at": "2025-10-18T03:47:18.058016+00:00"
    },
    {
      "id": "b2e0ceeb3b5402a672e631e0040042b7",
      "url": "https://arxiv.org/abs/2510.13883",
      "title": "Large Language Model Agents Enable Autonomous Design and Image Analysis of Microwell Microfluidics",
      "content": "arXiv:2510.13883v1 Announce Type: new \nAbstract: Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis.",
      "author": "Dinh-Nguyen Nguyen, Sadia Shakil, Raymond Kai-Yu Tong, Ngoc-Duy Dinh",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 241,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.057989+00:00",
      "updated_at": "2025-10-18T03:47:18.057991+00:00"
    },
    {
      "id": "63755f4ca0647047e36cc92610f13f46",
      "url": "https://arxiv.org/abs/2510.13845",
      "title": "Embodiment in multimodal large language models",
      "content": "arXiv:2510.13845v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated extraordinary progress in bridging textual and visual inputs. However, MLLMs still face challenges in situated physical and social interactions in sensorally rich, multimodal and real-world settings where the embodied experience of the living organism is essential. We posit that next frontiers for MLLM development require incorporating both internal and external embodiment -- modeling not only external interactions with the world, but also internal states and drives. Here, we describe mechanisms of internal and external embodiment in humans and relate these to current advances in MLLMs in early stages of aligning to human representations. Our dual-embodied framework proposes to model interactions between these forms of embodiment in MLLMs to bridge the gap between multimodal data and world experience.",
      "author": "Akila Kadambi, Lisa Aziz-Zadeh, Antonio Damasio, Marco Iacoboni, Srini Narayanan",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 129,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.057950+00:00",
      "updated_at": "2025-10-18T03:47:18.057952+00:00"
    },
    {
      "id": "ce4dcf9790072a5d8ec27caba6ab1ded",
      "url": "https://arxiv.org/abs/2510.13841",
      "title": "Hybrid Deep Learning Approaches for Classifying Autism from Brain MRI",
      "content": "arXiv:2510.13841v1 Announce Type: new \nAbstract: Autism spectrum disorder (ASD) is most often diagnosed using behavioral evaluations, which can vary between clinicians. Brain imaging, combined with machine learning, may help identify more objective patterns linked to ASD. This project used magnetic resonance imaging (MRI) data from the publicly available ABIDE I dataset (n = 1,112) to test two approaches for classifying ASD and control participants. The first was a 3D convolutional neural network (CNN) trained end-to-end. The second was a hybrid approach that used the CNN as a feature extractor and then applied a support vector machine (SVM) classifier. The baseline CNN reached moderate performance (accuracy = 0.66, AUC = 0.70), while the hybrid CNN + SVM achieved higher overall accuracy (0.76) and AUC (0.80). The hybrid model also produced more balanced results between ASD and control groups. Separating feature extraction and classification improved performance and reduced bias between diagnostic groups. These findings suggest that combining deep learning and traditional machine learning methods could enhance the reliability of MRI-based research on ASD.",
      "author": "Ashley Chen",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 171,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.057921+00:00",
      "updated_at": "2025-10-18T03:47:18.057923+00:00"
    },
    {
      "id": "3cd7aec4052579f9593668650cb055f8",
      "url": "https://arxiv.org/abs/2510.13826",
      "title": "Towards Neurocognitive-Inspired Intelligence: From AI's Structural Mimicry to Human-Like Functional Cognition",
      "content": "arXiv:2510.13826v1 Announce Type: new \nAbstract: Artificial intelligence has advanced significantly through deep learning, reinforcement learning, and large language and vision models. However, these systems often remain task specific, struggle to adapt to changing conditions, and cannot generalize in ways similar to human cognition. Additionally, they mainly focus on mimicking brain structures, which often leads to black-box models with limited transparency and adaptability. Inspired by the structure and function of biological cognition, this paper introduces the concept of \"Neurocognitive-Inspired Intelligence (NII),\" a hybrid approach that combines neuroscience, cognitive science, computer vision, and AI to develop more general, adaptive, and robust intelligent systems capable of rapid learning, learning from less data, and leveraging prior experience. These systems aim to emulate the human brain's ability to flexibly learn, reason, remember, perceive, and act in real-world settings with minimal supervision. We review the limitations of current AI methods, define core principles of neurocognitive-inspired intelligence, and propose a modular, biologically inspired architecture that emphasizes integration, embodiment, and adaptability. We also discuss potential implementation strategies and outline various real-world applications, from robotics to education and healthcare. Importantly, this paper offers a hybrid roadmap for future research, laying the groundwork for building AI systems that more closely resemble human cognition.",
      "author": "Noorbakhsh Amiri Golilarz, Hassan S. Al Khatib, Shahram Rahimi",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 203,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.057886+00:00",
      "updated_at": "2025-10-18T03:47:18.057888+00:00"
    },
    {
      "id": "1cab55360f1c8c23758b71c8977afbab",
      "url": "https://arxiv.org/abs/2510.13815",
      "title": "A Two-Feature Quantitative EEG Index of Pediatric Epilepsy Severity: External Pre-Validation on CHB-MIT and Roadmap to Dravet Cohorts",
      "content": "arXiv:2510.13815v1 Announce Type: new \nAbstract: Objective biomarkers for staging pediatric epileptic encephalopathies are scarce. We revisited a large open repository -- the CHB-MIT Scalp EEG Database, 22 subjects aged 1.5-19 y recorded at 256 Hz under the 10-20 montage -- to derive and validate a compact quantitative index, DS-Qi = (theta/alpha)_posterior + (1 - wPLI_beta). The first term captures excess posterior slow-wave power, a recognized marker of impaired cortical maturation; the second employs the debiased weighted Phase-Lag Index to measure loss of beta-band synchrony, robust to volume conduction and small-sample bias. In 30-min awake, eyes-open segments, DS-Qi was 1.69 +/- 0.21 in epilepsy versus 1.23 +/- 0.17 in age-matched normative EEG (Cohen's d = 1.1, p < 0.001). A logistic model trained with 10 x 10-fold cross-validation yielded an AUC of 0.90 (95% CI 0.81-0.97) and optimal sensitivity/specificity of 86%/83% at DS-Qi = 1.46. Across multi-day recordings, test-retest reliability was ICC = 0.74, and higher DS-Qi correlated with greater seizure burden (rho = 0.58, p = 0.004). These results establish DS-Qi as a reproducible, single-number summary of electrophysiological severity that can be computed from short scalp EEG segments using only posterior and standard 10-20 electrodes.",
      "author": "Khartik Uppalapati, Bora Yimenicioglu, Shakeel Abdulkareem, Bhavya Uppalapati, Viraj Kamath, Adan Eftekhari, Pranav Ayyappan",
      "published_date": "2025-10-17T04:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 195,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:18.057839+00:00",
      "updated_at": "2025-10-18T03:47:18.057846+00:00"
    },
    {
      "id": "56703644563a7813e9893ff320446b83",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306452225009832?dgcid=rss_sd_all",
      "title": "C9orf72 related poly-Glycine-Alanine promotes tau phosphorylation and cell death via ERK1/2 interaction in cellular models",
      "content": "<p>Publication date: 10 November 2025</p><p><b>Source:</b> Neuroscience, Volume 587</p><p>Author(s): Jiahan Zhuang, Zixuan Zhang, Hongfu Jin, Ji Qi, Yuanyuan Chen, Lin Ding, Chenglai Fu, Weiwei Cheng</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 24,
      "reading_time": 1,
      "created_at": "2025-10-18T03:47:11.773614+00:00",
      "updated_at": "2025-10-18T03:47:11.773616+00:00"
    }
  ]
}