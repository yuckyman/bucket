{
  "last_updated": "2025-12-16T05:25:11.760030+00:00",
  "count": 20,
  "articles": [
    {
      "id": "bdd171593d416ff5533a5fb1ddd7e15d",
      "url": "https://arxiv.org/abs/2512.12348",
      "title": "Understanding Trust Toward Human versus AI-generated Health Information through Behavioral and Physiological Sensing",
      "content": "arXiv:2512.12348v1 Announce Type: new \nAbstract: As AI-generated health information proliferates online and becomes increasingly indistinguishable from human-sourced information, it becomes critical to understand how people trust and label such content, especially when the information is inaccurate. We conducted two complementary studies: (1) a mixed-methods survey (N=142) employing a 2 (source: Human vs. LLM) $\\times$ 2 (label: Human vs. AI) $\\times$ 3 (type: General, Symptom, Treatment) design, and (2) a within-subjects lab study (N=40) incorporating eye-tracking and physiological sensing (ECG, EDA, skin temperature). Participants were presented with health information varying by source-label combinations and asked to rate their trust, while their gaze behavior and physiological signals were recorded. We found that LLM-generated information was trusted more than human-generated content, whereas information labeled as human was trusted more than that labeled as AI. Trust remained consistent across information types. Eye-tracking and physiological responses varied significantly by source and label. Machine learning models trained on these behavioral and physiological features predicted binary self-reported trust levels with 73% accuracy and information source with 65% accuracy. Our findings demonstrate that adding transparency labels to online health information modulates trust. Behavioral and physiological features show potential to verify trust perceptions and indicate if additional transparency is needed.",
      "author": "Xin Sun, Rongjun Ma, Shu Wei, Pablo Cesar, Jos A. Bosch, Abdallah El Ali",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 201,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.308045+00:00",
      "updated_at": "2025-12-16T05:24:51.308047+00:00"
    },
    {
      "id": "7f43a46e0c6bea90d4ca3fce117e600c",
      "url": "https://arxiv.org/abs/2512.12283",
      "title": "Large Language Models have Chain-of-Affective",
      "content": "arXiv:2512.12283v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly deployed as collaborative agents in emotionally charged settings, yet most evaluations treat them as purely cognitive systems and largely ignore their affective behaviour. Here we take a functional perspective and ask whether contemporary LLMs implement a structured chain-of-affective: organised affective dynamics that are family-specific, temporally coherent and behaviourally consequential. Across eight major LLM families (GPT, Gemini, Claude, Grok, Qwen, DeepSeek, GLM, Kimi), we combine two experimental modules. The first characterises inner chains-of-affective via baseline ''affective fingerprints'', 15-round sad-news exposure, and a 10-round news self-selection paradigm. We find stable, family-specific affective profiles, a reproducible three-phase trajectory under sustained negative input (accumulation, overload, defensive numbing), distinct defence styles, and human-like negativity biases that induce self-reinforcing affect-choice feedback loops. The second module probes outer consequences using a composite performance benchmark, human-AI dialogues on contentious topics, and multi-agent LLM interactions. We demonstrate that induced affect preserves core reasoning while reshaping high-freedom generation. Sentiment metrics predict user comfort and empathy but reveal trade-offs in resisting problematic views. In multi-agent settings, group structure drives affective contagion, role specialization (initiators, absorbers, firewalls), and bias. We characterize affect as an emergent control layer, advocating for 'chains-of-affect' as a primary target for evaluation and alignment.",
      "author": "Junjie Xu, Xingjiao Wu, Luwei Xiao, Yuzhe Yang, Jie Zhou, Zihao Zhang, Luhan Wang, Yi Huang, Nan Wu, Yingbin Zheng, Chao Yan, Cheng Jin, Honglin Li, Liang He",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 207,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.308015+00:00",
      "updated_at": "2025-12-16T05:24:51.308016+00:00"
    },
    {
      "id": "b50fee1f877625e562131a28d99fb6e5",
      "url": "https://arxiv.org/abs/2512.12240",
      "title": "System X: A Mobile Voice-Based AI System for EMR Generation and Clinical Decision Support in Low-Resource Maternal Healthcare",
      "content": "arXiv:2512.12240v1 Announce Type: new \nAbstract: We present the design, implementation, and in-situ deployment of a smartphone-based voice-enabled AI system for generating electronic medical records (EMRs) and clinical risk alerts in maternal healthcare settings. Targeted at low-resource environments such as Pakistan, the system integrates a fine-tuned, multilingual automatic speech recognition (ASR) model and a prompt-engineered large language model (LLM) to enable healthcare workers to engage naturally in Urdu, their native language, regardless of literacy or technical background. Through speech-based input and localized understanding, the system generates structured EMRs and flags critical maternal health risks. Over a seven-month deployment in a not-for-profit hospital, the system supported the creation of over 500 EMRs and flagged over 300 potential clinical risks. We evaluate the system's performance across speech recognition accuracy, EMR field-level correctness, and clinical relevance of AI-generated red flags. Our results demonstrate that speech based AI interfaces, can be effectively adapted to real-world healthcare settings, especially in low-resource settings, when combined with structured input design, contextual medical dictionaries, and clinician-in-the-loop feedback loops. We discuss generalizable design principles for deploying voice-based mobile healthcare AI support systems in linguistically and infrastructurally constrained settings.",
      "author": "Maryam Mustafa, Umme Ammara, Amna Shahnawaz, Moaiz Abrar, Bakhtawar Ahtisham, Fozia Umber Qurashi, Mostafa Shahin, Beena Ahmed",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 188,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307983+00:00",
      "updated_at": "2025-12-16T05:24:51.307984+00:00"
    },
    {
      "id": "ab536dba85278877581780ed7e61ba27",
      "url": "https://arxiv.org/abs/2512.12207",
      "title": "Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search",
      "content": "arXiv:2512.12207v1 Announce Type: new \nAbstract: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.",
      "author": "Jiangen He, Jiqun Liu",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 132,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307953+00:00",
      "updated_at": "2025-12-16T05:24:51.307954+00:00"
    },
    {
      "id": "da418d3b6697b0ce9f7d36679b0ff4ed",
      "url": "https://arxiv.org/abs/2512.12201",
      "title": "Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation",
      "content": "arXiv:2512.12201v1 Announce Type: new \nAbstract: Large language models (LLMs) have often been characterized as \"stochastic parrots\" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.",
      "author": "Predrag K. Nikoli\\'c, Robert Prentner",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 164,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307926+00:00",
      "updated_at": "2025-12-16T05:24:51.307928+00:00"
    },
    {
      "id": "c8e690678a881337f0affab3f9fd29cc",
      "url": "https://arxiv.org/abs/2512.12166",
      "title": "Beyond Riding: Passenger Engagement with Driver Labor through Gamified Interactions",
      "content": "arXiv:2512.12166v1 Announce Type: new \nAbstract: Modern cities increasingly rely on ridesharing services for on-demand transportation, which offer consumers convenience and mobility across the globe. However, these marketed consumer affordances give rise to burdens and vulnerabilities that drivers shoulder alone, without adequate infrastructures for labor regulations or consumer-led advocacy. To effectively and sustainably advance protections and oversight for drivers, consumers must first be aware of the labor, logistics and costs involved with ridehail driving. To motivate consumers to practice more socially responsible consumption behaviors and foster solidarity with drivers, we explore the potential for gamified in-ride interactions to facilitate engagement with real (and lived) driver experiences. Through nine workshops with 19 drivers and 15 passengers, we surface how gamified in-ride interactions revealed passenger knowledge gaps around latent ridehail conditions, prompt reflection and shifts in perception of their relative power and consumption behaviors, and highlight drivers' preferences for creating more immersive and contextualized service experiences, and identify opportunities to design safe and appropriate passenger-driver interactions that motivate solidarity with drivers. In sum, we advance conceptual understandings of in-ride social and managerial relations, demonstrate potential for future worker advocacy in algorithmically-managed labor, and offer design guidelines for more human-centered workplace technologies.",
      "author": "Jane Hsieh, Emmie Regan, Jose Elizalde, Haiyi Zhu",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 198,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307896+00:00",
      "updated_at": "2025-12-16T05:24:51.307898+00:00"
    },
    {
      "id": "1529ba9d3aa11ba4891e3395460d5e8c",
      "url": "https://arxiv.org/abs/2512.12115",
      "title": "Teaching Spell Checkers to Teach: Pedagogical Program Synthesis for Interactive Learning",
      "content": "arXiv:2512.12115v1 Announce Type: new \nAbstract: Spelling taught through memorization often fails many learners, particularly children with language-based learning disorders who struggle with the phonological skills necessary to spell words accurately. Educators such as speech-language pathologists (SLPs) address this instructional gap by using an inquiry-based approach to teach spelling that targets the phonology, morphology, meaning, and etymology of words. Yet, these strategies rarely appear in everyday writing tools, which simply detect and autocorrect errors. We introduce SPIRE (Spelling Inquiry Engine), a spell check system that brings this inquiry-based pedagogy into the act of composition. SPIRE implements Pedagogical Program Synthesis, a novel approach for operationalizing the inherently dynamic pedagogy of spelling instruction. SPIRE represents SLP instructional moves in a domain-specific language, synthesizes tailored programs in real-time from learner errors, and renders them as interactive interfaces for inquiry-based interventions. With SPIRE, spelling errors become opportunities to explore word meanings, word structures, morphological families, word origins, and grapheme-phoneme correspondences, supporting metalinguistic reasoning alongside correction. Evaluation with SLPs and learners shows alignment with professional practice and potential for integration into writing workflows.",
      "author": "Momin N. Siddiqui, Vincent Cavez, Sahana Rangasrinivasan, Abbie Olszewski, Srirangaraj Setlur, Maneesh Agrawala, Hari Subramonyam",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 177,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307866+00:00",
      "updated_at": "2025-12-16T05:24:51.307867+00:00"
    },
    {
      "id": "261686274fd063c2c00bd8215a68f172",
      "url": "https://arxiv.org/abs/2512.12045",
      "title": "AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers",
      "content": "arXiv:2512.12045v1 Announce Type: new \nAbstract: This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.\n  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.\n  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.",
      "author": "Alex Liu (Victor), Lief Esbenshade (Victor), Shawon Sarkar (Victor),  Zewei (Victor),  Tian, Min Sun, Zachary Zhang, Thomas Han, Yulia Lapicus, Kevin He",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 195,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307835+00:00",
      "updated_at": "2025-12-16T05:24:51.307837+00:00"
    },
    {
      "id": "894c4a50511b7274718c8065d15a804e",
      "url": "https://arxiv.org/abs/2512.11979",
      "title": "Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)",
      "content": "arXiv:2512.11979v1 Announce Type: new \nAbstract: The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.",
      "author": "Marc Scibelli, Krystelle Gonzalez Papaux, Julia Valenti, Srishti Kush",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 128,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307801+00:00",
      "updated_at": "2025-12-16T05:24:51.307803+00:00"
    },
    {
      "id": "2a4692d5beab20a474742c53d6741f2a",
      "url": "https://arxiv.org/abs/2512.11844",
      "title": "Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines",
      "content": "arXiv:2512.11844v1 Announce Type: new \nAbstract: We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.",
      "author": "Haoyang Shang, Zhengyang Yan, Xuan Liu",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:51.307770+00:00",
      "updated_at": "2025-12-16T05:24:51.307773+00:00"
    },
    {
      "id": "b9a94db718d92fd9f388bacfee3da17c",
      "url": "https://arxiv.org/abs/2512.12881",
      "title": "Unsupervised learning of multiscale switching dynamical system models from multimodal neural data",
      "content": "arXiv:2512.12881v1 Announce Type: cross \nAbstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.",
      "author": "DongKyu Kim, Han-Lin Hsieh, Maryam M. Shanechi",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 237,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223833+00:00",
      "updated_at": "2025-12-16T05:24:50.223834+00:00"
    },
    {
      "id": "be891eb6e13cc8aea3dbb3498bbccf76",
      "url": "https://arxiv.org/abs/2512.12462",
      "title": "Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference",
      "content": "arXiv:2512.12462v1 Announce Type: cross \nAbstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.",
      "author": "Eray Erturk, Maryam M. Shanechi",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223800+00:00",
      "updated_at": "2025-12-16T05:24:50.223801+00:00"
    },
    {
      "id": "e05f59cf08cc3ed0a97c9bb37c79c0f5",
      "url": "https://arxiv.org/abs/2512.12461",
      "title": "Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling",
      "content": "arXiv:2512.12461v1 Announce Type: cross \nAbstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.",
      "author": "Eray Erturk, Saba Hashemi, Maryam M. Shanechi",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 217,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223769+00:00",
      "updated_at": "2025-12-16T05:24:50.223770+00:00"
    },
    {
      "id": "7364cb2b79c8b46f2808c76ad8ac611f",
      "url": "https://arxiv.org/abs/2512.12135",
      "title": "BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity",
      "content": "arXiv:2512.12135v1 Announce Type: cross \nAbstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.",
      "author": "Lucine L. Oganesian, Saba Hashemi, Maryam M. Shanechi",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 249,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223733+00:00",
      "updated_at": "2025-12-16T05:24:50.223734+00:00"
    },
    {
      "id": "2398f822168721bea69586e40455acbb",
      "url": "https://arxiv.org/abs/2512.13539",
      "title": "Altered oscillatory brain networks during emotional face processing in ADHD: an eLORETA and functional ICA study",
      "content": "arXiv:2512.13539v1 Announce Type: new \nAbstract: Attention-deficit/hyperactivity disorder (ADHD) is characterized by executive dysfunction and difficulties in processing emotional facial expressions, yet the large-scale neural dynamics underlying these impairments remain insufficiently understood. This study applied network-based EEG source analysis to examine oscillatory cortical activity during cognitive and emotional Go/NoGo tasks in individuals with ADHD. EEG data from 272 participants (ADHD n equals 102, controls n equals 170, age range 6 to 60 years) were analyzed using exact low-resolution brain electromagnetic tomography combined with functional independent component analysis, yielding ten frequency-resolved cortical networks. Mixed-effects ANCOVAs were conducted on independent component loadings with Group, Task, and Condition as factors and age and sex as covariates. ADHD participants showed statistically significant but small increases in activation across several networks, including a gamma-dominant inferior temporal component showing a Group effect and a Group by Condition interaction with stronger NoGo-related activation in ADHD. Two additional components showed similar but weaker NoGo-selective patterns. A main effect of Task emerged only for one temporal delta component, with higher activation during the VCPT than the ECPT. No Group by Task interactions were observed. Behavioral results replicated the established ADHD performance profile, with slower responses, greater variability, and higher error rates, particularly during the emotional ECPT. Overall, the findings reveal subtle alterations in oscillatory brain networks during inhibitory processing in ADHD, with modest effect sizes embedded within substantial within-group variability. These results support a dimensional view of ADHD neurobiology and highlight the limited discriminative power of network-level EEG markers.",
      "author": "Saghar Vosough (Division of Neuropsychology, Department of Psychology, University of Zurich, Zurich, Switzerland), Gian Candrian (Brain,Trauma Foundation Grisons, Chur, Switzerland), Johannes Kasper (Praxisgemeinschaft f\\\"ur Psychiatrie und Psychotherapie, Lucerne, Switzerland), Hossam Abdel Rehim (Psychiatrie und Psychotherapie Rapperswil, Rapperswil, Switzerland), Dominique Eich (Department of Psychiatry, Psychotherapy,,Psychosomatics, University of Zurich, Zurich, Switzerland), Andreas Mueller (Brain,Trauma Foundation Grisons, Chur, Switzerland), Lutz J\\\"ancke (Division of Neuropsychology, Department of Psychology, University of Zurich, Zurich, Switzerland)",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 249,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223699+00:00",
      "updated_at": "2025-12-16T05:24:50.223700+00:00"
    },
    {
      "id": "8e50b93f5d3fe9a17983e4c62cb40243",
      "url": "https://arxiv.org/abs/2512.13517",
      "title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments",
      "content": "arXiv:2512.13517v1 Announce Type: new \nAbstract: Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.",
      "author": "Raymond Khazoum, Daniela Fernandes, Aleksandr Krylov, Qin Li, Stephane Deny",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 209,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223664+00:00",
      "updated_at": "2025-12-16T05:24:50.223666+00:00"
    },
    {
      "id": "f5b72b5d2be57031de63d37b777df5a1",
      "url": "https://arxiv.org/abs/2512.13052",
      "title": "Macular: a multi-scale simulation platform for the retina and the primary visual system",
      "content": "arXiv:2512.13052v1 Announce Type: new \nAbstract: We developed Macular, a simulation platform with a graphical interface, designed to produce in silico experiment scenarios for the retina and the primary visual system. A scenario consists of generating a three-dimensional structure with interconnected layers, each layer corresponding to a type of 'cell' in the retina or visual cortex. The cells can correspond to neurons or more complex structures (such as cortical columns). The inputs are arbitrary videos. The user can use the cells and synapses provided with the software, or create their own using a graphical interface where they enter the constituent equations in text format (e.g., LaTeX). They also create the three-dimensional structure via the graphical interface. Macular then automatically generates and compiles the C++ code and generates the simulation interface. This allows the user to view the input video and the three-dimensional structure in layers. It also allows the user to select cells and synapses in each layer and view the activity of their state variables. Finally, the user can adjust the phenomenological parameters of the cells or synapses via the interface. We provide several example scenarios, corresponding to published articles, including an example of a retino-cortical model. Macular was designed for neurobiologists and modelers, specialists in the primary visual system, who want to test hypotheses in silico without the need for programming. By design, this tool allows natural or altered conditions (pharmacology, pathology, development) to be simulated.",
      "author": "Bruno Cessac, Erwan Demairy, J\\'er\\^ome Emonet, Evgenia Kartsaki, Thibaud Kloczko, C\\^ome Le Breton, Nicolas Niclausse, Selma Souihel, Jean-Luc Szpyrka, Julien Wintz",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 237,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223632+00:00",
      "updated_at": "2025-12-16T05:24:50.223634+00:00"
    },
    {
      "id": "d8964037b0dfd91f74fafab9f20b0165",
      "url": "https://arxiv.org/abs/2512.12802",
      "title": "A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness",
      "content": "arXiv:2512.12802v1 Announce Type: new \nAbstract: The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.",
      "author": "Erik Hoel",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 173,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223598+00:00",
      "updated_at": "2025-12-16T05:24:50.223599+00:00"
    },
    {
      "id": "e8b63fa1b61f89a284a81e11463f706e",
      "url": "https://arxiv.org/abs/2512.12767",
      "title": "Random matrix theory of sparse neuronal networks with heterogeneous timescales",
      "content": "arXiv:2512.12767v1 Announce Type: new \nAbstract: Training recurrent neuronal networks consisting of excitatory (E) and inhibitory (I) units with additive noise for working memory computation slows and diversifies inhibitory timescales, leading to improved task performance that is attributed to emergent marginally stable equilibria [PNAS 122 (2025) e2316745122]. Yet the link between trained network characteristics and their roles in shaping desirable dynamical landscapes remains unexplored. Here, we investigate the Jacobian matrices describing the dynamics near these equilibria and show that they are sparse, non-Hermitian rectangular-block matrices modified by heterogeneous synaptic decay timescales and activation-function gains. We specify a random matrix ensemble that faithfully captures the spectra of trained Jacobian matrices, arising from the inhibitory core - excitatory periphery network motif (pruned E weights, broadly distributed I weights) observed post-training. An analytic theory of this ensemble is developed using statistical field theory methods: a Hermitized resolvent representation of the spectral density processed with a supersymmetry-based treatment in the style of Fyodorov and Mirlin. In this manner, an analytic description of the spectral edge is obtained, relating statistical parameters of the Jacobians (sparsity, weight variances, E/I ratio, and the distributions of timescales and gains) to near-critical features of the equilibria essential for robust working memory computation.",
      "author": "Thiparat Chotibut, Oleg Evnin, Weerawit Horinouchi",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 202,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223568+00:00",
      "updated_at": "2025-12-16T05:24:50.223570+00:00"
    },
    {
      "id": "239e7c9d13bc95f030cb8d13d81aaf21",
      "url": "https://arxiv.org/abs/2512.12467",
      "title": "Reduced rank regression for neural communication: a tutorial for neuroscientists",
      "content": "arXiv:2512.12467v1 Announce Type: new \nAbstract: Reduced rank regression (RRR) is a statistical method for finding a low-dimensional linear mapping between a set of high-dimensional inputs and outputs. In recent years, RRR has found numerous applications in neuroscience, in particular for identifying \"communication subspaces\" governing the interactions between brain regions. This tutorial article seeks to provide an introduction to RRR and its mathematical foundations, with a particular emphasis on neural communication. We discuss RRR's relationship to alternate dimensionality reduction techniques such as singular value decomposition (SVD), principal components analysis (PCA), principal components regression (PCR), and canonical correlation analysis (CCA). We also derive important extensions to RRR, including ridge regularization and non-spherical noise. Finally, we introduce new metrics for quantifying communication strength as well as the alignment between communication axes and the principal modes of neural activity. By the end of this article, readers should have a clear understanding of RRR and the practical considerations involved in applying it to their own data.",
      "author": "Bichan Wu, Jonathan Pillow",
      "published_date": "2025-12-16T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 161,
      "reading_time": 1,
      "created_at": "2025-12-16T05:24:50.223528+00:00",
      "updated_at": "2025-12-16T05:24:50.223533+00:00"
    }
  ]
}