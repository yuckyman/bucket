{
  "last_updated": "2025-11-06T17:22:38.368198+00:00",
  "count": 20,
  "articles": [
    {
      "id": "80d1f642eb4da0958753076107ac0cbe",
      "url": "http://daniellakens.blogspot.com/2025/07/easily-download-files-from-open-science.html",
      "title": "Easily download files from the Open Science Framework with Papercheck",
      "content": "<p>Researchers\nincreasingly use the <a href=\"https://osf.io/\">Open Science Framework</a> (OSF) to share files, such as data\nand code underlying scientific publications, or presentations and materials for\nscientific workshops. The OSF is an amazing service that has contributed\nimmensely to a changed research culture where psychologists share data, code, and\nmaterials. We are very grateful it exists.</p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">But it is\nnot always the most user-friendly. Specifically, downloading files from the OSF\nis a bigger hassle than we (<a href=\"https://debruine.github.io/\">Lisa DeBruine</a>\nand <a href=\"https://www.tue.nl/en/research/researchers/daniel-lakens/\">Daniel\nLakens</a>, the developers of Papercheck) would like it to be. Downloading individual\nfiles is so complex, <a href=\"https://bsky.app/profile/malte.the100.ci/post/3lpf4s6spns2i\">Malte Elson</a>\nrecently posted this meme on Bluesky. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"NL\"><!--[if gte vml 1]><v:shapetype\n id=\"_x0000_t75\" coordsize=\"21600,21600\" o:spt=\"75\" o:preferrelative=\"t\"\n path=\"m@4@5l@4@11@9@11@9@5xe\" filled=\"f\" stroked=\"f\">\n <v:stroke joinstyle=\"miter\"/>\n <v:formulas>\n  <v:f eqn=\"if lineDrawn pixelLineWidth 0\"/>\n  <v:f eqn=\"sum @0 1 0\"/>\n  <v:f eqn=\"sum 0 0 @1\"/>\n  <v:f eqn=\"prod @2 1 2\"/>\n  <v:f eqn=\"prod @3 21600 pixelWidth\"/>\n  <v:f eqn=\"prod @3 21600 pixelHeight\"/>\n  <v:f eqn=\"sum @0 0 1\"/>\n  <v:f eqn=\"prod @6 1 2\"/>\n  <v:f eqn=\"prod @7 21600 pixelWidth\"/>\n  <v:f eqn=\"sum @8 21600 0\"/>\n  <v:f eqn=\"prod @7 21600 pixelHeight\"/>\n  <v:f eqn=\"sum @10 21600 0\"/>\n </v:formulas>\n <v:path o:extrusionok=\"f\" gradientshapeok=\"t\" o:connecttype=\"rect\"/>\n <o:lock v:ext=\"edit\" aspectratio=\"t\"/>\n</v:shapetype><v:shape id=\"_x0000_i1026\" type=\"#_x0000_t75\" style='width:453.75pt;\n height:276pt;visibility:visible;mso-wrap-style:square'>\n <v:imagedata src=\"file:///C:/Users/DLakens/AppData/Local/Temp/msohtmlclip1/01/clip_image001.jpg\"\n  o:title=\"\"/>\n</v:shape><![endif]--><!--[if !vml]--><img border=\"0\" height=\"368\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEinS5tFoL5qX14Z2OcgT1APMZLGlghAD3BfBhSnJ9pleVbJyRFUFLShqReS2j7SXGozmLMcVQkoM62UhneJDtH4yx3NRnXOkVZZi-Dm-OPwbGaidxr2raF9wzjx5fU92nfPpE3jmGfwZEwHS1WGSB4gUfRfV3XWjOC2-lCjOYR108h3fwORIHOnqxIX9m8\" width=\"605\" /><!--[endif]--></span><span lang=\"EN-US\"></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span>&nbsp;</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Not only is\nthe download button for files difficult to find, but downloading all files\nrelated to a project can be surprisingly effortful. It is possible to download\nall files in a zip folder that will be called \u2018osfstorage-archive.zip\u2019 when\ndownloaded. But as the OSF supports a nested folder structure, you might miss a\nfolder, and you will quickly end up with \u2018osfstorage-archive (1).zip\u2019, \u2018osfstorage-archive\n(2).zip\u2019, etc. Unzipping these archives creates a lot of files without the\norganized folder structure, in folders with meaningless names, making it\ndifficult to understand where files are. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<h2 style=\"text-align: left;\"><b><span lang=\"EN-US\">The\nosf_file_download function in Papercheck </span></b></h2>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">We have added\na new function to our R package \u2018<a href=\"https://scienceverse.github.io/papercheck/\">Papercheck</a>\u2019 that will download all files and\nfolders in an OSF repository. It saves all files by recreating the folder\nstructure from the OSF in your download folder. Just install Papercheck, load\nthe library, and use the osf_file_download function to grab all files on the OSF:</span></p><p class=\"MsoNormal\"><span lang=\"EN-US\"><br /></span></p>\n\n<div style=\"text-align: left;\"><span style=\"font-family: courier;\"><b><span lang=\"EN-US\">devtools::install_github(\"scienceverse/papercheck\")<br /></span></b><b><span lang=\"EN-US\">library(papercheck)<br /></span></b><b><span lang=\"EN-US\">osf_file_download(\"6nt4v\")</span></b></span></div>\n\n\n\n\n\n<p class=\"MsoNormal\"><b><span lang=\"EN-US\">&nbsp;</span></b></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">All files\nwill be downloaded to your working directory. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Are you feeling\nFOMO for missing out on the <a href=\"https://www.kcl.ac.uk/events/open-research-summer-school-2025\">King Open\nResearch Summer School</a> that is going on these days, where <a href=\"https://research.tue.nl/en/persons/sajedeh-rasti\">Sajedeh Rasti</a> talked\nabout Preregistration, and <a href=\"https://research.tue.nl/en/persons/cristian-mesquida-caldentey\">Cristian\nMesquida</a> will give a workshop on using Papercheck? Well, at least it is\nvery easy to download all the files they have shared on the OSF and look at the\npresentations: </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"b7es8\")</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">In the\noutput, we see that by default large files (more than 10mb) are omitted. <span><!--[if gte vml 1]><v:shape id=\"Picture_x0020_1\"\n o:spid=\"_x0000_i1025\" type=\"#_x0000_t75\" alt=\"A screenshot of a computer&#10;&#10;AI-generated content may be incorrect.\"\n style='width:453.75pt;height:292.5pt;visibility:visible;mso-wrap-style:square'>\n <v:imagedata src=\"file:///C:/Users/DLakens/AppData/Local/Temp/msohtmlclip1/01/clip_image003.png\"\n  o:title=\"A screenshot of a computer&#10;&#10;AI-generated content may be incorrect\"/>\n</v:shape><![endif]--><!--[if !vml]--><img alt=\"A screenshot of a computer\n\nAI-generated content may be incorrect.\" border=\"0\" height=\"390\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEj2jH76ywmEeLzL43u6gJl7GKR2lEGlhYS0LOXRPMMovOsJEVdXyamYCvmq96ez3p_GXHLoFz4JDyAKHlARK9pSy8QfzVa7yt1_uEg_H9IJfKgunnYWUwlROXABNcU6TvrA0_hx0KPZfj00b3Cb-Wn_7Bf3sFu9JApZoClcWoh_Vfl5bk1GQVZUH1tuGao\" width=\"605\" /><!--[endif]--></span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">If you want\nto download all files, regardless of the size, then set the parameter to ignore\nthe maximum file size: </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"b7es8\",\nmax_file_size = NULL)</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Sometimes\nyou might want to download all files but ignore the file structure on the OSF,\nto just have all the files in one folder. Setting the parameter ignore_folder_structure\n= TRUE will give you all the files on the OSF in a single folder. By default, files will be downloaded into your working directory, but you can also specify\nwhere you want the files to be saved. </span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\"><span style=\"font-family: courier;\">osf_file_download(\"6nt4v\",\nignore_folder_structure = TRUE, download_to = \"C:\\\\test_download\")</span></span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">We hope\nthis function will make it easier for reviewers to access all supplementary\nfiles stored on the OSF during peer review, and for researchers who want to re-use\ndata, code, or materials shared on the OSF by downloading all the files they\nneed easily. Make sure to install the latest version of Papercheck (0.0.0.9050)\nto get access to this new function. Papercheck is still in active development,\nso report any bugs on <a href=\"https://github.com/scienceverse/papercheck\">GitHub</a>.</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>",
      "author": "noreply@blogger.com (Daniel Lakens)",
      "published_date": "2025-07-22T09:53:00+00:00",
      "source": "Twenty Percent Statistician",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 765,
      "reading_time": 3,
      "created_at": "2025-11-06T17:22:32.277168+00:00",
      "updated_at": "2025-11-06T17:22:32.277169+00:00"
    },
    {
      "id": "5cf06dd1c8477abb17ef4e5c3b5426e0",
      "url": "https://erpinfo.org/blog/2021/12/22/applications-2023",
      "title": "Applications now being accepted for UC-Davis/SDSU ERP Boot Camp, July 31 \u2013 August 9, 2023",
      "content": "<p class=\"\">The next 10-day ERP Boot Camp will be held July 31 \u2013 August 9, 2023 in San Diego, California. We are now taking applications, which will be due by April 1, 2023. <a href=\"https://erpinfo.org/summer-boot-camp\">Click here</a> for more information.</p><p class=\"\">We are currently planning to hold this workshop as an in-person event. However, these plans are subject to change as the COVID-19 pandemic evolves. If the event is held in person, we will require that everyone is fully vaccinated, and we will also implement any other safety measures that are warranted at the time of the workshop.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"980\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/1609175691205-RTD3XM69YGOFMVP23U6T/Boot_Camp_Logo.png?format=1000w\" width=\"1148\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>",
      "author": "Steve Luck",
      "published_date": "2023-01-16T18:31:57+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 108,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:29.764293+00:00",
      "updated_at": "2025-11-06T17:22:29.764295+00:00"
    },
    {
      "id": "bd7398ecbbd90ecd3269866b2fd3744f",
      "url": "https://erpinfo.org/blog/2023/6/23/decoding-webinar",
      "title": "ERP Decoding for Everyone: Software and Webinar",
      "content": "<p class=\"\"><strong>You can access the recording </strong><a href=\"https://video.ucdavis.edu/media/Virtual+ERP+Boot+CampA+Decoding+for+Everyone%2C+July+25+2023/1_lmwj6bu0\"><strong>here</strong></a><strong>.<br />You can access the final PDF of the slides </strong><a href=\"https://ucdavis.box.com/s/flf9gzeo12rz2jhxptih7xjl0omka2k7\"><strong>here</strong></a><strong>. <br />You can access the data </strong><a href=\"https://doi.org/10.18115/D5KS6S\"><strong>here</strong></a><strong>.</strong></p><p class=\"\">fMRI research has used decoding methods for over 20 years. These methods make it possible to decode what an individual is perceiving or holding in working memory on the basis of the pattern of BOLD activity across voxels. Remarkably, these methods can also be applied to ERP data, using the pattern of voltage across electrode sites rather than the pattern of activity across voxels to decode the information being represented by the brain (<a href=\"https://erpinfo.org/blog/2018/9/16/decoding\">see this previous blog post</a>). For example, ERPs can be used to decode the identity of a face that is being perceived, the emotional valence of a scene, the identity and semantic category of a word, and the features of an object that is being maintained in working memory. Moreover, decoding methods can be more sensitive than traditional methods for detecting conventional ERP effects (e.g., whether a word is semantically related or unrelated to a previous word in an N400 paradigm).</p><p class=\"\">So far, these methods have mainly been used by a small set of experts. We aim to change that with the upcoming Version 10 of <a href=\"https://erpinfo.org/erplab\">ERPLAB Toolbox</a>. This version of ERPLAB will contain an ERP decoding tool that makes it trivially easy for anyone who knows how to do conventional ERP processing to take advantage of the power of decoding. It should be available in mid-July at <a href=\"https://github.com/ucdavis/erplab/releases\">our GitHub site</a>. You can join the <a href=\"https://github.com/ucdavis/erplab/wiki/ERPLAB-email-list\">ERPLAB email list</a> to receive an announcement when this version is released. Please do not contact us with questions until it has been released and you have tried using it.</p><p class=\"\">On July 25, 2023, we will hold a 2-hour Zoom webinar to explain how decoding works at a conceptual level and show how to implement in ERPLAB Toolbox. The webinar will begin at 9:00 AM Pacific Time (California), 12:00 PM Eastern Time (New York), 5:00 PM British Summer Time (London), 6:00 PM Central European Summer Time (Berlin). </p><p class=\"\">The webinar is co-sponsored by the <a href=\"https://erpinfo.org/the-erp-boot-camp\">ERP Boot Camp</a> and the <a href=\"https://sprweb.org\">Society for Psychophysiological Research</a>. It is completely free, but you must register in advance at <a href=\"https://ucdavis.zoom.us/meeting/register/tJUrc-CtpzorEtBSmZXJINOlLJB9ZR0evpr4\">https://ucdavis.zoom.us/meeting/register/tJUrc-CtpzorEtBSmZXJINOlLJB9ZR0evpr4</a>. Once you register, you will receive an email with your own individual Zoom link. </p><p class=\"\">We will make a recording available a few days after the webinar on the <a href=\"https://erpinfo.org\">ERPinfo.org</a> web site.</p><p class=\"\">Please direct any questions about the webinar to <a href=\"mailto:erpbootcamp@gmail.com\">erpbootcamp@gmail.com</a>.</p>",
      "author": "Steve Luck",
      "published_date": "2023-06-23T21:05:26+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 420,
      "reading_time": 2,
      "created_at": "2025-11-06T17:22:29.764249+00:00",
      "updated_at": "2025-11-06T17:22:29.764250+00:00"
    },
    {
      "id": "3d370217cb4a351bb54e7854066e15c3",
      "url": "https://erpinfo.org/blog/2024/2/4/optimal-filters",
      "title": "New Papers: Optimal Filter Settings for ERP Research",
      "content": "<p class=\"\">Zhang, G., Garrett, D. R., &amp; Luck, S. J. (in press). Optimal filters for ERP research I: A general approach for selecting filter settings. <em>Psychophysiology</em>. <a href=\"https://doi.org/10.1111/psyp.14531\"><span>https://doi.org/10.1111/psyp.14531</span></a> [<a href=\"https://www.researchgate.net/publication/377773270_Optimal_filters_for_ERP_research_I_A_general_approach_for_selecting_filter_settings\"><span>preprint</span></a>]</p><p class=\"\">Zhang, G., Garrett, D. R., &amp; Luck, S. J. (in press). Optimal filters for ERP research II: Recommended settings for seven common ERP components. <em>Psychophysiology</em>. <a href=\"https://doi.org/10.1111/psyp.14530\"><span>https://doi.org/10.1111/psyp.14530</span></a> [<a href=\"https://www.researchgate.net/publication/377766656_Optimal_filters_for_ERP_research_II_Recommended_settings_for_seven_common_ERP_components\"><span>preprint</span></a>]</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n  \n    \n\n      \n\n      \n        <figure class=\"\n              sqs-block-image-figure\n              intrinsic\n            \">\n          \n        \n        \n\n        \n          \n            \n          \n            \n                \n                \n                \n                \n                \n                \n                \n                <img alt=\"\" height=\"1490\" src=\"https://images.squarespace-cdn.com/content/v1/5abefa62d274cb16de90e935/d64086cc-e3b4-457d-89df-08d9b3f96439/Filter_Table.png?format=1000w\" width=\"2062\" />\n\n            \n          \n        \n          \n        \n\n        \n      \n        </figure>\n      \n\n    \n  \n\n\n  \n\n\n\n\n\n  <p class=\"\">What filter settings should you apply to your ERP data? If your filters are too weak to attenuate the noise in your data, your effects may not be statistically significant. If your filters are too strong, they may create artifactual peaks that lead you to draw bogus conclusions.</p><p class=\"\">For years, I have been recommending a bandpass of 0.1\u201330 Hz for most cognitive and affective research in neurotypical young adults. In this kind of research, I have found that filtering from 0.1\u201330 Hz usually does a good job of minimizing noise while creating minimal waveform distortion. </p><p class=\"\">However, this recommendation was based on a combination of informal observations from many experimental paradigms and a careful examination of a couple paradigms, so it was a bit hand-wavy. In addition, the optimal filter settings will depend on the waveshape of the ERP effects and the nature of the noise in a given study, so I couldn\u2019t make any specific recommendations about other experimental paradigms and participant populations. Moreover, different filter settings may be optimal for different scoring methods (e.g., mean amplitude vs. peak amplitude vs. peak latency).</p><p class=\"\">Guanghui Zhang, David Garrett, and I spent the last year focusing on this issue. First we developed a general method that can be used to determine the optimal filter settings for a given dataset and scoring method (see <a href=\"https://doi.org/10.1111/psyp.14531\"><span>this paper</span></a>). Then we applied this method to the <a href=\"https://doi.org/10.1016/j.neuroimage.2020.117465\"><span>ERP CORE</span></a> data to determine the optimal filter settings for the N170, MMN, P3b, N400, N2pc, LRP, and ERN components in neurotypical young adults (see <a href=\"https://doi.org/10.1111/psyp.14530\"><span>this paper</span></a> and the table above).</p><p class=\"\">If you are doing research with these components (or similar components) in neurotypical young adults, you can simply use the filter settings that we identified. If you are using a very different paradigm or testing a very different subject population, you can apply our method to your own data to find the optimal settings. We added some new tools to <a href=\"https://github.com/ucdavis/erplab\"><span>ERPLAB Toolbox</span></a> to make this easier.</p><p class=\"\">One thing that we discovered was that our old recommendation of 0.1\u201330 Hz does a good job of avoiding filter artifacts but is overly conservative for some components. For example, we can raise the low end to 0.5 Hz when measuring N2pc and MMN amplitudes, which gets rid of more noise without producing problematic waveform distortions. And we can go all the way up to 0.9 Hz for the N170 component. However, later/slower components like P3b and N400 require lower cutoffs (no higher than 0.2 Hz).</p><p class=\"\">You might be wondering how we defined the \u201coptimal\u201d filter settings. At one level, the answer is simple: The optimal filter is the one that maximizes the signal-to-noise ratio without producing too much waveform distortion. The complexities arise in quantifying the signal-to-noise ratio, quantifying the waveform distortion, and deciding how much waveform distortion is \u201ctoo much\u201d. We believe we have found reasonably straightforward and practical solutions to these problems, which you can read about in the published papers.</p>",
      "author": "Steve Luck",
      "published_date": "2024-02-04T23:46:20+00:00",
      "source": "Erp Boot Camp",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 568,
      "reading_time": 2,
      "created_at": "2025-11-06T17:22:29.764199+00:00",
      "updated_at": "2025-11-06T17:22:29.764201+00:00"
    },
    {
      "id": "b3cd4fc4257e4deef4e24f2c3cdd8b67",
      "url": "https://brain.ieee.org/publications/neuroethics-framework/education/education-social-and-cultural-issues/education-social-and-cultural-issues/",
      "title": "Education: Social and Cultural Issues",
      "content": "Devices that therapeutically aid users with cognitive and learning disabilities/differences should not be equally applied to a general population seeking learning advantages. It must not be assumed that therapies able to improve cognition for mental and cognitive disorders (such as executive control and working memory) would work similarly on nondisabled people linearly to improve their cognition above standard levels. Although ...",
      "author": "Adriel Carridice",
      "published_date": "2025-02-05T15:45:23+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:27.102413+00:00",
      "updated_at": "2025-11-06T17:22:27.102414+00:00"
    },
    {
      "id": "1348eff8059203eea8f1cf0c47dfeafd",
      "url": "https://brain.ieee.org/podcasts/qa-with-dr-richard-carson-professor-of-biomedical-engineering-and-radiology-biomedical-imaging-yale-university-and-yale-school-of-medicine/",
      "title": "Q&A with Dr. Richard Carson, Professor of Biomedical Engineering and Radiology & Biomedical Imaging, Yale University and Yale School of Medicine",
      "content": "",
      "author": "Adriel Carridice",
      "published_date": "2025-11-04T18:24:59+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:27.102178+00:00",
      "updated_at": "2025-11-06T17:22:27.102182+00:00"
    },
    {
      "id": "21edf5ffd9853f7b13856eb1a1b27eef",
      "url": "http://ieeexplore.ieee.org/document/10976660",
      "title": "Inverse Problem Approach to Aberration Correction for In Vivo Transcranial Imaging Based on a Sparse Representation of Contrast-Enhanced Ultrasound Data",
      "content": "Objective: Transcranial ultrasound imaging is currently limited by attenuation and aberration induced by the skull. First used in contrast-enhanced ultrasound (CEUS), highly echoic microbubbles allowed for the development of novel imaging modalities such as ultrasound localization microscopy (ULM). Herein, we develop an inverse problem approach to aberration correction (IPAC) that leverages the sparsity of microbubble signals. Methods: We propose to use the a priori knowledge of the medium based upon microbubble localization and wave propagation to build a forward model to link the measured signals directly to the aberration function. A standard least-squares inversion is then used to retrieve the aberration function. We first validated IPAC on simulated data of a vascular network using plane wave as well as divergent wave emissions. We then evaluated the reproducibility of IPAC in vivo in 5 mouse brains. Results: We showed that aberration correction improved the contrast of CEUS images by 4.6 dB. For ULM images, IPAC yielded sharper vessels, reduced vessel duplications, and improved the resolution from 21.1 $\\mu$m to 18.3 $\\mu$m. Aberration correction also improved hemodynamic quantification for velocity magnitude and flow direction. Conclusion: We showed that IPAC can perform skull-induced aberration correction and improved power Doppler as well as ULM images acquired on the mouse brain. Significance: This technique is promising for more reliable transcranial imaging of the brain vasculature with potential non-invasive clinical applications.",
      "author": "",
      "published_date": "2025-04-25T13:17:31+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 225,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:22.844504+00:00",
      "updated_at": "2025-11-06T17:22:22.844505+00:00"
    },
    {
      "id": "ea005bf9a4d1debaee7a63b57743133e",
      "url": "http://ieeexplore.ieee.org/document/10974669",
      "title": "CLaI: Collaborative Learning and Inference for Low-Resolution Physiological Signals: Validation in Clinical Event Detection and Prediction",
      "content": "While machine learning (ML) techniques have been applied to detection and prediction tasks in clinical data, most methods rely on high-resolution data, which is not routinely available in most Intensive Care Units (ICUs), and perform poorly when faced with class imbalance. Here, we introduce and validate Collaborative Learning and Inference (CLaI) for detection and prediction of events from learned latent representations of multivariate physiological time series, leveraging similarities across patients. Our method offers a new way to detect and predict events using low-resolution physiological time series. We evaluate its performance on predicting intracranial hypertension and sepsis using the KidsBrainIT (minute-by-minute resolution) and MIMIC-IV (hourly resolution) datasets, respectively, comparing our approach with classification-based and sequence-to-sequence benchmarks from existing studies. Additional experiments on sepsis detection, robustness to class imbalance, and generalizability\u2014demonstrated via seizure detection using the CHB-MIT scalp electroencephalogram dataset\u2014confirm that CLaI effectively handles class imbalance, consistently achieving competitive performance and the highest F1 score. Overall, our approach introduces a novel method for analyzing routinely collected ICU physiological time series by leveraging patient similarity thus enabling ML interpretability through case-based reasoning.",
      "author": "",
      "published_date": "2025-04-23T13:18:26+00:00",
      "source": "Transactions Biomedical Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 179,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:22.844470+00:00",
      "updated_at": "2025-11-06T17:22:22.844471+00:00"
    },
    {
      "id": "a47557cc5e826f075324e7184c294afd",
      "url": "http://ieeexplore.ieee.org/document/11153351",
      "title": "Perceptual Decoupling Underlies Internal Shielding Benefit during Switches between External and Internal Attention: Evidence from Early Sensory Event-related Potential Components",
      "content": "People need to often switch attention between external and internal sources of information, that is, external and internal attention, respectively. There has been a recent surge of research interest in this type of attentional flexibility, which has revealed that it is characterized by an asymmetrical cost, being larger for switching toward internal than external attention. This cost asymmetry has been explained in terms of an internal shielding benefit, that is, the maintenance of stable internal attention against external interference. Although it is currently unclear how internal information might be shielded from external input during switches, a likely candidate is perceptual decoupling. In this study, we instructed participants to repeat external or internal attention, or to switch between them from trial to trial, while simultaneously recording 64-channel EEG. At the behavioral level, we replicated the switch cost asymmetry. Our ERP analysis provided evidence for three different processing stages. First, participants prepared more strongly for an upcoming internal than external attentional selection, as reflected in the increased contingent negative variation component. Second, during internal trials, participants moreover showed a blunted sensory response, most notable in the P1 and N1 components, reflecting perceptual decoupling. Finally, we found an increased P2 component when switching toward internal attention compared with repeating it, indicating more stable perceptual decoupling on internal repetition trials, in line with an internal shielding benefit. We integrate these findings here with behavioral accounts of the cost asymmetry and conclude that perceptual decoupling provides a potential mechanism for the internal shielding benefit of attention.",
      "author": "",
      "published_date": "2025-09-08T13:16:44+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 251,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:20.427745+00:00",
      "updated_at": "2025-11-06T17:22:20.427747+00:00"
    },
    {
      "id": "e0899403e0a8c6fb0b591f26e5e5d1b5",
      "url": "http://ieeexplore.ieee.org/document/11153358",
      "title": "Lexical and Information Structure Functions of Prosody and Their Relevance for Spoken Communication: Evidence from Psychometric and Electroencephalographic Data",
      "content": "Prosody not only distinguishes \u201clexical\u201d meaning but also plays a key role in information packaging by highlighting the most relevant constituent of the discourse, namely, \u201cfocus\u201d information. The present study investigated the role of lexical and focus functions of prosody in the coherent interpretation of linguistic input. To this end, we manipulated the correctness of prosodic markers in the context and scrutinized how listeners evaluate these violations\u2014whether they result in lexical or focus anomalies\u2014using psychometric and EEG measures. Psychometric data from 40 participants indicated that prosodic violations were judged as incorrect by the listeners both at the lexical and focus levels, with focus level violations leading to lower correctness scores than lexical level violations, and combined violations receiving the lowest scores. EEG data from 20 participants documented a strong N400 effect (350\u2013550 msec) in response to combined violations, and a late posterior negativity (600\u2013900 msec) present only for combined violations and focus-level violations. Consistent with the psychometric data, the EEG data suggest that prosodic violations at the focus level result in higher costs for comprehension than prosodic violations at the lexical level, whereas combined prosodic violations most significantly disrupt the interpretation. Taken together, these findings suggest that the language comprehension system is sensitive to accurate representations of both lexical and information structure prosody, and benefits from the interaction between them; however, they are weighted differently based on their relevance for a functioning spoken communication.",
      "author": "",
      "published_date": "2025-09-08T13:16:44+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:20.427702+00:00",
      "updated_at": "2025-11-06T17:22:20.427704+00:00"
    },
    {
      "id": "7c4031974efdff1e7f6c52ab82a8e379",
      "url": "http://ieeexplore.ieee.org/document/11153363",
      "title": "Musical Structure Influences the Perception of Sound Location",
      "content": "The perception of multilayered auditory stimuli, such as music or speech, relies on the integration of progressively more complex and abstract features as they are processed along the auditory pathway. To investigate whether higher-level musical structure modulates auditory perception or merely the interpretation of perceived information, we examined the interaction between sound location\u2014a low-level feature\u2014and musical phrases, which are structures spanning across seconds and require temporal integration of information within continuous stimuli. This was to observe whether musical phrase boundaries modulate pre-attentive and explicit sensitivity to the location changes. Participants listened to melodies with randomized location changes and either actively reported detection of change or passively listened while EEG data were collected. Analysis of mismatch negativity responses revealed significantly larger amplitudes for location changes occurring at phrase boundaries, suggesting that musical grouping enhances the perceptual salience of these changes, conveyed by physically identical cues. Behaviorally, participants showed no difference in sensitivity but were more likely to report location changes at phrase boundaries, even when no change occurred. These findings demonstrate that higher-level musical structure modulates pre-attentive auditory processing and influences perception of spatial location. This effect appears to rely on fundamental auditory mechanisms rather than musical expertise, highlighting the dynamic interaction between abstract musical structure and low-level sensory processing.",
      "author": "",
      "published_date": "2025-09-08T13:16:44+00:00",
      "source": "Cognitive Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 209,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:20.427651+00:00",
      "updated_at": "2025-11-06T17:22:20.427656+00:00"
    },
    {
      "id": "531780bd9b4422343606f0da4ea82642",
      "url": "https://www.sciencedirect.com/science/article/pii/S0006899325005633?dgcid=rss_sd_all",
      "title": "Apprehending relational events: The visual world paradigm and the interplay of event perception and language",
      "content": "<p>Publication date: 15 December 2025</p><p><b>Source:</b> Brain Research, Volume 1869</p><p>Author(s): Alon Hafri, John C. Trueswell</p>",
      "author": "",
      "published_date": null,
      "source": "Brain Research",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 14,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:09.951873+00:00",
      "updated_at": "2025-11-06T17:22:09.951875+00:00"
    },
    {
      "id": "abb4e51f2f0960c87f5936f785bcd344",
      "url": "https://www.sciencedirect.com/science/article/pii/S030645222500987X?dgcid=rss_sd_all",
      "title": "The psychoactive compound ibogaine sex-dependently alters the firing rate and afterhyperpolarization of I<sub>h</sub>-negative neurons in the mouse ventral tegmental area",
      "content": "<p>Publication date: 28 November 2025</p><p><b>Source:</b> Neuroscience, Volume 589</p><p>Author(s): Jannik Nicklas Eliasen, Amir Rezagholizadeh, Helene P\u00e5b\u00f8l Jacobsen, Uffe Kristiansen, Kristi A. Kohlmeier</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroscience Journal",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 21,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:08.787568+00:00",
      "updated_at": "2025-11-06T17:22:08.787569+00:00"
    },
    {
      "id": "b8981a050010d9c517cdd6c146e88784",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005567?dgcid=rss_sd_all",
      "title": "Predictive modeling of TMS-evoked responses: Unraveling instantaneous excitability states",
      "content": "<p>Publication date: 15 November 2025</p><p><b>Source:</b> NeuroImage, Volume 322</p><p>Author(s): Oskari Ahola, Lisa Haxel, Maria Ermolova, Dania Humaidan, Tuomas P. Mutanen, Mikael Laine, Matilda Makkonen, Elena Ukharova, Timo Roine, Pantelis Lioumis, Roberto Guidotti, Risto J. llmoniemi, Ulf Ziemann</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 36,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:06.511597+00:00",
      "updated_at": "2025-11-06T17:22:06.511599+00:00"
    },
    {
      "id": "5f2feef4ea2382f4ed7b096aedbe2db3",
      "url": "https://www.sciencedirect.com/science/article/pii/S1053811925005464?dgcid=rss_sd_all",
      "title": "The golden age of online readout: EEG-informed TMS from manual probing to closed-loop neuromodulation",
      "content": "<p>Publication date: 15 November 2025</p><p><b>Source:</b> NeuroImage, Volume 322</p><p>Author(s): Giuseppe Varone, Mana Biabani, Sara Tremblay, Joshua C. Brown, Elisa Kallioniemi, Nigel C. Rogasch</p>",
      "author": "",
      "published_date": null,
      "source": "Neuroimage",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 22,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:06.511470+00:00",
      "updated_at": "2025-11-06T17:22:06.511472+00:00"
    },
    {
      "id": "533c6bdf66ef39c987fc87a5bbc0b09d",
      "url": "https://www.biorxiv.org/content/10.1101/2025.11.04.686589v1?rss=1",
      "title": "Neural Dynamics Underlying Repeated Learning of Visual Image Sequences",
      "content": "Humans possess a remarkable ability to recognize visual objects with high fidelity, supported by complex neural mechanisms underlying memory retrieval. Event-related potential (ERP) studies have identified two key neural signatures of recognition memory: the parietal old/new effect and the frontal old/new effect. Despite extensive research on these ERP components, the extent to which these components reflect distinct memory processes remains debated. In the present study, we investigated how repetitive learning modulates these ERP components. Participants repeatedly studied a fixed list of 32 real-world images across up to five study-test repetitions while EEG was recorded. Additionally, a separate set size 1 condition served as a proxy for working memory. Our results showed that with increased repetitions, the parietal old/new effect exhibited enhanced amplitude and earlier peak latency, reflecting more efficient retrieval of well-learned memories. In contrast, the frontal old/new effect remained unchanged in both amplitude and timing. These findings suggest that the parietal old/new effect is a sensitive neural marker of learning-related changes in long-term memory representations, while the frontal effect is less influenced by repetition. Additionally, despite similarly high accuracy between the well-practiced set size 32 condition and the set size 1 working memory condition, both parietal and frontal old/new effects peaked significantly earlier for set size 1, suggesting that access to working memory is substantially faster than even well-practiced long-term memory. Together, our results highlight the unique role of the parietal old/new effect, but not the frontal old/new effect, in repetitive learning, despite both components being important for successful recognition of learnt visual stimuli.",
      "author": "Zhao, C., Kim, A., Campos, L., Vogel, E. K.",
      "published_date": "2025-11-05T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 255,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:05.348430+00:00",
      "updated_at": "2025-11-06T17:22:05.348431+00:00"
    },
    {
      "id": "886a83f0dd0f814590ca375b6b0c7e9c",
      "url": "https://www.biorxiv.org/content/10.1101/2025.11.04.686568v1?rss=1",
      "title": "Linking Electrophysiological Metrics to Oxidative Metabolism: Implications for EEG-fMRI Association",
      "content": "Resting-state functional magnetic resonance imaging (rs-fMRI) is widely used to study brain function, yet its biophysiological basis remains incompletely understood. Building on our recent work, we investigated how EEG activity and cerebral metabolic rate of oxygen (CMRO2) are related to one another, and how they jointly underpin rs-fMRI metrics. Using a multimodal dataset with macrovascular correction applied to all rs-fMRI metrics, we first examined associations between EEG metrics and CMRO2, then applied mediation analysis to evaluate how CMRO2 mediates EEG-fMRI associations. We found that bandlimited EEG theta and alphafractional power was significantly associated with CMRO2. Bandlimited EEG coherence was also associated with CMRO2 across all the bands. Bandlimited EEG fractional power and coherence were also significantly associated with cerebral blood flow (CBF) and oxygen extraction fraction (OEF) in a manner that varied by frequency. EEG broadband temporal complexity was positively associated with CMRO2; and EEG coherence was negatively associated with OEF. Notably, there are pronounced sex differences in these relationships, which suggests that the biophysical underpinnings of rs-fMRI are sex dependent. Moreover, the baseline metabolic and hemodynamic variables did partially mediate EEG-fMRI associations, with CMRO2 serving as the primary mediator. However, most of the mediations are partial, highlighting the complex interplay among electrophysiological activity, oxidative metabolism, and hemodynamics. This study advances our understanding of the biophysical basis of rs-fMRI and provides a foundation for developing sex-specific diagnostic and therapeutic strategies for neurological disorders.",
      "author": "Zhong, X., Van Lankveld, H., Mathew, A., Chen, J. J.",
      "published_date": "2025-11-05T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 233,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:05.348391+00:00",
      "updated_at": "2025-11-06T17:22:05.348393+00:00"
    },
    {
      "id": "1820f7bfc6c35032aacd3917189e3e1d",
      "url": "https://www.biorxiv.org/content/10.1101/2025.11.04.686314v1?rss=1",
      "title": "Feedforward representation of face information in the fusiform face area revealed by VASO 7T layer fMRI",
      "content": "Recognizing faces is a critical cognitive process in social interaction. Face recognition has been extensively studied with functional magnetic resonance imaging (fMRI), ranging from large coverage studies comprehensively examining the entire face pathway to high-resolution laminar-specific (layer) fMRI studies probing feedforward and feedback signals in early visual and face-selective areas during particular face processing. However, it is still unclear which brain region structures face information in the face pathway. To further elucidate this mechanism, we investigated fusiform face area (FFA) during passive face image presentation using a whole-brain vascular space occupancy (VASO) 7T layer fMRI open dataset combined with multi-voxel pattern analysis (MVPA). We analysed lateral occipital cortex (LOC) as a control region and BOLD data in the dataset as a control contrast. We trained multiple binary classification decoders with various image categories (e.g., face vs. others, house vs. others) and three cortical layer groups independently to assess information representation across cortical depths. Our results revealed that decoding accuracies in VASO data peaked in the middle layer of FFA, suggesting feedforward signature specifically during face processing. This effect was not observed in LOC or in decoders for other image categories. These findings indicate that face information is structured prior to processing in the FFA, consistent with previous reports suggesting that face-related features are partially extracted before reaching higher-level face areas. Overall, this study highlights the potential of combining high-specificity VASO layer-fMRI with high-sensitivity MVPA to dissect cortical information flow.",
      "author": "Koiso, K., Akamatsu, K., Huber, R., Miyawaki, Y.",
      "published_date": "2025-11-05T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 239,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:05.348353+00:00",
      "updated_at": "2025-11-06T17:22:05.348355+00:00"
    },
    {
      "id": "77ae94689368becb75bd88a519915a7a",
      "url": "https://www.biorxiv.org/content/10.1101/2025.11.04.686499v1?rss=1",
      "title": "Limited Effects of Isolated Congenital Anosmia on Cerebral White Matter Morphology",
      "content": "Lack of sensory input is associated with alterations in brain morphology; mainly in or near cerebral regions normally devoted to processing of the missing sense. We have in multiple studies demonstrated that the only consistent morphological finding within the gray matter of individuals born without the sense of smell (isolated congenital anosmia; ICA), are changes in or near the olfactory sulcus. For the connecting tissue of the brain, the white matter (WM), previous studies have yielded inconsistent findings. Here, we show that individuals with ICA (n=49) exhibit alterations in WM volume as compared to age- and sex-matched controls. Consistent evidence from both voxel-based morphometry and multi-voxel pattern analysis shows that individuals with ICA show decreased WM in areas surrounding the olfactory sulcus. Importantly, no WM alterations were found in areas surrounding the olfactory (piriform) cortex. In contrast to congenital sensory loss in other systems, we show that morphological alterations due to lifelong olfactory deprivation are limited. Alterations are primarily localized around the olfactory sulcus and likely due to the absence of olfactory bulbs. A possible explanation for the lack of major morphological alterations in individuals with congenital anosmia is that the olfactory regions may be recruited for non-olfactory functions.",
      "author": "Winter, A. L., Peter, M., Thunell, E., Lundstrom, J. N., Darki, F.",
      "published_date": "2025-11-05T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 199,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:05.348311+00:00",
      "updated_at": "2025-11-06T17:22:05.348314+00:00"
    },
    {
      "id": "99d7324cb74abb76e8b2b5900edd66e6",
      "url": "https://www.biorxiv.org/content/10.1101/2025.11.04.686617v1?rss=1",
      "title": "Biased inter-columnar communication and short-term plasticity in mouse barrel cortex",
      "content": "The barrel cortex (BC) processes input from whiskers to probe and analyze objects in an environment. This sensory input exhibits a complex phase, direction, and frequency-dependent structure arising from whisking kinematics. Little is known about how BC microcircuits process this information. In particular, it remains unclear how the BC extracts relevant spatiotemporal features by integrating input from multiple whiskers. To investigate communication within and between cortical barrels, we targeted a hybrid voltage sensor (hVOS) to Scnn1a excitatory neurons in BC layer 4 (L4) of male and female mice (mean age 7.8 weeks), and imaged population responses to electrical stimulation. Coronal and sagittal slices presented the laminar structure with barrels aligned along stereotyped whisking directions. Voltage imaging tracked activity along an L4[-&gt;]L2/3[-&gt;]L4 relay during inter-barrel communication. AMPA receptor blockade demonstrated that this relay depends on excitatory synaptic transmission and revealed intra- and inter-barrel feedforward inhibition. Single-pulse responses were isotropic in amplitude, conduction velocity, and half-width, but latency was longer for communication with dorsal and caudal barrels. Furthermore, paired-pulse depression was weakest and recovery slowest for protraction-related directions, especially between caudally adjacent barrels, suggesting preferential enhancement of repetitive inputs in this direction. These results identify direction-dependent synaptic circuitry in the shaping of inter-barrel communication. Anisotropy in short-term plasticity aligns with whisker motion kinematics, suggesting that BC microcircuits are tuned to preserve temporal fidelity and selectively filter inputs according to whisking phase and direction.",
      "author": "Judge, J. M., Jackson, M. B.",
      "published_date": "2025-11-06T00:00:00+00:00",
      "source": "Biorxiv Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 231,
      "reading_time": 1,
      "created_at": "2025-11-06T17:22:05.348161+00:00",
      "updated_at": "2025-11-06T17:22:05.348165+00:00"
    }
  ]
}