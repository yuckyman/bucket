{
  "last_updated": "2026-01-22T05:56:11.891537+00:00",
  "count": 20,
  "articles": [
    {
      "id": "8a37f4aa5925a5559bd2dd315d94013b",
      "url": "https://brain.ieee.org/publications/neuroethics-framework/education/standards-education/education-standards/",
      "title": "Education: Standards",
      "content": "",
      "author": "Adriel Carridice",
      "published_date": "2025-02-13T19:51:15+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2026-01-22T05:56:01.265857+00:00",
      "updated_at": "2026-01-22T05:56:01.265859+00:00"
    },
    {
      "id": "417111e13e4858f79734781862a95c17",
      "url": "https://brain.ieee.org/publications/neuroethics-framework/education/educational-and-training-resources-education/education-additional-resources/",
      "title": "Education: Additional Resources",
      "content": "Buckingham Shum, S. (2022). The UTS \u201cEdTech Ethics\u201d Deliberative Democracy Consultation: Rationale, Process and Outcomes. Connected Intelligence Centre, University of Technology Sydney, AUS. https://cic.uts.edu.au/projects/edtech-ethics Le\u00f3n Declaration on European neurotechnology (2023): a human-focused and rights-oriented approach October 2023. An informal meeting will be held with all telecommunications and digital ministers from EU member states. https://spanish-presidency.consilium.europa.eu/media/o4rh53jr/le%C3%B3n-declaration.pdf Neurotechnology Report: https://www.perseus-strategies.com/wp-content/uploads/2024/04/FINAL-Consumer-Neurotechnology-Report-Neurorights-Foundation-March-2024-3.pdf Al-Emran, M., Al-Nuaimi, ...",
      "author": "Adriel Carridice",
      "published_date": "2025-02-13T19:54:30+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2026-01-22T05:56:01.265840+00:00",
      "updated_at": "2026-01-22T05:56:01.265842+00:00"
    },
    {
      "id": "eddcbadea083b0a5930cbd4a70baf32e",
      "url": "https://brain.ieee.org/publications/neuroethics-framework/education/references/education-references/",
      "title": "Education: References",
      "content": "[1] OECD \u201cNeurotechnology Toolkit To support policymakers in implementing the OECD Recommendation on Responsible Innovation in Neurotechnology,\u201d 2024.: https://www.oecd.org/content/dam/oecd/en/topics/policy-sub-issues/emerging-technologies/neurotech-toolkit.pdf. [2] van Kesteren and Meeter, 2020 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7339924/ [3]\u00a0 Bikson, M., Esmaeilpour, Z., Adair, D., Kronberg, G., Tyler, W. J., Antal, A., Datta, A., Sabel, B. A., Nitsche, M. A., Loo, C., Edwards, D., Ekhtiari, H., Knotkova, H., Woods, A. J., Hampstead, ...",
      "author": "Adriel Carridice",
      "published_date": "2025-02-13T19:57:58+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2026-01-22T05:56:01.265815+00:00",
      "updated_at": "2026-01-22T05:56:01.265817+00:00"
    },
    {
      "id": "8bfe7b7f37af142b24b0fcd318868131",
      "url": "https://brain.ieee.org/braininsight-articles/ieee-brain-annual-flagship-workshop-a-success/",
      "title": "IEEE Brain Annual Flagship Workshop a Success",
      "content": "IEEE Brain once again hosted the IEEE Brain Discovery and Neurotechnology Workshop as a satellite event to the 2024 Society of Neuroscience Workshop (SfN). Approximately 180 attended the two-day event, which was held at the University of Illinois Chicago (UIC), October 3-4, 2024 (Figure 1). Groundbreaking solutions with the potential to improve quality of life and address neural disorders require ...",
      "author": "ieeebrain",
      "published_date": "2025-03-03T16:55:37+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2026-01-22T05:56:01.265791+00:00",
      "updated_at": "2026-01-22T05:56:01.265793+00:00"
    },
    {
      "id": "2eb57dbdbc858b57282cfad12fa8d826",
      "url": "https://brain.ieee.org/braininsight-articles/ieee-brain-workshop-on-ai-for-neurotechnology/",
      "title": "IEEE Brain Workshop on AI for Neurotechnology",
      "content": "The IEEE Brain Workshop on AI for Neurotechnology was held on June 30, 2024, at the Pacifico Yokohama Conference Center in Japan. This event was part of the World Congress on Computational Intelligence (WCCI 2024) and was conducted in association with the International Joint Conference on Neural Networks (IJCNN). The workshop focused on the application of artificial intelligence to neurotechnology, ...",
      "author": "ieeebrain",
      "published_date": "2025-03-03T17:05:59+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2026-01-22T05:56:01.265767+00:00",
      "updated_at": "2026-01-22T05:56:01.265769+00:00"
    },
    {
      "id": "0034e844e06f50c554f3ea92b4f7be58",
      "url": "https://iopscience.iop.org/article/10.1088/1741-2552/ae2805",
      "title": "A pretrained foundation model for headache disorders based on magnetoencephalography",
      "content": "Objective. Foundation models have demonstrated transformative potential in medical artificial intelligence but remain underexplored in functional neuroimaging, particularly magnetoencephalography (MEG). This study aims to develop a domain-specific, self-supervised MEG clinical foundation model tailored for headache disorders to address the challenges of high-dimensional data and limited labeled datasets in clinical research. Approach. We developed a transformer-based model pretrained on a large-scale dataset comprising multi-state MEG recordings (resting-state, auditory, and somatosensory stimulation) from 416 participants (362 headache patients and 54 healthy controls). The model utilized a self-supervised masked-signal reconstruction strategy to learn latent spatiotemporal representations of neural activity. We evaluated the model\u2019s performance through signal reconstruction, visualization of attention weights, and downstream classification tasks comparing model-derived features against original MEG signals for migraine diagnosis. Main results. The pretrained model successfully reconstructed both continuous MEG signals and stimulus-specific evoked responses, effectively capturing intrinsic spatiotemporal brain dynamics. Visualization of the model\u2019s attention weights demonstrated spatial alignment with corresponding sensory brain regions, confirming its neurophysiological interpretability. Furthermore, classifiers trained on features extracted from the pretrained model significantly outperformed those using original MEG signals in identifying migraine patients, revealing distinct neural response patterns. Significance. This study introduces a scalable, data-efficient framework for clinical MEG analysis that significantly reduces reliance on manual feature extraction and labeled data. It demonstrates the efficacy of foundation models in decoding complex neural dynamics, offering promising implications for understanding neuropathology and facilitating precision diagnostics in neurology.",
      "author": "Pan Liao, Jie Liang, Dong Qiu, Cunxin Lin, Zhonghua Xiong, Hao Wang, Jia-Hong Gao, Yonggang Wang and Bingjiang Lyu",
      "published_date": "2026-01-08T00:00:00+00:00",
      "source": "Journal Neural Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 234,
      "reading_time": 1,
      "created_at": "2026-01-22T05:55:26.093994+00:00",
      "updated_at": "2026-01-22T05:55:26.093996+00:00"
    },
    {
      "id": "efbd1fbf2b13b98ed3aa0bd6ef3fc254",
      "url": "http://doi.org/10.1037/bne0000632",
      "title": "Chemogenetic modulation of PAC1-expressing neurons in the bed nucleus of the stria terminalis (BNST) alters anxiety-related behaviors in male mice.",
      "content": "Pituitary adenylate cyclase-activating polypeptide (PACAP, <em>ADCYAP1</em>) is a highly conserved neuropeptide that plays essential roles in numerous physiological functions, and central PACAP signaling has been associated with mechanisms regulating stress-induced psychopathologies. PACAP binds to several receptor subtypes, including PAC1 (<em>ADCYAP1R1</em>), VPAC1 (<em>VIPR1</em>), and VPAC2 (<em>VIPR2</em>), to activate several signaling cascades that can alter neuronal excitability and enhance indices of neuroplasticity, and much of our prior work has suggested that the anxiogenic effects of bed nucleus of the stria terminalis (BNST) PACAP depend on the activation of PAC1 receptors. To complement our previous work that evaluated the roles of BNST PACAP expression and secretion in anxiety-related responses, we employed in the current work chemogenetic approaches in male PAC1-Ires-Cre mice to directly and specifically modulate the activities of BNST PAC1 receptor-expressing neurons. Inhibition of BNST PAC1 receptor neuron activity with clozapine-N-oxide significantly increased open arm exploration without reducing total locomotor activity; conversely, stimulating BNST PAC1 receptor function significantly reduced open arm exploratory activities. In sum, these data are consistent with our prior work suggesting a key role for BNST PACAP receptor activation in anxiety and stress; further, these observations importantly clarify the neural circuits involved in anxiety-like behaviors. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2025-09-01T00:00:00+00:00",
      "source": "Behavioral Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 205,
      "reading_time": 1,
      "created_at": "2026-01-22T05:55:05.039085+00:00",
      "updated_at": "2026-01-22T05:55:05.039087+00:00"
    },
    {
      "id": "08e664e88f4ab84df159a973a76a40e7",
      "url": "http://doi.org/10.1037/bne0000634",
      "title": "Conditioned place preferences for virtual reality cannabis cues.",
      "content": "This study investigated whether 221 undergraduates (123 males, 98 females) with varying levels of cannabis use displayed a conditioned place preference (CPP) for a virtual reality (VR) room that previously contained virtual cannabis stimuli compared to a neutral VR room that was not paired with cannabis cues. We hypothesized that cannabis-using participants (<em>n</em> = 180) would spend a greater amount of time in, report greater subjective enjoyment in, and explicitly prefer a VR room that was previously paired with virtual cannabis stimuli relative to a neutral room, while participants with nonuse (<em>n</em> = 41) would not. Overall, participants did not demonstrate an implicit or explicit CPP for a VR room that was previously paired with cannabis cues. Interestingly, however, participants with recent cannabis use (<em>n</em> = 41) exhibited a significant implicit CPP for the cannabis-cue-paired VR room, while participants with nonrecent cannabis use (<em>n</em> = 113) did not. Furthermore, relative to males with cannabis use (<em>n</em> = 93), females with cannabis use (<em>n</em> = 87) demonstrated a significant explicit CPP for the cannabis-cue-paired context as well as significantly greater cannabis cravings. These findings elucidate the need for further research on the role of acute cannabis intoxication, sex, and cue-induced cravings in modulating CPP for cannabis-associated contexts. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2025-09-01T00:00:00+00:00",
      "source": "Behavioral Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 215,
      "reading_time": 1,
      "created_at": "2026-01-22T05:55:05.039050+00:00",
      "updated_at": "2026-01-22T05:55:05.039051+00:00"
    },
    {
      "id": "5168657f83102357e9854c856d6e8912",
      "url": "http://doi.org/10.1037/bne0000630",
      "title": "Paraventricular thalamic inputs to the ventral pallidum shape reward seeking during threat and fear responding in extinction.",
      "content": "Environmental threats are typically encountered when animals are searching for food and other necessities. Adaptive behavior must balance competition between fear behavior and reward seeking. We gave rats local neuronal deletions of the ventral pallidum (VP) or specifically deleted paraventricular thalamic nucleus (PVT) neurons projecting directly to the VP. Rats were then assessed in a conditioned suppression procedure in which cues predicting unique foot shock probabilities were presented during, but independent from, reward seeking. Foot shock introduction generally suppressed reward seeking in rats, and recovery from shock introduction was facilitated in rats with VP or PVT \u2192 VP pathway deletions. Discriminative fear was observed in controls, and this fear responding reduced over a single extinction session. VP deletion enhanced extinction fear responding, and PVT \u2192 VP pathway deletion abolished within-session fear reductions. The results demonstrate the VP and its inputs from the PVT shape reward seeking in threat settings and govern fear extinction responding. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2025-09-01T00:00:00+00:00",
      "source": "Behavioral Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 163,
      "reading_time": 1,
      "created_at": "2026-01-22T05:55:05.039012+00:00",
      "updated_at": "2026-01-22T05:55:05.039014+00:00"
    },
    {
      "id": "b1cdce7f5e061a4ca5bf268adc4e71f2",
      "url": "http://doi.org/10.1037/bne0000629",
      "title": "Patterns of prefrontal cortical activity associated with attention-demanding and motor aspects of dual-task walking as measured with functional near-infrared spectroscopy.",
      "content": "The ability to engage in everyday tasks, such as walking, requires the integration of cognitive and motor processes. How these processes integrate may be discernable through the relation of brain activity patterns to behavioral performance, particularly in the prefrontal cortex (PFC), examination of which has been restricted because of the limitations in experimental design. We related behavior (cognition, walking) to brain activity, as measured by functional near-infrared spectroscopy, under dual-task conditions (cognition while walking) in healthy young adults. Our probe design enabled us to examine eight regions of interest across PFC and motor cortex to identify key areas related to behavior. Healthy young adults (N = 19) engaged in standing cognition (Serial 3 subtraction), single-task walking, and dual-task walking. We used functional near-infrared spectroscopy to identify regions associated with increases or decreases in activity under dual-task relative to the other conditions. We observed differences in brain activity patterns by task across multiple regions of interest, mostly in PFC. Specifically, more lateral regions were related to attention-demanding tasks, whereas motor tasks were related to relatively medial regions. Our results relate behavior to brain activity, as measured by functional near-infrared spectroscopy, under dual-task conditions. Our finding of relatively lateral PFC activity during attention-demanding tasks provides insights into behavioral and brain processes during experimental analogues of everyday activity, bringing us closer to understanding behavior-brain relations in the real world. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
      "author": "",
      "published_date": "2025-09-01T00:00:00+00:00",
      "source": "Behavioral Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 235,
      "reading_time": 1,
      "created_at": "2026-01-22T05:55:05.038966+00:00",
      "updated_at": "2026-01-22T05:55:05.038971+00:00"
    },
    {
      "id": "e4ce644e22b3482ad1db9b723cdab07b",
      "url": "https://arxiv.org/abs/2601.14641",
      "title": "MIND: Empowering Mental Health Clinicians with Multimodal Data Insights through a Narrative Dashboard",
      "content": "arXiv:2601.14641v1 Announce Type: new \nAbstract: Advances in data collection enable the capture of rich patient-generated data: from passive sensing (e.g., wearables and smartphones) to active self-reports (e.g., cross-sectional surveys and ecological momentary assessments). Although prior research has demonstrated the utility of patient-generated data in mental healthcare, significant challenges remain in effectively presenting these data streams along with clinical data (e.g., clinical notes) for clinical decision-making. Through co-design sessions with five clinicians, we propose MIND, a large language model-powered dashboard designed to present clinically relevant multimodal data insights for mental healthcare. MIND presents multimodal insights through narrative text, complemented by charts communicating underlying data. Our user study (N=16) demonstrates that clinicians perceive MIND as a significant improvement over baseline methods, reporting improved performance to reveal hidden and clinically relevant data insights (p<.001) and support their decision-making (p=.004). Grounded in the study results, we discuss future research opportunities to integrate data narratives in broader clinical practices.",
      "author": "Ruishi Zou, Shiyu Xu, Margaret E Morris, Jihan Ryu, Timothy D. Becker, Nicholas Allen, Anne Marie Albano, Randy Auerbach, Dan Adler, Varun Mishra, Lace Padilla, Dakuo Wang, Ryan Sultan, Xuhai \"Orson\" Xu",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928916+00:00",
      "updated_at": "2026-01-22T05:30:26.928918+00:00"
    },
    {
      "id": "4a3f0de0627e8f33b9a4d320c444904f",
      "url": "https://arxiv.org/abs/2601.14639",
      "title": "DesignBridge: Bridging Designer Expertise and User Preferences through AI-Enhanced Co-Design for Fashion",
      "content": "arXiv:2601.14639v1 Announce Type: new \nAbstract: Effective collaboration between designers and users is important for fashion design, which can increase the user acceptance of fashion products and thereby create value. However, it remains an enduring challenge, as traditional designer-centric approaches restrict meaningful user participation, while user-driven methods demand design proficiency, often marginalizing professional creative judgment. Current co-design practices, including workshops and AI-assisted frameworks, struggle with low user engagement, inefficient preference collection, and difficulties in balancing user feedback with design considerations. To address these challenges, we conducted a formative study with designers and users experienced in co-design (N=7), identifying critical challenges for current collaboration between designers and users in the co-design process, and their requirements. Informed by these insights, we introduce DesignBridge, a multi-platform AI-enhanced interactive system that bridges designer expertise and user preferences through three stages: (1) Initial Design Framing, where designers define initial concepts. (2) Preference Expression Collection, where users intuitively articulate preferences via interactive tools. (3) Preference-Integrated Design, where designers use AI-assisted analytics to integrate feedback into cohesive designs. A user study demonstrates that DesignBridge significantly enhances user preference collection and analysis, enabling designers to integrate diverse preferences with professional expertise.",
      "author": "Yuheng Shao, Yuansong Xu, Yifan Jin, Shuhao Zhang, Wenxin Gu, Quan Li",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 192,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928886+00:00",
      "updated_at": "2026-01-22T05:30:26.928887+00:00"
    },
    {
      "id": "f007e278f0f625eb274c3c679822541b",
      "url": "https://arxiv.org/abs/2601.14611",
      "title": "Seeing to Think? How Source Transparency Design Shapes Interactive Information Seeking and Evaluation in Conversational AI",
      "content": "arXiv:2601.14611v1 Announce Type: new \nAbstract: Conversational AI systems increasingly function as primary interfaces for information seeking, yet how they present sources to support information evaluation remains under-explored. This paper investigates how source transparency design shapes interactive information seeking, trust, and critical engagement. We conducted a controlled between-subjects experiment (N=372) comparing four source presentation interfaces - Collapsible, Hover Card, Footer, and Aligned Sidebar - varying in visibility and accessibility. Using fine-grained behavioral analysis and automated critical thinking assessment, we found that interface design fundamentally alters exploration strategies and evidence integration. While the Hover Card interface facilitated seamless, on-demand verification during the task, the Aligned Sidebar uniquely mitigated the negative effects of information overload: as citation density increased, Sidebar users demonstrated significantly higher critical thinking and synthesis scores compared to other conditions. Our results highlight a trade-off between designs that support workflow fluency and those that enforce reflective verification, offering practical implications for designing adaptive and responsible conversational AI that fosters critical engagement with AI generated content.",
      "author": "Jiangen He, Jiqun Liu",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 165,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928851+00:00",
      "updated_at": "2026-01-22T05:30:26.928853+00:00"
    },
    {
      "id": "81e792f7cebcf07fca23773c31954407",
      "url": "https://arxiv.org/abs/2601.14589",
      "title": "Designing KRIYA: An AI Companion for Wellbeing Self-Reflection",
      "content": "arXiv:2601.14589v1 Announce Type: new \nAbstract: Most personal wellbeing apps present summative dashboards of health and physical activity metrics, yet many users struggle to translate this information into meaningful understanding. These apps commonly support engagement through goals, reminders, and structured targets, which can reinforce comparison, judgment, and performance anxiety. To explore a complementary approach that prioritizes self-reflection, we design KRIYA, an AI wellbeing companion that supports co-interpretive engagement with personal wellbeing data. KRIYA aims to collaborate with users to explore questions, explanations, and future scenarios through features such as Comfort Zone, Detective Mode, and What-If Planning. We conducted semi-structured interviews with 18 college students interacting with a KRIYA prototype using hypothetical data. Our findings show that through KRIYA interaction, users framed engaging with wellbeing data as interpretation rather than performance, experienced reflection as supportive or pressuring depending on emotional framing, and developed trust through transparency. We discuss design implications for AI companions that support curiosity, self-compassion, and reflective sensemaking of personal health data.",
      "author": "Shanshan Zhu, Wenxuan Song, Jiayue Melissa Shi, Dong Whi Yoo, Karthik S. Bhat, Koustuv Saha",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 162,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928806+00:00",
      "updated_at": "2026-01-22T05:30:26.928808+00:00"
    },
    {
      "id": "9b32107b41c6e05208a75cd19c7ff276",
      "url": "https://arxiv.org/abs/2601.14587",
      "title": "Explainable OOHRI: Communicating Robot Capabilities and Limitations as Augmented Reality Affordances",
      "content": "arXiv:2601.14587v1 Announce Type: new \nAbstract: Human interaction is essential for issuing personalized instructions and assisting robots when failure is likely. However, robots remain largely black boxes, offering users little insight into their evolving capabilities and limitations. To address this gap, we present explainable object-oriented HRI (X-OOHRI), an augmented reality (AR) interface that conveys robot action possibilities and constraints through visual signifiers, radial menus, color coding, and explanation tags. Our system encodes object properties and robot limits into object-oriented structures using a vision-language model, allowing explanation generation on the fly and direct manipulation of virtual twins spatially aligned within a simulated environment. We integrate the end-to-end pipeline with a physical robot and showcase diverse use cases ranging from low-level pick-and-place to high-level instructions. Finally, we evaluate X-OOHRI through a user study and find that participants effectively issue object-oriented commands, develop accurate mental models of robot limitations, and engage in mixed-initiative resolution.",
      "author": "Lauren W. Wang, Mohamed Kari, Parastoo Abtahi",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 150,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928774+00:00",
      "updated_at": "2026-01-22T05:30:26.928775+00:00"
    },
    {
      "id": "29b0ccc788906fa8395d25f9c453d048",
      "url": "https://arxiv.org/abs/2601.14566",
      "title": "SCSimulator: An Exploratory Visual Analytics Framework for Partner Selection in Supply Chains through LLM-driven Multi-Agent Simulation",
      "content": "arXiv:2601.14566v1 Announce Type: new \nAbstract: Supply chains (SCs), complex networks spanning from raw material acquisition to product delivery, with enterprises as interconnected nodes, play a pivotal role in organizational success. However, optimizing SCs remains challenging, particularly in partner selection, a key bottleneck shaped by competitive and cooperative dynamics. This challenge constitutes a multi-objective dynamic game requiring a synergistic integration of Multi-Criteria Decision-Making and Game Theory. Traditional approaches, grounded in mathematical simplifications and managerial heuristics, fail to capture real-world intricacies and risk introducing subjective biases. Multi-agent simulation offers promise, but prior research has largely relied on fixed, uniform agent logic, limiting practical applicability. Recent advances in LLMs create opportunities to represent complex SC requirements and hybrid game logic. However, challenges persist in modeling dynamic SC relationships, ensuring interpretability, and balancing agent autonomy with expert control. We present SCSimulator, a visual analytics framework that integrates LLM-driven MAS with human-in-the-loop collaboration for SC partner selection. It simulates SC evolution via adaptive network structures and enterprise behaviors, which are visualized via interpretable interfaces. By combining CoT reasoning with XAI techniques, it generates multi-faceted, transparent explanations of decision trade-offs. Users can iteratively adjust simulation settings to explore outcomes aligned with their expectations and strategic priorities. Developed through iterative co-design with SC experts and industry managers, SCSimulator serves as a proof-of-concept, offering methodological contributions and practical insights for future research on SC decision-making and interactive AI-driven analytics. Usage scenarios and a user study demonstrate the system's effectiveness and usability.",
      "author": "Shenghan Gao, Junye Wang, Junjie Xiong, Yun Jiang, Yun Fang, Qifan Hu, Baolong Liu, Quan Li",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 243,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928735+00:00",
      "updated_at": "2026-01-22T05:30:26.928736+00:00"
    },
    {
      "id": "0316fe6ab494a200ff4d262162cb9e4c",
      "url": "https://arxiv.org/abs/2601.14561",
      "title": "Evaluating Preattentive Features for Detecting Changes in Virtual Environments",
      "content": "arXiv:2601.14561v1 Announce Type: new \nAbstract: Visual perception plays a critical role in detecting changes within immersive Virtual Reality (VR) environments. However, as visual complexity increases, perceptual performance declines, making it more difficult to detect changes quickly and accurately. This study examines how visual features, known for facilitating preattentive processing, impact a change detection task in immersive 3D environments, with a focus on visual complexity, object attributes, and spatial proximity. Our results demonstrate that preattentive processing enhances change detection, particularly when the altered object is spatially isolated and not perceptually grouped with similar surrounding objects. Changes to isolated objects were detected more reliably, suggesting that perceptual isolation reduces cognitive load and draws more attention. Conversely, when a changed object was surrounded by visually similar elements, participants were less likely to detect the change, indicating that perceptual grouping hinders individual object recognition in complex scenes. These results provide guidelines for designing VR applications that strategically utilize spatial isolation and visual features to improve the user experience.",
      "author": "DongHoon Kim, Isaac Cho",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 164,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928690+00:00",
      "updated_at": "2026-01-22T05:30:26.928692+00:00"
    },
    {
      "id": "d7ae69e65249c0dfb9d9e9ff9a82d3ca",
      "url": "https://arxiv.org/abs/2601.14435",
      "title": "SPIRIT: A Design Framework To Support Technology Interventions for Spiritual Care Within and Beyond the Clinic",
      "content": "arXiv:2601.14435v1 Announce Type: new \nAbstract: Despite its importance for well-being, spiritual care remains under-explored in HCI, while the adoption of technology in clinical spiritual care lags behind other healthcare fields. Prior work derived a definition of \"spiritual support\" through co-design workshops with stakeholders in online health communities. This paper contributes: (1) a revision of that definition through member checking with professional spiritual care providers (SCPs); (2) a novel design framework -- SPIRIT -- which can help to expand models of delivery for spiritual care using digital technologies. Through re-analysis of previous data and new interviews with SCPs, we identify three prerequisites for meaningful spiritual care: openness to care, safe space, and the ability to discern and articulate spiritual needs. We also propose six design dimensions: loving presence, meaning-making, appropriate degree of technology use, location, degree of relational closeness, and temporality. We discuss how SPIRIT offers guidance for designing impactful digital spiritual care intervention systems within and beyond clinical settings.",
      "author": "C. Estelle Smith, Alemitu Bezabih, Shadi Nourriz, Jesan Ahammed Ovi",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 159,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928652+00:00",
      "updated_at": "2026-01-22T05:30:26.928654+00:00"
    },
    {
      "id": "d033ba328fe702823801d43d0a6b6132",
      "url": "https://arxiv.org/abs/2601.14423",
      "title": "Loss Aversion Online: Emotional Responses to Financial Booms and Crashes",
      "content": "arXiv:2601.14423v1 Announce Type: new \nAbstract: Financial events negatively affect emotional well-being, but large-scale studies examining their impact on online emotional expression using real-time social media data remain limited. To address this gap, we propose analyzing Reddit communities (financial and non-financial) across two case studies: a financial crash and a boom. We investigate how emotional and psycholinguistic responses differ between financial and non-financial communities, and the extent to which the type of financial event affects user behavior during the two case study periods. To examine the effect of these events on expressed language, we analyze daily sentiment, emotion, and LIWC counts using quasi-experimental methods: Difference-in-Differences (DiD) and Causal Impact analyses during a financial boom and a financial crash. Overall, we find coherent, negative shifts in emotional responses during financial crashes, but weaker, mixed responses during booms, consistent with loss aversion. By exploring emotional and psycholinguistic expressions during financial events, we identify future implications for understanding online users' mental health and building connected, healthy communities.",
      "author": "Aryan Ramchandra Kapadia, Niharika Bhattacharjee, Mung Yao Jia, Ishq Gupta, Dong Wang, Koustuv Saha",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 163,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928619+00:00",
      "updated_at": "2026-01-22T05:30:26.928621+00:00"
    },
    {
      "id": "28e82a9d13970ab85fff8b5aa37b2b8e",
      "url": "https://arxiv.org/abs/2601.14324",
      "title": "When Generative AI Is Intimate, Sexy, and Violent: Examining Not-Safe-For-Work (NSFW) Chatbots on FlowGPT",
      "content": "arXiv:2601.14324v1 Announce Type: new \nAbstract: User-created chatbots powered by generative AI offer new ways to share and interact with Not-Safe-For-Work (NSFW) content. However, little is known about the characteristics of these GenAI-based chatbots and their user interactions. Drawing on the functional theory of NSFW on social media, this study analyzes 376 NSFW chatbots and 307 public conversation sessions on FlowGPT. Findings identify four chatbot types: roleplay characters, story generators, image generators, and do-anything-now bots. AI Characters portraying fantasy personas and enabling hangout-style interactions are most common, often using explicit avatar images to invite engagement. Sexual, violent, and insulting content appears in both user prompts and chatbot outputs, with some chatbots generating explicit material even when users do not create erotic prompts. In sum, the NSFW experience on FlowGPT can be understood as a combination of virtual intimacy, sexual delusion, violent thought expression, and unsafe content acquisition. We conclude with implications for chatbot design, creator support, user safety, and content moderation.",
      "author": "Xian Li, Yuanning Han, Di Liu, Pengcheng An, Shuo Niu",
      "published_date": "2026-01-22T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 160,
      "reading_time": 1,
      "created_at": "2026-01-22T05:30:26.928577+00:00",
      "updated_at": "2026-01-22T05:30:26.928583+00:00"
    }
  ]
}