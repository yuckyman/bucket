{
  "last_updated": "2025-12-29T05:30:42.358524+00:00",
  "count": 20,
  "articles": [
    {
      "id": "eddcbadea083b0a5930cbd4a70baf32e",
      "url": "https://brain.ieee.org/publications/neuroethics-framework/education/references/education-references/",
      "title": "Education: References",
      "content": "[1] OECD \u201cNeurotechnology Toolkit To support policymakers in implementing the OECD Recommendation on Responsible Innovation in Neurotechnology,\u201d 2024.: https://www.oecd.org/content/dam/oecd/en/topics/policy-sub-issues/emerging-technologies/neurotech-toolkit.pdf. [2] van Kesteren and Meeter, 2020 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7339924/ [3]\u00a0 Bikson, M., Esmaeilpour, Z., Adair, D., Kronberg, G., Tyler, W. J., Antal, A., Datta, A., Sabel, B. A., Nitsche, M. A., Loo, C., Edwards, D., Ekhtiari, H., Knotkova, H., Woods, A. J., Hampstead, ...",
      "author": "Adriel Carridice",
      "published_date": "2025-02-13T19:57:58+00:00",
      "source": "Brain",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 61,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:30.497636+00:00",
      "updated_at": "2025-12-29T05:30:30.497638+00:00"
    },
    {
      "id": "af466c476921b1cf2c29e1975eb0ed61",
      "url": "https://arxiv.org/abs/2512.21552",
      "title": "Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments",
      "content": "arXiv:2512.21552v1 Announce Type: cross \nAbstract: Artificial intelligence (AI) is transforming education, offering unprecedented opportunities to personalize learning, enhance assessment, and support educators. Yet these opportunities also introduce risks related to equity, privacy, and student autonomy. This chapter develops the concept of bidirectional human-AI alignment in education, emphasizing that trustworthy learning environments arise not only from embedding human values into AI systems but also from equipping teachers, students, and institutions with the skills to interpret, critique, and guide these technologies. Drawing on emerging research and practical case examples, we explore AI's evolution from support tool to collaborative partner, highlighting its impacts on teacher roles, student agency, and institutional governance. We propose actionable strategies for policymakers, developers, and educators to ensure that AI advances equity, transparency, and human flourishing rather than eroding them. By reframing AI adoption as an ongoing process of mutual adaptation, the chapter envisions a future in which humans and intelligent systems learn, innovate, and grow together.",
      "author": "Hua Shen",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 158,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783546+00:00",
      "updated_at": "2025-12-29T05:30:19.783548+00:00"
    },
    {
      "id": "b5ed77d36821380a3c34d95a5ffa27f9",
      "url": "https://arxiv.org/abs/2512.21506",
      "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding",
      "content": "arXiv:2512.21506v1 Announce Type: cross \nAbstract: As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral summaries. We construct a novel dataset of 54383 (actigraphy, text) pairs derived from real-world NHANES recordings, and train the model using cross-entropy loss with supervision only on the language tokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7 percent in ROUGE-1. The average training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative analysis confirms that MotionTeller captures circadian structure and behavioral transitions, while PCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these results position MotionTeller as a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, introducing new pathways for behavioral monitoring, clinical review, and personalized health interventions.",
      "author": "Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 206,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783517+00:00",
      "updated_at": "2025-12-29T05:30:19.783518+00:00"
    },
    {
      "id": "e2025314cc5baa0349efea4b6a5021f9",
      "url": "https://arxiv.org/abs/2512.22032",
      "title": "Context-Aware Intelligent Chatbot Framework Leveraging Mobile Sensing",
      "content": "arXiv:2512.22032v1 Announce Type: new \nAbstract: With the rapid advancement of large language models (LLMs), intelligent conversational assistants have demonstrated remarkable capabilities across various domains. However, they still mainly rely on explicit textual input and do not know the real world behaviors of users. This paper proposes a context-sensitive conversational assistant framework grounded in mobile sensing data. By collecting user behavior and environmental data through smartphones, we abstract these signals into 16 contextual scenarios and translate them into natural language prompts, thus improving the model's understanding of the user's state. We design a structured prompting system to guide the LLM in generating a more personalized and contextually relevant dialogue. This approach integrates mobile sensing with large language models, demonstrating the potential of passive behavioral data in intelligent conversation and offering a viable path toward digital health and personalized interaction.",
      "author": "Ziyan Zhang, Nan Gao, Zhiqiang Nie, Shantanu Pal, Haining Zhang",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 138,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783484+00:00",
      "updated_at": "2025-12-29T05:30:19.783486+00:00"
    },
    {
      "id": "2fc6a1de2fe67ab525b35e93d555abee",
      "url": "https://arxiv.org/abs/2512.22016",
      "title": "SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching",
      "content": "arXiv:2512.22016v1 Announce Type: new \nAbstract: Creating physically realistic content in VR often requires complex modeling tools or predefined 3D models, textures, and animations, which present significant barriers for non-expert users. In this paper, we propose SketchPlay, a novel VR interaction framework that transforms humans' air-drawn sketches and gestures into dynamic, physically realistic scenes, making content creation intuitive and playful like drawing. Specifically, sketches capture the structure and spatial arrangement of objects and scenes, while gestures convey physical cues such as velocity, direction, and force that define movement and behavior. By combining these complementary forms of input, SketchPlay captures both the structure and dynamics of user-created content, enabling the generation of a wide range of complex physical phenomena, such as rigid body motion, elastic deformation, and cloth dynamics. Experimental results demonstrate that, compared to traditional text-driven methods, SketchPlay offers significant advantages in expressiveness, and user experience. By providing an intuitive and engaging creation process, SketchPlay lowers the entry barrier for non-expert users and shows strong potential for applications in education, art, and immersive storytelling.",
      "author": "Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 173,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783457+00:00",
      "updated_at": "2025-12-29T05:30:19.783458+00:00"
    },
    {
      "id": "cd386855ed6c44daae9e1586682432d7",
      "url": "https://arxiv.org/abs/2512.21968",
      "title": "Positive Narrativity Enhances Sense of Agency toward a VR Avatar",
      "content": "arXiv:2512.21968v1 Announce Type: new \nAbstract: The full-body illusion (FBI) refers to the experience of perceiving a virtual avatar as one's own body. In virtual reality (VR) environments, inducing the FBI has been shown to modulate users' bodily experiences and behavior. Previous studies have demonstrated that embodying avatars with specific characteristics can influence users' actions, largely through the activation of implicit stereotypes. However, few studies have explicitly manipulated users' impressions of an avatar by introducing narrative context. The present study investigated how avatar narrativity, induced through contextual narratives, affects the FBI. Healthy participants embodied a powerful artificial lifeform avatar in VR after listening to either a positive narrative, in which the avatar used its abilities to protect others, or a negative narrative, in which it misused its power. Participants' impressions of the avatar and indices of bodily self-consciousness were subsequently assessed. The results showed that positive narratives significantly enhanced the sense of agency (SoA), and that SoA was positively correlated with participants' perceived personal familiarity with the avatar. These findings suggest that the avatar narrativity can modulate embodiment in VR.",
      "author": "Kureha Hamagashira, Miyuki Azuma, Sotaro Shimada",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 179,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783426+00:00",
      "updated_at": "2025-12-29T05:30:19.783428+00:00"
    },
    {
      "id": "7a70a150150414499da2dcc44ec38239",
      "url": "https://arxiv.org/abs/2512.21796",
      "title": "Generative Lecture: Making Lecture Videos Interactive with LLMs and AI Clone Instructors",
      "content": "arXiv:2512.21796v1 Announce Type: new \nAbstract: We introduce Generative Lecture, a concept that makes existing lecture videos interactive through generative AI and AI clone instructors. By leveraging interactive avatars powered by HeyGen, ElevenLabs, and GPT-5, we embed an AI instructor into the video and augment the video content in response to students' questions. This allows students to personalize the lecture material, directly ask questions in the video, and receive tailored explanations generated and delivered by the AI-cloned instructor. From a design elicitation study (N=8), we identified four goals that guided the development of eight system features: 1) on-demand clarification, 2) enhanced visuals, 3) interactive example, 4) personalized explanation, 5) adaptive quiz, 6) study summary, 7) automatic highlight, and 8) adaptive break. We then conducted a user study (N=12) to evaluate the usability and effectiveness of the system and collected expert feedback (N=5). The results suggest that our system enables effective two-way communication and supports personalized learning.",
      "author": "Hye-Young Jo, Ada Zhao, Xiaoan Liu, Ryo Suzuki",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 155,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783395+00:00",
      "updated_at": "2025-12-29T05:30:19.783397+00:00"
    },
    {
      "id": "89415df711861c6700c86b1480891082",
      "url": "https://arxiv.org/abs/2512.21747",
      "title": "Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG",
      "content": "arXiv:2512.21747v1 Announce Type: new \nAbstract: Driver drowsiness remains a primary cause of traffic accidents, necessitating the development of real-time, reliable detection systems to ensure road safety. This study presents a Modified TSception architecture designed for the robust assessment of driver fatigue using Electroencephalography (EEG). The model introduces a novel hierarchical architecture that surpasses the original TSception by implementing a five-layer temporal refinement strategy to capture multi-scale brain dynamics. A key innovation is the use of Adaptive Average Pooling, which provides the structural flexibility to handle varying EEG input dimensions, and a two - stage fusion mechanism that optimizes the integration of spatiotemporal features for improved stability. When evaluated on the SEED-VIG dataset and compared against established methods - including SVM, Transformer, EEGNet, ConvNeXt, LMDA-Net, and the original TSception - the Modified TSception achieves a comparable accuracy of 83.46% (vs. 83.15% for the original). Critically, the proposed model exhibits a substantially reduced confidence interval (0.24 vs. 0.36), signifying a marked improvement in performance stability. Furthermore, the architecture's generalizability is validated on the STEW mental workload dataset, where it achieves state-of-the-art results with 95.93% and 95.35% accuracy for 2-class and 3-class classification, respectively. These improvements in consistency and cross-task generalizability underscore the effectiveness of the proposed modifications for reliable EEG-based monitoring of drowsiness and mental workload.",
      "author": "Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 214,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783366+00:00",
      "updated_at": "2025-12-29T05:30:19.783367+00:00"
    },
    {
      "id": "a04ed99601d7da14b2126dc7d6dc0b91",
      "url": "https://arxiv.org/abs/2512.21649",
      "title": "Ghostcrafting AI: Under the Rug of Platform Labor",
      "content": "arXiv:2512.21649v1 Announce Type: new \nAbstract: Platform laborers play an indispensable yet hidden role in building and sustaining AI systems. Drawing on an eight-month ethnography of Bangladesh's platform labor industry and inspired by Gray and Suri, we conceptualize Ghostcrafting AI to describe how workers materially enable AI while remaining invisible or erased from recognition. Workers pursue platform labor as a path to prestige and mobility but sustain themselves through resourceful, situated learning - renting cyber-cafe computers, copying gig templates, following tutorials in unfamiliar languages, and relying on peer networks. At the same time, they face exploitative wages, unreliable payments, biased algorithms, and governance structures that make their labor precarious and invisible. To cope, they develop tactical repertoires such as identity masking, bypassing platform fees, and pirated tools. These practices reveal both AI's dependency on ghostcrafted labor and the urgent need for design, policy, and governance interventions that ensure fairness, recognition, and sustainability in platform futures.",
      "author": "ATM Mizanur Rahman (University of Illinois Urbana-Champaign, USA), Sharifa Sultana (University of Illinois Urbana-Champaign, USA)",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 154,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783330+00:00",
      "updated_at": "2025-12-29T05:30:19.783332+00:00"
    },
    {
      "id": "ebf55dd350e7c13991bffd61b088fb1a",
      "url": "https://arxiv.org/abs/2512.21589",
      "title": "Emotion-Aware Smart Home Automation Based on the eBICA Model",
      "content": "arXiv:2512.21589v1 Announce Type: new \nAbstract: Smart home automation that adapts to a user's emotional state can enhance psychological safety in daily living environments. This study proposes an emotion-aware automation framework guided by the emotional Biologically Inspired Cognitive Architecture (eBICA), which integrates appraisal, somatic responses, and behavior selection. We conducted a proof-of-concept experiment in a pseudo-smart-home environment, where participants were exposed to an anxiety-inducing event followed by a comfort-inducing automation. State anxiety (STAI-S) was measured throughout the task sequence. The results showed a significant reduction in STAI-S immediately after introducing the avoidance automation, demonstrating that emotion-based control can effectively promote psychological safety. Furthermore, an analysis of individual characteristics suggested that personality and anxiety-related traits modulate the degree of relief, indicating the potential for personalized emotion-adaptive automation. Overall, this study provides empirical evidence that eBICA-based emotional control can function effectively in smart home environments and offers a foundation for next-generation affective home automation systems.",
      "author": "Masaaki Yamauchi, Yiyuan Liang, Hiroko Hara, Hideyuki Shimonishi, Masayuki Murata",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 152,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783300+00:00",
      "updated_at": "2025-12-29T05:30:19.783302+00:00"
    },
    {
      "id": "bc1251aaea58ea1fc410c2ab4338af00",
      "url": "https://arxiv.org/abs/2512.21551",
      "title": "Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures",
      "content": "arXiv:2512.21551v1 Announce Type: new \nAbstract: The rapid integration of generative AI into everyday life underscores the need to move beyond unidirectional alignment models that only adapt AI to human values. This workshop focuses on bidirectional human-AI alignment, a dynamic, reciprocal process where humans and AI co-adapt through interaction, evaluation, and value-centered design. Building on our past CHI 2025 BiAlign SIG and ICLR 2025 Workshop, this workshop will bring together interdisciplinary researchers from HCI, AI, social sciences and more domains to advance value-centered AI and reciprocal human-AI collaboration. We focus on embedding human and societal values into alignment research, emphasizing not only steering AI toward human values but also enabling humans to critically engage with and evolve alongside AI systems. Through talks, interdisciplinary discussions, and collaborative activities, participants will explore methods for interactive alignment, frameworks for societal impact evaluation, and strategies for alignment in dynamic contexts. This workshop aims to bridge the disciplines' gaps and establish a shared agenda for responsible, reciprocal human-AI futures.",
      "author": "Hua Shen (Cassandra), Tiffany Knearem (Cassandra), Divy Thakkar (Cassandra), Pat Pataranutaporn (Cassandra), Anoop Sinha (Cassandra),  Yike (Cassandra),  Shi, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Cs Hc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 163,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:19.783264+00:00",
      "updated_at": "2025-12-29T05:30:19.783268+00:00"
    },
    {
      "id": "f3ef16c1b80d0c5a14d9e3f1abed1ee3",
      "url": "https://arxiv.org/abs/2512.21881",
      "title": "SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis",
      "content": "arXiv:2512.21881v1 Announce Type: cross \nAbstract: Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.",
      "author": "Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 185,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:18.636955+00:00",
      "updated_at": "2025-12-29T05:30:18.636958+00:00"
    },
    {
      "id": "6269efe798c0ad90851bb63293b00dee",
      "url": "https://arxiv.org/abs/2512.22045",
      "title": "Learning continually with representational drift",
      "content": "arXiv:2512.22045v1 Announce Type: new \nAbstract: Deep artificial neural networks famously struggle to learn from non-stationary streams of data. Without dedicated mitigation strategies, continual learning is associated with continuous forgetting of previous tasks and a progressive loss of plasticity. Current approaches to continual learning have either focused on increasing the stability of representations of past tasks, or on promoting plasticity for future learning. Paradoxically, while animals including humans achieve a desirable stability-plasticity trade-off, the responses of biological neurons to external stimuli that are associated with stable behaviors gradually change over time. This suggests that, although unstable representations have historically been seen as undesirable in artificial systems, they could be a core property of biological neural networks learning continually. Here, we examine how linking representational drift to continual learning in biological neural networks could inform artificial systems. We highlight the existence of representational drift across numerous animal species and brain regions and propose that drift reflects a mixture of homeostatic turnover and learning-related synaptic plasticity. In particular, we evaluate how plasticity induced by learning new tasks could induce drift in the representation of previous tasks, and how such drift could accumulate across brain regions. In deep artificial neural networks, we propose that representational drift is only compatible with approaches that do not explicitly prevent parameter changes to mitigate forgetting. Remarkably, jointly promoting plasticity while mitigating forgetting could in principle induce representational drift in continual learning. While we argue that drift is a byproduct rather than a solution to incremental learning, its investigation could inform approaches to continual learning in artificial systems.",
      "author": "Suzanne van der Veldt, Gido M. van de Ven, Sanne Moorman, Guillaume Etter",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 259,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:18.636892+00:00",
      "updated_at": "2025-12-29T05:30:18.636894+00:00"
    },
    {
      "id": "17ae4de26b585daa5ec226f54aeb244b",
      "url": "https://arxiv.org/abs/2512.21768",
      "title": "Numerical Twin with Two Dimensional Ornstein--Uhlenbeck Processes of Transient Oscillations in EEG signal",
      "content": "arXiv:2512.21768v1 Announce Type: new \nAbstract: Stochastic burst-like oscillations are common in physiological signals, yet there are few compact generative models that capture their transient structure. We propose a numerical-twin framework that represents transient narrowband activity as a two-dimensional Ornstein-Uhlenbeck (OU) process with three interpretable parameters: decay rate, mean frequency, and noise amplitude. We develop two complementary estimation strategies. The first fits the power spectral density, amplitude distribution, and autocorrelation to recover OU-parameters. The second segments burst events and performs a statistical match between empirical spindle statistics (duration, amplitude, inter-event interval) and simulated OU output via grid search, resolving parameter degeneracies by including event counts. We extend the framework to multiple frequency bands and piecewise-stationary dynamics to track slow parameter drifts. Applied to electroencephalography (EEG) recorded during general anesthesia, the method identifies OU models that reproduce alpha-spindle (8-12 Hz) morphology and band-limited spectra with low residual error, enabling real-time tracking of state changes that are not apparent from band power alone. This decomposition yields a sparse, interpretable representation of transient oscillations and provides interpretable metrics for brain monitoring.",
      "author": "P. O. Michel, C. Sun, S. Jaffard, D. Longrois, D. Holcman",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 177,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:18.636851+00:00",
      "updated_at": "2025-12-29T05:30:18.636853+00:00"
    },
    {
      "id": "4d03e890a8eabb6ba8ef36ad97fb0725",
      "url": "https://arxiv.org/abs/2512.21659",
      "title": "Metaboplasticity: The Reciprocal Regulation of Neuronal Activity and Cellular Energetics",
      "content": "arXiv:2512.21659v1 Announce Type: new \nAbstract: Standard Spiking Neural Network (SNN) models typically neglect metabolic constraints, treating neurons as energetically unconstrained components. We bridge this gap by implementing a conductance-based leaky integrate-and-fire (gLIF) microcircuit (N=5,000) in Brian2, using temperature-dependent Q10 scaling to as a biophysically grounded proxy to couple metabolic state with intrinsic excitability and synaptic plasticity. Our simulations revealed five distinct emergent properties: (1) Dynamics Bifurcation: Learning trajectories diverged significantly, with hypometabolic states plateauing near baseline and hypermetabolic states exhibiting non-linear, runaway potentiation; (2) STDP Window Deformation: Thermal stress structurally deformed the plasticity kernel, where hypermetabolism sharpened coincidence detection and hypometabolism flattened synaptic integration; (3) Signal Degradation: While metabolic rate positively correlated with connectivity strength, high-energy states caused synaptic saturation and a loss of sparse coding specificity; (4) Topological Shift: Network activity transitioned from sparse, asynchronous firing in energy-restricted states to pathological, seizure-like hypersynchronization in high-energy states ; and (5) Parametric Robustness: Sensitivity analysis confirmed these attractor states were intrinsic biophysical properties, robust across random network initializations. Collectively, these results define an \"inverted-U\" relationship between bioenergetics and learning, demonstrating that metabolic constraints are necessary hardware regulators for network stability.",
      "author": "Ece \\\"Oner, Cenk Denkta\\c{s}",
      "published_date": "2025-12-29T05:00:00+00:00",
      "source": "Arxiv Qbio Nc",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:18.636814+00:00",
      "updated_at": "2025-12-29T05:30:18.636818+00:00"
    },
    {
      "id": "a2bcc610f5f8b619e444e0f684389a1a",
      "url": "https://www.nature.com/articles/s41380-025-03439-6",
      "title": "Rapid antidepressant potential of nitrous oxide: current state and major questions",
      "content": "",
      "author": "",
      "published_date": "2025-12-29T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:07.136683+00:00",
      "updated_at": "2025-12-29T05:30:07.136685+00:00"
    },
    {
      "id": "88011594189f5a7bee0b9a7132f4f485",
      "url": "https://www.nature.com/articles/s41467-025-67517-7",
      "title": "Parallel encoding of speech in human frontal and temporal lobes",
      "content": "",
      "author": "",
      "published_date": "2025-12-29T00:00:00+00:00",
      "source": "Nature Neuroscience Subjects",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 0,
      "reading_time": 1,
      "created_at": "2025-12-29T05:30:07.136598+00:00",
      "updated_at": "2025-12-29T05:30:07.136603+00:00"
    },
    {
      "id": "cce4d30e1d2374e23b4330c69793d7c8",
      "url": "https://iopscience.iop.org/article/10.1088/1741-2552/ae2ba7",
      "title": "An augmented preference-based Bayesian approach for optimizing neuromodulation stimulation parameters using meta learning",
      "content": "Background. Electrical neuromodulation is increasingly used in the treatment of neurological disorders; however, the selection of stimulation parameters that provide optimal therapeutic benefits remains a major challenge. Moreover, identifying pathological biomarkers linking the effect of stimulation parameters to alleviating symptoms, and hence required for optimizing stimulation parameters, might not always be possible. Objective. We present an augmented, preference-based Bayesian optimization algorithm to optimize stimulation parameters for participants undergoing neuromodulation. This algorithm incorporates two key features: I) It prioritizes the participant\u2019s preferences for stimulation parameters, making it independent of the need for pathological biomarkers. II) It leverages meta learning, using historical participant data to guide the initial optimization for new participants and overcome initial data sparsity. This approach improves both prediction accuracy and convergence speed. Approach. Consider preference training data collected from a set of historical participants who share the same neurological disorder as a new (target) participant. Within that population, there may be different response phenotypes. The goal is to identify historical participants whose stimulation-response phenotype is most similar to the target participant, and leverage their data to accelerate and improve parameter optimization for the target participant. To achieve this, the algorithm iteratively performs a two-step process:(I) a novel, iterative weighting procedure that identifies historical participants with stimulation preferences closest to the target participant, and (II) meta learning that combines the training data of the identified participants with the limited training data of the target participant to train novel, augmented preference learning models. These models are then used to predict the stimulation parameters expected to maximize the target participant\u2019s preference. Mainresults. The proposed algorithm has been validated using synthetically generated data sets that simulate participant preference behavior during neuromodulation. Significance. This approach holds promise for improving personalized neuromodulation therapies and advancing treatment outcomes for neurological disorders without the need for a tedious data collection process and disease-specific pathological biomarkers.",
      "author": "Hafsa Farooqi, Zixi Zhao, David Darrow, Andrew Lamperski and Th\u00e9oden I Netoff",
      "published_date": "2025-12-23T00:00:00+00:00",
      "source": "Journal Neural Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 309,
      "reading_time": 1,
      "created_at": "2025-12-29T05:29:51.775142+00:00",
      "updated_at": "2025-12-29T05:29:51.775144+00:00"
    },
    {
      "id": "d836f968e21b6822df4a3de929edb8df",
      "url": "https://iopscience.iop.org/article/10.1088/1741-2552/ae2954",
      "title": "STeCANet: spatio-temporal cross attention network for brain computer interface systems using EEG-fNIRS signals",
      "content": "Objective. Multimodal neuroimaging fusion has shown promise in enhancing brain\u2013computer interface (BCI) performance by capturing complementary neural dynamics. However, most existing fusion frameworks inadequately model the temporal asynchrony and adaptive fusion between electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS), thereby limiting their ability to generalize across sessions and subjects. This work aims to develop an adaptive fusion framework that effectively aligns and integrates EEG and fNIRS representations to improve cross-session and cross-subject generalization in BCI applications. Approach. To address this, we propose STeCANet, a novel Spatiotemporal Cross-Attention Network that integrates EEG and fNIRS signals through hierarchical attention-based alignment. The model leverages fNIRS-guided spatial attention, EEG-fNIRS temporal alignment, adaptive fusion, and adversarial training to ensure robust cross-modal interaction and spatiotemporal consistency. Main results. Evaluations across three cognitive paradigms, namely motor imagery, mental arithmetic, and word generation, demonstrate that STeCANet significantly outperforms unimodal and recent multimodal baselines under both session-independent and subject-independent settings. Ablation studies confirm the contribution of each sub-module and loss function, including the domain adaptation component, in boosting classification accuracy and robustness. Significance. These results suggest that STeCANet offers a robust and interpretable solution for next-generation BCI applications.",
      "author": "Mohd Faisal, Sudarsan Sahoo and Jupitara Hazarika",
      "published_date": "2025-12-23T00:00:00+00:00",
      "source": "Journal Neural Engineering",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 189,
      "reading_time": 1,
      "created_at": "2025-12-29T05:29:51.775094+00:00",
      "updated_at": "2025-12-29T05:29:51.775096+00:00"
    },
    {
      "id": "14f0b2add61a1ce215f0ef02a0a02132",
      "url": "https://www.frontiersin.org/articles/10.3389/fncom.2025.1710961",
      "title": "Neurocognition, cerebellar functions and psychiatric features in spinocerebellar ataxia type 34: a case series",
      "content": "ObjectiveThis study primarily aimed to comprehensively characterize the neurological, neuroradiological and neurocognitive profiles, as well psychiatric features of individuals with Spinocerebellar Ataxia Type 34 (SCA34) associated with pathogenic variants in the ELOVL4 gene. Secondarily, we investigated the relationship between neurocognitive functions and cerebellar morphology in individuals with SCA34 by correlating structural changes to cognitive performance. Given involvement of the cerebellum in SCA34, our findings will contribute to a broader understanding of the role of the cerebellum in cognition.MethodsFour individuals (52 f, 72\u2009m, 76\u2009m, 76 f) underwent DNA testing using Next-Generation Sequencing and detailed assessment of neurocognitive functions. The test battery evaluated all six cognitive domains: verbal functions, executive functions, attention and processing speed, learning and memory, visuospatial perception and abilities, and social cognition. In addition, cerebellar and motor functions were evaluated using Finger Tapping, Prism Adaptation, and the Motor Speed subtest of the Delis-Kaplan executive function system (D-KEFS). Test results were compared with each individual\u2019s estimated premorbid cognitive level, determined from their highest educational attainment or occupational status prior to disease onset. Psychiatric symptoms related to anxiety, depression, and sleep were reported using clinical scales. The Scale for the Assessment and Rating of Ataxia (SARA) was used to assess ataxia severity. Two individuals and one matched control underwent high-resolution 7T MRI to characterize cerebellar morphology.ResultsNeurocognitive assessments identified cognitive and motor dysfunction across all individuals, including distinct neurocognitive impairments consistent with cerebellar cognitive-affective syndrome (CCAS), along with additional deficits in learning, visual and verbal episodic memory, emotion recognition\u2014a component of social cognition. Anxiety and sleep disturbance, but not depression, were observed in both female participants. High-resolution 7\u2009T MRI revealed structural cerebellar alterations, with moderate to severe bilateral cerebellar atrophy, including the vermis and multiple lobules (Crus II, VIIb, VIIIa, VIIIb, IX), as well as atrophy of the middle and superior cerebellar peduncles, accompanied by mild pontine atrophy. Genetic analyses confirmed the involvement of ELOVL4-related disruptions in long-chain fatty acid biosynthesis, offering insight into the molecular underpinnings of cerebellar degeneration in SCA34.ConclusionIndividuals with SCA34 show cerebellar degeneration accompanied by cognitive, motor, and social-affective impairments consistent with CCAS. Atrophy of the vermis, multiple lobules, and cerebellar peduncles align with these deficits, highlighting the cerebellum\u2019s key role in cognition. ELOVL4-related disruptions in fatty acid biosynthesis provides insight into the molecular basis of SCA34. Together, these findings advance our understanding of how cerebellar pathology contributes to complex neurocognitive and psychiatric symptoms in genetic ataxias.",
      "author": "Sorina Gorcenco",
      "published_date": "2025-12-09T00:00:00+00:00",
      "source": "Frontiers Computational Neuroscience",
      "status": "pending",
      "priority": "medium",
      "tags": [],
      "word_count": 402,
      "reading_time": 2,
      "created_at": "2025-12-29T04:51:28.164617+00:00",
      "updated_at": "2025-12-29T04:51:28.164619+00:00"
    }
  ]
}