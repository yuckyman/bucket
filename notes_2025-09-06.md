# bucket + github actions automation plan

## current bucket strengths
- discord bot integration (`!add`, `!feeds`, `!brief`)
- rss feed management and fetching
- ai summarization (ollama/openai)
- pdf generation for offline reading
- sqlite database for persistence
- api endpoints for automation

## github actions adaptation strategy

### 1. cloud database transition
move from local sqlite to github repo-based storage:
```
bucket/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ articles.json        # replaces articles table
â”‚   â”œâ”€â”€ feeds.json          # replaces feeds table  
â”‚   â”œâ”€â”€ summaries.json      # replaces summaries table
â”‚   â””â”€â”€ queue.json          # pending items
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ newsletters/        # daily/weekly digests
â”‚   â”œâ”€â”€ briefings/         # on-demand reports
â”‚   â””â”€â”€ api/               # json endpoints
â””â”€â”€ .github/workflows/
```

### 2. workflow architecture
```yaml
# .github/workflows/collectors.yml
name: Data Collection
on:
  schedule:
    - cron: '*/30 * * * *'  # rss fetching
  workflow_dispatch:        # manual trigger
  repository_dispatch:      # discord webhook

# .github/workflows/processors.yml  
name: Content Processing
on:
  schedule:
    - cron: '0 */2 * * *'   # summarization
  push:
    paths: ['data/queue.json']

# .github/workflows/generators.yml
name: Output Generation  
on:
  schedule:
    - cron: '0 8 * * *'     # daily briefing
  workflow_dispatch:
```

### 3. discord webhook integration
transform discord bot commands into github workflow triggers:
```python
# discord bot webhook sender
@bot.command(name='add')
async def add_url(ctx, url):
    payload = {
        "event_type": "add_article",
        "client_payload": {
            "url": url,
            "user": str(ctx.author),
            "channel": str(ctx.channel),
            "timestamp": datetime.now().isoformat()
        }
    }
    # trigger github workflow via repository_dispatch
    requests.post(
        f"https://api.github.com/repos/{REPO}/dispatches",
        headers={"Authorization": f"token {GITHUB_TOKEN}"},
        json=payload
    )
    await ctx.send(f"queued {url} for processing")
```

### 4. enhanced automation blocks

#### rss-crawler (30min intervals)
```python
# .github/workflows/rss-crawler.py
def collect_rss_items():
    feeds = load_json('data/feeds.json')
    new_items = []
    
    for feed in feeds:
        if feed['active']:
            items = parse_feed(feed['url'])
            new_items.extend(dedupe_items(items))
    
    # append to queue for processing
    append_to_queue(new_items)
    update_api_endpoints()
    git_commit_push("rss: collected {len(new_items)} new items")
```

#### bookmark-processor (on discord webhook)
```python  
def process_discord_bookmark(payload):
    url = payload['url']
    metadata = {
        'source': 'discord',
        'user': payload['user'],
        'added_at': payload['timestamp']
    }
    
    # fetch and clean content
    content = fetch_article_content(url)
    
    # add to processing queue
    queue_item = {
        'url': url,
        'content': content,
        'metadata': metadata,
        'status': 'pending_summary'
    }
    
    append_to_json('data/queue.json', queue_item)
    git_commit_push(f"bookmark: added {url}")
```

#### content-summarizer (2hr intervals)
```python
def process_pending_summaries():
    queue = load_json('data/queue.json')
    pending = [item for item in queue if item['status'] == 'pending_summary']
    
    for item in pending[:5]:  # rate limit
        summary = generate_summary(item['content'])
        
        # move to processed
        processed_item = {**item, 'summary': summary, 'status': 'processed'}
        append_to_json('data/summaries.json', processed_item)
        
        # remove from queue  
        remove_from_queue(item['url'])
    
    git_commit_push(f"summaries: processed {len(pending)} items")
```

#### newsletter-generator (daily 8am)
```python
def generate_daily_newsletter():
    today = datetime.now().strftime('%Y-%m-%d')
    
    # collect recent summaries
    recent_summaries = get_recent_summaries(days=1)
    rss_highlights = get_rss_highlights(days=1) 
    
    # generate newsletter content
    newsletter = {
        'date': today,
        'summaries': recent_summaries,
        'rss_items': rss_highlights,
        'stats': get_daily_stats()
    }
    
    # create multiple outputs
    save_json(f'outputs/api/latest-newsletter.json', newsletter)
    save_pdf(f'outputs/newsletters/{today}-digest.pdf', newsletter)
    save_markdown(f'outputs/newsletters/{today}-digest.md', newsletter)
    
    git_commit_push(f"newsletter: generated {today} digest")
```

### 5. personal api endpoints (github pages)
```
https://yuckyman.github.io/bucket/api/
â”œâ”€â”€ latest-newsletter.json
â”œâ”€â”€ recent-bookmarks.json  
â”œâ”€â”€ rss-stats.json
â”œâ”€â”€ processing-queue.json
â”œâ”€â”€ feed-health.json
â””â”€â”€ automation-status.json
```

### 6. discord feedback integration
```python
# webhook back to discord after processing
def notify_discord_completion(webhook_url, message):
    if os.getenv('GITHUB_ACTIONS'):  # only in actions
        requests.post(webhook_url, json={"content": message})

# in workflow:
notify_discord_completion(
    os.getenv('DISCORD_WEBHOOK'),
    f"ðŸ“° daily newsletter ready: {newsletter_url}"
)
```

### 7. local sync setup
```bash
# crontab for local access
*/15 * * * * cd ~/bucket-outputs && git pull >/dev/null 2>&1

# quick access aliases
alias morning-brief="cat ~/bucket-outputs/outputs/api/latest-newsletter.json | jq -r '.summaries[].title'"
alias bucket-stats="curl -s https://yourusername.github.io/bucket/api/automation-status.json"
```

## migration strategy

### phase 1: parallel operation
- keep existing bucket functionality
- add github actions workflows alongside
- test with subset of feeds/bookmarks

### phase 2: workflow transition  
- migrate discord bot to webhook model
- transition database to json files
- validate output quality

### phase 3: full automation
- retire local bucket instance
- rely fully on github actions
- optimize for cloud execution

## enhanced discord commands
```
# existing bucket commands work, plus:
!bucket status           # github actions workflow status
!bucket trigger daily    # manual newsletter generation
!bucket feeds health     # check feed parsing status  
!bucket queue show       # see pending items
!bucket api latest       # get latest newsletter url
```

## benefits of this approach
- **resilience**: no local server to maintain
- **scalability**: github actions handle heavy lifting
- **accessibility**: outputs available anywhere via git/api
- **cost effective**: generous github actions free tier
- **version controlled**: full history of all content/changes
- **integration ready**: api endpoints for other automations

this keeps your existing bucket investment while adding cloud automation superpowers. want to start with migrating the rss-crawler workflow?